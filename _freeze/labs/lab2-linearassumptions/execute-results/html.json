{
  "hash": "b41baca48de893b2ed655038169def14",
  "result": {
    "markdown": "---\ntitle: \"Lab 2: Assumption in Linear Regression\"\nauthor: \"Patrick Ward (original), Chris Slaughter (modifications)\"\nname: labs/lab2-lienarassumptions.qmd\n---\n\n\n## Introduction\n\n-   Original simulation (Patrick Ward)\n    -   [Simulations in R Part 5: Homoskedasticity Assumption in Regression](https://optimumsportsperformance.com/blog/simulations-in-r-part-5-homoskedasticity-assumption-in-regression/)\n    -   Consult parts 1 to 4 if you want more background information simulations and resampling in R\n-   Modifications (Chris Slaughter)\n    -   Allow for different sample sizes\n\n    -   Add robust standard error estimates for comparison\n\n        -   Use the rms packages and ols function to fit linear models\n\n    -   Consider different error distributions\n\n## Regression Assumptions\n\nThere are a number of assumptions that underpin linear regression models. Simulation can be a useful way of exploring these assumptions and understanding how violating these assumptions can lead to bias, large variance in the regression coefficients, and/or poor predictions.\n\nSome typical assumptions include:\n\n1.  Homoskedasticity\n2.  Multicollinearity of independent variables\n3.  Measurement Error\n4.  Serial correlation\n\nToday, we will explore the assumption of homoskedasticity and the distribution of the error terms for varying sample sizes.\n\n## Coverage Probability\n\nWe will primarily be evaluating the **coverage probability**. The **coverage probability** is the probability that confidence interval will contain the true values of interest. Coverage probability is a common frequentist statistic that is used to describe the behavior of a model.\n\nWe can estimate the the coverage probability through statistical simulation. If we simulate many datasets and calculate a 95% confidence interval based on each dataset, we can count the number of times that the confidence interval contains the true value. If a 95% CI contains the true value 95% of the time, the coverage probability is correct.\n\n## Lab Instructions\n\n1.  Consider the following simulation as currently written. Run the simulation an obtain the results for baseline simulation and the homoskedastiicity simulation.\n\n2.  Modify the simulation code to include robust standard error estimates in addition to classical standard error estimates. Then, compare the coverage probabilities of the intercept and slope when using robust standard errors to classical standard errors.\n\n3.  Create a new section to allow us to modify the models further. Consider\n\n    1.  Different sample sizes, particularly sample sizes below 500. How does sample size impact the coverage probabilities when using the robust and classical standard error estimates?\n\n    2.  Different distributions for the error variance rather than Normal. Such as,\n\n        -   t-distribution with 1 d.f. (Cauchy; very heavy tailed)\n\n        -   t-distribution with 7 d.f. (heavy tailed, less so)\n\n        -   exponential (1) - 1 (skewed right, subtract 1 so has mean 0)\n\n        -   Other distributions of your choice\n\n    3.  I will collect results from the class and collect in a table so we can see that patterns, if any, that appear\n\n## Creating the baseline simulation\n\nBefore exploring how violations of the homoskedasticity assumption influence a regression model, we need a baseline model to compare it against. So, we will begin by simulating a simple linear regression with 1 predictor. Our model will look like this:\n\n$y = 2 + 5*x + e$\n\nWhere `e` will be random error from a normal distribution with a mean of 0 and standard deviation of 1.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n✔ broom        1.0.6      ✔ recipes      1.0.10\n✔ dials        1.2.1      ✔ rsample      1.2.1 \n✔ dplyr        1.1.4      ✔ tibble       3.2.1 \n✔ ggplot2      3.5.1      ✔ tidyr        1.3.1 \n✔ infer        1.0.7      ✔ tune         1.2.1 \n✔ modeldata    1.3.0      ✔ workflows    1.1.4 \n✔ parsnip      1.2.1      ✔ workflowsets 1.1.0 \n✔ purrr        1.0.2      ✔ yardstick    1.3.1 \n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Learn how to get started at https://www.tidymodels.org/start/\n```\n:::\n\n```{.r .cell-code}\nlibrary(patchwork)\nlibrary(rms)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: Hmisc\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'Hmisc'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:parsnip':\n\n    translate\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:dplyr':\n\n    src, summarize\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:base':\n\n    format.pval, units\n```\n:::\n\n```{.r .cell-code}\n## set seed for reproducibility\nset.seed(58968)\n\n## create a data frame to store intercept values, slope values, their standard errors, and the model residual standard error, for each simulation\nsim_params <- data.frame(intercept = NA,\n                      slope = NA,\n                      intercept_se = NA,\n                      slope_se = NA,\n                      model_rse = NA)\n\n# true intercept value\nintercept <- 2\n\n# true slope value\nslope <- 5\n\n## Number of indepdendent observations\nn <- 500\n\n## Number of simulation replications to run\nreps <- 5000\n\n# random draw from a uniform distribution to simulate the predictor variable\nX <- runif(n = n, min = -1, max = 1)\n\n## loop for regression model\nfor(i in 1:reps){\n  \n  # create dependent variable, Y\n  Y <- intercept + slope*X + rnorm(n = n, mean = 0, sd = 1)\n  \n  # build model\n  model <- ols(Y ~ X, x=TRUE)\n  \n  # # store predictions\n  fitted_vals <- model$fitted.values\n\n  # # store residuals\n  # output_df[i, 2] <- model$residuals\n  \n  # variance-covariance matrix for the model\n  vcv <- vcov(model)\n  \n  # estimates for the intercept\n  sim_params[i, 1] <- model$coef[1]\n  \n  # estimates for the slope\n  sim_params[i, 2] <- model$coef[2]\n  \n  # SE for the intercept\n  sim_params[i, 3] <- sqrt(diag(vcv)[1])\n  \n  # SE for the slope\n  sim_params[i, 4] <- sqrt(diag(vcv)[2])\n  \n  # model RSE\n  sim_params[i, 5] <- model$stats[\"Sigma\"]\n  \n}\n\nhead(sim_params)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  intercept    slope intercept_se   slope_se model_rse\n1  1.966323 4.977753   0.04495439 0.07883016 1.0036923\n2  1.947826 4.966831   0.04460191 0.07821206 0.9958226\n3  2.111264 4.900025   0.04508832 0.07906500 1.0066825\n4  2.029259 4.970287   0.04317960 0.07571795 0.9640667\n5  1.976044 5.064834   0.04446964 0.07798012 0.9928694\n6  1.977989 5.071687   0.04461868 0.07824146 0.9961969\n```\n:::\n:::\n\n\nNow we summarize the data to see if we have values close to the specified model parameters\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate mean for each column of sim_params\napply(sim_params, 2, mean)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   intercept        slope intercept_se     slope_se    model_rse \n  1.99925865   4.99970448   0.04479625   0.07855285   1.00016164 \n```\n:::\n:::\n\n\nThe final model of the 'r reps' iterations is also stored from our for loop and we can look directly at it and create plots of the model fit.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd<- datadist(X)\noptions(datadist=\"d\")\n# model summary\nmodel\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear Regression Model\n\nols(formula = Y ~ X, x = TRUE)\n\n                 Model Likelihood    Discrimination    \n                       Ratio Test           Indexes    \nObs     500    LR chi2    1121.81    R2       0.894    \nsigma0.9745    d.f.             1    R2 adj   0.894    \nd.f.    498    Pr(> chi2)  0.0000    g        3.264    \n\nResiduals\n\n     Min       1Q   Median       3Q      Max \n-2.42004 -0.63347 -0.02173  0.71448  2.72872 \n\n          Coef   S.E.   t     Pr(>|t|)\nIntercept 2.0326 0.0436 46.57 <0.0001 \nX         4.9585 0.0765 64.78 <0.0001 \n```\n:::\n\n```{.r .cell-code}\n# model fit plots\nplot(model, which=1)\n```\n\n::: {.cell-output-display}\n![](lab2-linearassumptions_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nWe can also create a function that lets us evaluate how often the 95% confidence interval of our simulated beta coefficients cover the true beta coefficients that we specified for the simulation. From there, we can get a coverage probability and a 95% probability coverage interval\n\n\n::: {.cell}\n\n```{.r .cell-code}\n### Create a coverage probability function\ncoverage_interval95 <- function(beta_coef, se_beta_coef, true_beta_val, model_df){\n  \n  level95 <- 1 - (1 - 0.95) / 2\n  \n  # lower 95\n  lower95 <- beta_coef - qt(level95, df = model_df)*se_beta_coef\n  \n  # upper 95\n  upper95 <- beta_coef + qt(level95, df = model_df)*se_beta_coef\n  \n  # what rate did we cover the true value (hits and misses)\n  hits <- ifelse(true_beta_val >= lower95 & true_beta_val <= upper95, 1, 0)\n  prob_cover <- mean(hits)\n  \n  # create the probability coverage intervals\n  low_coverage_interval <- prob_cover - 1.96 * sqrt((prob_cover * (1 - prob_cover)) / length(beta_coef))\n  \n  upper_coverage_interval <- prob_cover + 1.96 * sqrt((prob_cover * (1 - prob_cover)) / length(beta_coef))\n  \n  # results in a list\n  return(list('Probability of Covering the True Value' = prob_cover,\n              '95% Probability Coverage Intervals' = c(low_coverage_interval, upper_coverage_interval)))\n  \n}\n```\n:::\n\n\nLet's apply it to the intercept.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoverage_interval95(beta_coef = sim_params$intercept,\n                    se_beta_coef = sim_params$intercept_se,\n                    true_beta = intercept,\n                    model_df = model$df.residual)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$`Probability of Covering the True Value`\n[1] 0.948\n\n$`95% Probability Coverage Intervals`\n[1] 0.9418457 0.9541543\n```\n:::\n:::\n\n\nNow apply it to the slope\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoverage_interval95(beta_coef = sim_params$slope,\n                    se_beta_coef = sim_params$slope_se,\n                    true_beta = slope,\n                    model_df = model$df.residual)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$`Probability of Covering the True Value`\n[1] 0.9538\n\n$`95% Probability Coverage Intervals`\n[1] 0.9479814 0.9596186\n```\n:::\n:::\n\n\nIn both cases we are covering the true betas around 95% of the time, with relatively small intervals.\n\n## Homoskedasticity\n\nLinear models make an assumption that the variance of the residuals remain constant across the predicted values (homoskedastic). We can see what this looks like by plotting the fitted values relative to the residuals, which was the first plot in the model check plots we created for the last simulation above. We can see that the residuals exhibit relatively the same amount of variance across the fitted values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(model, which = 1)\n```\n\n::: {.cell-output-display}\n![](lab2-linearassumptions_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nLet's simulate a model with heteroskedastic residuals and see what it looks like. We will keep the same intercept and slope parameters as above. The only thing will we do is add an exponential parameter to the error term of the model to create a heteroskedastic outcome in the residuals.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## parameter for heteroskedasticity \nheteroskedasticity_param <- 2\n\n## set seed for reproducibility\nset.seed(22)\n\n## data frame for results\nheteroskedastic_sim_params <- data.frame(intercept = NA,\n                      slope = NA,\n                      intercept_se = NA,\n                      slope_se = NA,\n                      model_rse = NA)\n\n## for loop\nfor(i in 1:reps){\n  \n  # the error variance of Y is a function of X plus some random noise\n  Y <- intercept + slope*X + rnorm(n = n, mean = 0, sd = exp(X*heteroskedasticity_param))\n  \n  # model\n  heteroskedastic_model <- ols(Y ~ X, x=TRUE)\n  \n  \n  # variance-covariance matrix\n  vcv <- vcov(heteroskedastic_model)\n  \n  # estimates for the intercept\n  heteroskedastic_sim_params[i, 1] <- heteroskedastic_model$coef[1]\n  \n  # estimates for the slope\n  heteroskedastic_sim_params[i, 2] <- heteroskedastic_model$coef[2]\n  \n  # SE for the intercept\n  heteroskedastic_sim_params[i, 3] <- sqrt(diag(vcv)[1])\n  \n  # SE for the slope\n  heteroskedastic_sim_params[i, 4] <- sqrt(diag(vcv)[2])\n  \n  # model RSE\n  heteroskedastic_sim_params[i, 5] <- heteroskedastic_model$stats[\"Sigma\"]\n  \n}\n\nhead(heteroskedastic_sim_params)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  intercept    slope intercept_se  slope_se model_rse\n1  1.884165 4.743169    0.0989576 0.1735279  2.209417\n2  2.096654 5.246191    0.1156089 0.2027270  2.581189\n3  1.950191 4.884566    0.1109784 0.1946072  2.477805\n4  1.996244 5.062478    0.1135296 0.1990809  2.534765\n5  2.048404 5.156628    0.1130880 0.1983065  2.524906\n6  2.043749 4.992856    0.1283863 0.2251330  2.866469\n```\n:::\n\n```{.r .cell-code}\nplot(X, Y, pch = 19)\n```\n\n::: {.cell-output-display}\n![](lab2-linearassumptions_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nThe relationship between X and Y certainly looks weird given how it starts very tightly on the left side and then fans out on the right side.\n\nLet's take the average across all simulations for each coefficient and their corresponding standard errors.\n\n\n::: {.cell}\n\n```{.r .cell-code}\napply(heteroskedastic_sim_params, 2, mean)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   intercept        slope intercept_se     slope_se    model_rse \n   2.0006111    5.0038511    0.1119348    0.1962842    2.4991571 \n```\n:::\n:::\n\n\nThe coefficients of 2.0 for the intercept and 5 for the slope are exactly what we set them as for the simulation. However, notice how much larger the standard errors are for the intercept and slope compared to the original model above. Additionally, notice that the model residual standard error has increased substantially compared to the previous model.\n\nLet's get the last model again and check out the fitted vs residual plot.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# fitted vs residuals\nplot(heteroskedastic_model, which = 1)\n```\n\n::: {.cell-output-display}\n![](lab2-linearassumptions_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nThat looks like a large amount of heteroskedasticity as the residual variance is no longer homogenous across the range of fitted values. Notice the large fanning out towards the right side of the plot. As the predictions get larger so two does the variability in residuals, which we noticed when we plotted Y and X above.\n\nWhat we've learned is that the estimate of intercept and slope is unbiased for both the heteroskedastic and homoskedastic models, as they both are centered on the parameters that we specified for the simulation (intercept = 2, slope = 5). However, the heteroskedastic model creates greater variance in our coefficients. We can visualize how much uncertainty there is under the heteroskedastic model relative to the homoskedastic model by visualizing the density of the coefficient estimates from our two model simulations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplt_intercept <- sim_params %>%\n  mutate(model = 'homoskedastic model') %>%\n  bind_rows(\n    heteroskedastic_sim_params %>%\n      mutate(model = 'heteroskedastic model')\n  ) %>%\n  ggplot(aes(x = intercept, fill = model)) +\n  geom_density(alpha = 0.6) +\n  theme_classic() +\n  theme(legend.position = \"top\")\n\nplt_slope <- sim_params %>%\n  mutate(model = 'homoskedastic model') %>%\n  bind_rows(\n    heteroskedastic_sim_params %>%\n      mutate(model = 'heteroskedastic model')\n  ) %>%\n  ggplot(aes(x = slope, fill = model)) +\n  geom_density(alpha = 0.6) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\nplt_intercept | plt_slope\n```\n\n::: {.cell-output-display}\n![](lab2-linearassumptions_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\nFinally, let's see how often the 95% coverage interval is covering the true intercept and slope in the heteroskedastic model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoverage_interval95(beta_coef = heteroskedastic_sim_params$intercept,\n                    se_beta_coef = heteroskedastic_sim_params$intercept_se,\n                    true_beta = intercept,\n                    model_df = model$df.residual)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$`Probability of Covering the True Value`\n[1] 0.9344\n\n$`95% Probability Coverage Intervals`\n[1] 0.9275374 0.9412626\n```\n:::\n\n```{.r .cell-code}\ncoverage_interval95(beta_coef = heteroskedastic_sim_params$slope,\n                    se_beta_coef = heteroskedastic_sim_params$slope_se,\n                    true_beta = slope,\n                    model_df = model$df.residual)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$`Probability of Covering the True Value`\n[1] 0.829\n\n$`95% Probability Coverage Intervals`\n[1] 0.8185637 0.8394363\n```\n:::\n:::\n\n\nNotice that we are no longer covering the true model values at the 95% level.\n",
    "supporting": [
      "lab2-linearassumptions_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}