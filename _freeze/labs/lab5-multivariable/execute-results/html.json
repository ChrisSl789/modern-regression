{
  "hash": "dbf7f2f91ff5ae5b3abfb1e9e1c7d014",
  "result": {
    "markdown": "---\ntitle: \"Lab 5: Multivariable Regression\"\nauthor: \"Author\"\nformat:\n  html:\n    embed-resources: true\n    code-tools: true\neditor: source\n---\n\n\n\n\n## Background\n\nAnother way to think about regression is the amount of variablity in the outcome (Y) that is explained by the predictors (X).  In simple linear regression, we regress X on Y because we believe that X will explain some of the variability in Y.  This leads to an alternate way of thinking about statistical tests for regression coefficients in terms of the variability of the outcome.  Specifically, the null hypothesis is that X explains none of the variability in Y, and the alternative hypothesis is that X explains more variability than would be expected by chance alone.  In this lab we will consider the interpretation of statistical tests in a multivariable model that adds another predictor (W) to the model.  Salary will be outcome (Y), male gender the predictor of interest (X), and year of degree the additional covariate (W).\n\n\nAn added-variable plot is a scatterplot of the transformations of an independent variable (say, $X_1$) and the dependent variable ($y$) that nets out the influence of all the other independent variables. The fitted regression line through the origin between these transformed variables has the same slope as the coefficient on x1 in the full regression model which includes all the independent variables. An added-variable plot is the multivariable analogue of using a simple scatterplot with a regression fit when there are no other covariates to show the relationship between a single x variable and a y variable. An added-variable plot is a visually compelling method for showing the nature of the partial correlation between $X_1$ and y as estimated\nin a multiple regression.\n\n\n### New functions\n\nTo obtain residuals for a model fit, use the resid() function on the model fit.\n\navPlots in the car packages will added-variable, also called partial-regression, plots for linear and generalized linear models.\n\nIn part 1 and part 2 of the lab, we will construct added variable plots in steps without the use of a specialized function.\n\n\n## Example 1: Duncan Data\n\nDuncan's Occupational Prestige. Data on the prestige and other characteristics of 45 U. S. occupations in 1950.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(car)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: carData\n```\n:::\n\n```{.r .cell-code}\nhead(Duncan)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           type income education prestige\naccountant prof     62        86       82\npilot      prof     72        76       83\narchitect  prof     75        92       90\nauthor     prof     55        90       76\nchemist    prof     64        86       90\nminister   prof     21        84       87\n```\n:::\n:::\n\n\n\nObtain summary statistics for the variables in `Duncan`:\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(Duncan)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   type        income        education         prestige    \n bc  :21   Min.   : 7.00   Min.   :  7.00   Min.   : 3.00  \n prof:18   1st Qu.:21.00   1st Qu.: 26.00   1st Qu.:16.00  \n wc  : 6   Median :42.00   Median : 45.00   Median :41.00  \n           Mean   :41.87   Mean   : 52.56   Mean   :47.69  \n           3rd Qu.:64.00   3rd Qu.: 84.00   3rd Qu.:81.00  \n           Max.   :81.00   Max.   :100.00   Max.   :97.00  \n```\n:::\n:::\n\n\nAs a first graph, we view a histogram of the variable `prestige`:\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(Duncan, hist(prestige))\n```\n\n::: {.cell-output-display}\n![](lab5-multivariable_files/figure-html/unnamed-chunk-6-1.png){width=480}\n:::\n:::\n\n\n\n#####  Examining the Data\n\nThe `scatterplotMatrix()` function in the **car** package produces scatterplots for all paris of variables.  A few relatively remote points are marked by case names, in this instance by occupation.\n\n::: {.cell}\n\n```{.r .cell-code}\nscatterplotMatrix( ~ prestige + education + income, \n    id=list(n=3), data=Duncan)\n```\n\n::: {.cell-output-display}\n![](lab5-multivariable_files/figure-html/unnamed-chunk-8-1.png){width=768}\n:::\n:::\n\n\n#### Regression Analysis\n\nWe use the`lm()` function to fit a linear regression model to the data:\n\n::: {.cell}\n\n```{.r .cell-code}\n(duncan.model <- lm(prestige ~ education + income, data=Duncan))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = prestige ~ education + income, data = Duncan)\n\nCoefficients:\n(Intercept)    education       income  \n    -6.0647       0.5458       0.5987  \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(duncan.model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = prestige ~ education + income, data = Duncan)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.538  -6.417   0.655   6.605  34.641 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -6.06466    4.27194  -1.420    0.163    \neducation    0.54583    0.09825   5.555 1.73e-06 ***\nincome       0.59873    0.11967   5.003 1.05e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.37 on 42 degrees of freedom\nMultiple R-squared:  0.8282,\tAdjusted R-squared:   0.82 \nF-statistic: 101.2 on 2 and 42 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n#### Added-variable plot\n\n\nAdded-variable plots for Duncan's regression, looking for influential cases:\n\n\n::: {.cell}\n\n```{.r .cell-code}\navPlots(duncan.model, \n    id=list(cex=0.75, n=3, method=\"mahal\"))\n```\n\n::: {.cell-output-display}\n![](lab5-multivariable_files/figure-html/unnamed-chunk-14-1.png){width=768}\n:::\n:::\n\n\n\n## Example 2: FEV and smoking, age, height\n\nRecall the FEV data presented in the notes\n\n* In unadjusted models, smokers had lower FEV that non-smokers.  We suspect this is due to confouding by age\n* Age is suspected to be associated with smoking (in the sample) and FEV (in truth)\n* In unadjusted models, height is likely associated with FEV and age.  There is likely an association between height and smoking through age.\n\n* In adjusted models, we might expect\n\n1. Lower FEV comparing a smoker to non-smoker of the same age and height\n2. Higher FEV comparing individuals differing by on year of age but with the same smoking status and height\n3. Higher FEV in taller individuals of the same age and smoking status\n\nTry to identify these observations in the plots that follow\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfev <- read.table(\"http://www.emersonstatistics.com/Datasets/fev.txt\", header = TRUE)\nfev$smoker <- ifelse(fev$smoke == 2, 0, 1)\nsummary(fev)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     seqnbr          subjid           age              fev       \n Min.   :  1.0   Min.   :  201   Min.   : 3.000   Min.   :0.791  \n 1st Qu.:164.2   1st Qu.:15811   1st Qu.: 8.000   1st Qu.:1.981  \n Median :327.5   Median :36071   Median :10.000   Median :2.547  \n Mean   :327.5   Mean   :37170   Mean   : 9.931   Mean   :2.637  \n 3rd Qu.:490.8   3rd Qu.:53638   3rd Qu.:12.000   3rd Qu.:3.119  \n Max.   :654.0   Max.   :90001   Max.   :19.000   Max.   :5.793  \n     height           sex            smoke           smoker       \n Min.   :46.00   Min.   :1.000   Min.   :1.000   Min.   :0.00000  \n 1st Qu.:57.00   1st Qu.:1.000   1st Qu.:2.000   1st Qu.:0.00000  \n Median :61.50   Median :1.000   Median :2.000   Median :0.00000  \n Mean   :61.14   Mean   :1.486   Mean   :1.901   Mean   :0.09939  \n 3rd Qu.:65.50   3rd Qu.:2.000   3rd Qu.:2.000   3rd Qu.:0.00000  \n Max.   :74.00   Max.   :2.000   Max.   :2.000   Max.   :1.00000  \n```\n:::\n:::\n\n\n### Scatterplot matrix\n\nThe scatterplot matrix summarizes the unadjusted associations.  \n\n\n::: {.cell}\n\n```{.r .cell-code}\nscatterplotMatrix( ~ fev + age + height + smoker, data=fev)\n```\n\n::: {.cell-output-display}\n![](lab5-multivariable_files/figure-html/unnamed-chunk-18-1.png){width=768}\n:::\n:::\n\n\n\n#### Regression Analysis\n\nWe use the`lm()` function to fit a linear regression model to the data in those 9 and older.\n\n::: {.cell}\n\n```{.r .cell-code}\n(fev.model <- lm(fev ~ smoker + height + age, data=fev[fev$age>=9,]))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = fev ~ smoker + height + age, data = fev[fev$age >= \n    9, ])\n\nCoefficients:\n(Intercept)       smoker       height          age  \n    -6.4313      -0.1784       0.1353       0.0712  \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fev.model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = fev ~ smoker + height + age, data = fev[fev$age >= \n    9, ])\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.63401 -0.27416  0.00003  0.26550  1.82040 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -6.431311   0.355959 -18.068  < 2e-16 ***\nsmoker      -0.178429   0.065107  -2.741  0.00639 ** \nheight       0.135303   0.006322  21.401  < 2e-16 ***\nage          0.071201   0.011884   5.991 4.37e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4478 on 435 degrees of freedom\nMultiple R-squared:  0.6632,\tAdjusted R-squared:  0.6609 \nF-statistic: 285.5 on 3 and 435 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n#### Added-variable plot\n\n\nAdded-variable plots for FEV regression to visualize the adjusted associations\n\n\n::: {.cell}\n\n```{.r .cell-code}\navPlots(fev.model)\n```\n\n::: {.cell-output-display}\n![](lab5-multivariable_files/figure-html/unnamed-chunk-24-1.png){width=768}\n:::\n:::\n\n\n\n## Initial Lab Setup\n\nFor this lab, we will be using salary data from they year 1995.  We will focus on the variables: salary, sex, and yrdeg\n\nInitial dataset manipulations\n\n1. Read in the salary dataset\n2. Remove any observations that are not from 1995 (use the 'year' variable)\n3. Describe the dataset\n4. Create an indicator variable for male gender\n\n\n## Lab Part 1\n\n### Model 1: Fit a simple linear regression model with salary as the outcome and male as the predictor.  Save the residuals from this model.  Interpret these residuals in terms of the unexplained variability in salary.\n\n### Model 2: Fit a simple linear regression model with yrdeg as the outcome and male as the predictor.  Save the residuals from this model.  Interpret these residuals in terms of the unexplained variability in yrdeg.\n\n### Plot the residuals from model 2 (X-axis) versus the residuals from model 1 (y-axis).  Describe any association you see.  It may be helpful to add a lowess smooth or other smooth line to the plot.\n\n### Model 3: Fit a simple linear regression model using the residuals from model 1 as the outcome and the residuals from model 2 as the predictor.  Interpret the slope coefficient from this model.\n\n### Model 4: Fit a multivariable linear regression model with salary as the outcome using predictors male and yrdeg.  What is the interpretation of the male coefficient in this model?  What is the interpretation of the yrdeg coefficient?\n\n### Compare the slope estimate for yrdeg from Model 3 to the slope estimate obtained in Model 4.  Explain your findings.\n\n\n## Lab Part 2\n\n### Create a new variable FULL that takes on the value 1 for full professors and 0 for Assistant or Associate Professors.\n\n### Determine if FULL explains some of the variability in salary after adjusting for year of degree and gender by fitting the multivariable regression model and by regressing residuals from “Model A” on the residuals from “Model B” other as was done previously (you will need to figure out what “Model A” and “Model B” should be).  Compare the results from the two models.\n\n",
    "supporting": [
      "lab5-multivariable_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}