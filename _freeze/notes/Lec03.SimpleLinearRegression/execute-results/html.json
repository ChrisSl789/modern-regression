{
  "hash": "81f903ec2cffcd943b2eba84a4737206",
  "result": {
    "markdown": "---\ntitle: \"Simple Linear Regression\"\nsubtitle: \"Lecture 03\"\nauthor: \"Chris Slaughter\"\nfooter: \"Bios 6312\"\ndate: last-modified\nformat:\n  html:\n    number-sections: true\n    number-depth: 4\n    anchor-sections: true\n    smooth-scroll: true\n    theme: journal\n    toc: true\n    toc-depth: 4\n    toc-title: Contents\n    toc-location: left\n    code-link: false\n    code-tools: true\n    code-fold: true\n    code-block-bg: \"#f1f3f5\"\n    code-block-border-left: \"#31BAE9\"\n    reference-location: margin\n    fig-cap-location: margin\n    fontsize: medium\nexecute:\n   warning: false\n   message: false\neditor: \n  markdown: \n    wrap: 72\n---\n\n\n## General Regression Setting\n\n### Two variable setting\n\n-   Many statistical problems examine the association between two\n    variables\n\n    -   Outcome variable (response variable, dependent variable)\n\n    -   Grouping variable (covariate, predictor variable, independent\n        variable)\n\n-   Compare distribution of the outcome variable across levels of the\n    grouping variable\n\n-   Groups are defined by the grouping variable\n\n    -   Within each group, the grouping variable is constant Â \n\n-   In intro course, statistical analysis is characterized by two\n    factors\n\n    -   Number of groups (samples)\n\n    -   If subjects in groups are independent\n\n-   In the two variable setting, statistical analysis is more generally\n    characterized by the grouping variable. If the grouping variable is\n\n    -   Constant: One sample problem\n\n    -   Binary: Two sample problem\n\n    -   Categorical: $k$ sample problem (e.g. ANOVA)\n\n    -   Continuous: Infinite sample problem (analyzed with regression)\n\n-   Regression thus *extends* the one- and two-sample problems up to\n    infinite sample problems\n\n    -   Of course, in reality we never have *infinite* samples, but\n        models that can handle this case are the ultimate generalization\n\n    -   Continuous predictors of interest\n\n    -   Continuous adjustment variables\n\n\n::: {.cell}\n\n```{.r .cell-code}\n  ### Make a cholesterol and age dataframe.  Set the random number seed so everything is reproducible\nset.seed(19)\n\nplotdata <- data.frame(age=c(63, 63, rep(65:80,15), rep(81:85,10), 86,86,86, 87,89, 90, 93, 95, 100),\n                       chol=NA)\nplotdata$chol <- 190 + .5*plotdata$age + rnorm(length(plotdata$age), 0, 15)\n\nlibrary(ggplot2)\nggplot(plotdata, aes(x=age, y=chol)) + geom_point() + ylab(\"Cholesterol\") + xlab(\"Age (years)\") + theme_bw()\n```\n\n::: {.cell-output-display}\n![Scatterplot of cholesterol and age. Age is a continuous grouping variable.](Lec03.SimpleLinearRegression_files/figure-html/fig-chol-age-1.png){#fig-chol-age width=672}\n:::\n:::\n\n\n### Regression versus two sample approaches\n\n-   With a binary grouping variable, regression models reduce to the\n    corresponding two variable methods\n\n-   Linear regression with a binary predictor\n\n    -   t-test, equal variance: Classic linear regression\n    -   t-test, unequal variance: Linear regression with robust standard\n        errors (approximately)\n\n-   Logistic regression with a binary predictor\n\n    -   (Pearson) Chi-squared test: Score test from logistic regression\n\n-   Proportional odds regression with a binary predictor\n\n    -   Wilcoxon Rank Sum Test\n\n-   Cox (proportional hazards) regression with a binary predictor\n\n    -   Log-rank test: Score test from Cox regression\n\n### Guiding principle\n\n-   Everything can be recast as a regression problem.\n\n## Motivating Problem: Cholesterol and Age\n\n### Definitions\n\n-   Is there an association between cholesterol and age?\n\n-   Scientific question: Does aging effect cholesterol?\n\n-   Statistical question: Does the distribution of cholesterol differ\n    across age groups?\n\n    -   Acknowledges variability in the response (cholesterol)\n\n    -   Acknowledges cause-effect relationship is uncertain\n\n    -   Association does not imply causation\n\n    -   Any observed differences could be due to other factors. For\n        example, the association could be with year of birth rather than\n        age.\n\n-   Continuous response variable: Cholesterol\n\n-   Continuous grouping variable (predictor of interest): Age\n\n    -   An infinite number of ages are possible\n    -   We will not sample every possible age, and may round for\n        simplicity\n\n### Simple Regression Model\n\n-   Attempt to answer scientific question by assessing linear trends in\n    average cholesterol\n\n-   Estimate the best fitting line to average cholesterol within age\n    groups\n\n$$\nE[\\textrm{Chol} | \\textrm{Age}] = \\beta_0 + \\beta_1 \\times \\textrm{Age}\n$$\n\n-   The expected value of cholesterol given age is modeled using an\n    intercept ($\\beta_0$) and slope ($\\beta_1$)\n\n    -   An association exists if the slope is nonzero\n\n    -   A non-zero slope indicates that the average cholesterol will be\n        different across different age groups\n\n#### Descriptive plots of cholesterol and age\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(plotdata, aes(x=age, y=chol)) + geom_point() + ylab(\"Cholesterol\") + xlab(\"Age (years)\") + theme_bw() + geom_smooth()\n```\n\n::: {.cell-output-display}\n![Scatterplot of cholesterol and age. Association is described using a lowess smooth line.](Lec03.SimpleLinearRegression_files/figure-html/fig-chol-age-lowess-1.png){#fig-chol-age-lowess width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(plotdata, aes(x=age, y=chol)) + geom_point() + ylab(\"Cholesterol\") + xlab(\"Age (years)\") + theme_bw() + geom_smooth(method=\"lm\")\n```\n\n::: {.cell-output-display}\n![Scatterplot of cholesterol and age. Association is described using a linear trend line.](Lec03.SimpleLinearRegression_files/figure-html/fig-chol-age-linear-1.png){#fig-chol-age-linear width=672}\n:::\n:::\n\n\n#### Simple linear regression model\n\n-   The simple regression model produces an easy to remember (but\n    approximate) rule of thumb.\n\n    -   \"Normal cholesterol is 190 plus half your age\"\n\n    -   $E[\\textrm{Chol} | \\textrm{Age}] = 190 + 0.50 \\times \\textrm{Age}$\n\n#### Frequentist Estimates and Interpretation\n\n-   Raw, unformatted estimates, model summary, and confidence intervals\n    for age\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm.chol <- lm(chol ~ age, data=plotdata)\nsummary(m.chol)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = chol ~ age, data = plotdata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-39.591 -10.524  -0.234  11.154  41.819 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 203.2259    10.3138  19.704   <2e-16 ***\nage           0.3209     0.1375   2.333   0.0203 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15.34 on 299 degrees of freedom\nMultiple R-squared:  0.01788,\tAdjusted R-squared:  0.0146 \nF-statistic: 5.444 on 1 and 299 DF,  p-value: 0.0203\n```\n:::\n\n```{.r .cell-code}\nconfint.default(m.chol, \"age\") # Based on asymptotic Normality\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       2.5 %    97.5 %\nage 0.051334 0.5904842\n```\n:::\n:::\n\n\n$E[\\textrm{Chol} | \\textrm{Age}] = 190.7 + 0.471 \\times \\textrm{Age}$\n\n-   Using finalfit to create a nicer output table of the coefficients\n    and confidence intervals\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmykable = function(x){\n  knitr::kable(x, row.names = FALSE, align = c(\"l\", \"l\", \"r\", \"r\", \"r\", \"r\", \"r\", \"r\", \"r\"),\n               booktabs=TRUE)\n}\n\n\nlibrary(finalfit)\nlibrary(dplyr)\nlibrary(rms)\nexplanatory = c(\"age\")\ndependent = 'chol'\n\nlabel(plotdata$chol) <- \"Cholesterol\"\nlabel(plotdata$age) <- \"Age (years)\"\nplotdata %>% \n  finalfit(dependent, explanatory) %>% mykable()\n```\n\n::: {.cell-output-display}\n|Dependent: Cholesterol |             |      unit|        value|    Coefficient (univariable)|  Coefficient (multivariable)|\n|:----------------------|:------------|---------:|------------:|----------------------------:|----------------------------:|\n|Age (years)            |[63.0,100.0] | Mean (sd)| 227.2 (15.5)| 0.32 (0.05 to 0.59, p=0.020)| 0.32 (0.05 to 0.59, p=0.020)|\n:::\n:::\n\n\n#### Bayesian Estimates and Interpretation\n\n-   Bayesian approach to the linear model requires specifying\n\n    -   The model, e.g. a linear model with intercept and slope for age,\n        Normally distributed errors and constant variance\n\n    -   Prior distributions on parameters\n\n        -   For the simple linear regression model, we have parameters\n            $\\beta_0$, $\\beta_1$, and $\\sigma$.\n\n        -   For now, we will use default prior distributions that are\n            are intended to be *weakly informative* in that they provide\n            moderate regularization and help stabilize computation. See\n            the [STAN\n            documentation](https://mc-stan.org/rstanarm/reference/priors.html)\n            for more details\n\n        -   Appropriate priors can be based on scientific considerations\n\n        -   Sensitivity analyses can evaluate the the robustness of\n            finding to different prior assumptions\n\n-   Output from Bayesian linear regression\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rstanarm)\nlibrary(bayesplot)\n\nfit1 <- stan_glm(chol ~ age,\n                 data=plotdata, family=gaussian(),\n                 seed=1234,\n                 refresh=0)\nsummary(fit1, digits=2, prob=c(.025, .5, .975))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nModel Info:\n function:     stan_glm\n family:       gaussian [identity]\n formula:      chol ~ age\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 301\n predictors:   2\n\nEstimates:\n              mean   sd     2.5%   50%    97.5%\n(Intercept) 203.28  10.69 182.56 203.18 224.80 \nage           0.32   0.14   0.04   0.32   0.60 \nsigma        15.39   0.63  14.25  15.37  16.74 \n\nFit Diagnostics:\n           mean   sd     2.5%   50%    97.5%\nmean_PPD 227.19   1.24 224.79 227.19 229.63 \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.18 1.00 3721 \nage           0.00 1.00 3735 \nsigma         0.01 1.00 3469 \nmean_PPD      0.02 1.00 3911 \nlog-posterior 0.03 1.00 1638 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n```\n:::\n:::\n\n\n-   And a summary of the prior distributions used\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprior_summary(fit1, digits = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPriors for model 'fit1' \n------\nIntercept (after predictors centered)\n  Specified prior:\n    ~ normal(location = 227, scale = 2.5)\n  Adjusted prior:\n    ~ normal(location = 227, scale = 39)\n\nCoefficients\n  Specified prior:\n    ~ normal(location = 0, scale = 2.5)\n  Adjusted prior:\n    ~ normal(location = 0, scale = 6)\n\nAuxiliary (sigma)\n  Specified prior:\n    ~ exponential(rate = 1)\n  Adjusted prior:\n    ~ exponential(rate = 0.065)\n------\nSee help('prior_summary.stanreg') for more details\n```\n:::\n:::\n\n\n-   Interpretation\n\n    -   Slope for age is of primary scientific importance\n\n    -   *A priori* we assume that no association between age and\n        cholesterol. Specifically, we assumed a Normal prior with\n        location (mean) of 0 and scale (standard devation) of 6 for\n        $\\beta_1$.\n\n    -   Conditional on the data, we estimate that for every 1 year\n        increase in age, cholesterol increases by 0.32 mg/dl (95%\n        credible interval 0.06 to 0.58).\n\n#### Comparison of Frequentist and Bayesian interpretation of the slope\n\n-   Following was given by gpt-4.0 [^1]\n\n[^1]: Query \"Explain how a slope from simple linear regression is\n    interpreted using language a college graduate could understand.\n    Provide a Bayesian and Frequentist interpretation.\"\n\n::: {.classname style=\"color: gray\"}\nIn simple linear regression, the slope (often denoted as \"b\" or \"beta\")\nis interpreted as the expected change in the dependent (or response)\nvariable for a one-unit increase in the independent (or predictor)\nvariable, assuming all other variables are held constant.\n\nFrom a Frequentist perspective, the slope is considered a fixed but\nunknown value which is estimated from the sample data. If the slope is\nfound to be 2, for instance, it would suggest that for every one-unit\nincrease in our predictor variable, we expect, on average, a 2-unit\nincrease in the response variable. This is based on the data at hand and\nis subject to sampling variability. Confidence intervals can be\nconstructed around this estimate to provide a range of plausible values\nfor this unknown fixed parameter. If a 95% confidence interval for the\nslope is (1.5, 2.5), it would suggest that we are 95% confident that the\ntrue slope falls within this range.\n\nIn contrast, a Bayesian approach treats the slope as a random variable.\nRather than a single fixed value, the slope has a distribution of\npossible values. This distribution combines our prior beliefs about what\nthe slope might be (prior distribution) and the observed data\n(likelihood). The resulting distribution (posterior distribution)\nreflects our updated belief about the slope after seeing the data. If\nthe mean of the posterior distribution is 2, it suggests that, given the\nprior and the data, our best guess is that a one-unit increase in the\npredictor variable corresponds to a 2-unit increase in the response\nvariable on average. A credible interval, say a 95% credible interval of\n(1.5, 2.5), suggests that there is a 95% probability that the true value\nof the slope falls within this range, given our prior and the observed\ndata.\n:::\n\n### Uses of Regression\n\n-   Borrowing information\n\n    -   Use other groups to make estimates in groups with sparse data\n\n    -   Intuitively, 67 and 69 year olds would provide some relevant\n        information about 68 year olds\n\n    -   Assuming a straight line relationship tells us about other, even\n        more distant, individuals\n\n    -   If we do not want to assume a straight line, we may only want to\n        borrow information from nearby groups\n\n    -   Locally weighted scatterplot smooth line (lowess) added to the\n        previous figures\n\n    -   Splines discussed in future lectures\n\n    -   May not want to borrow too much information\n\n-   Linear relationship is an assumption, with often low power to detect\n    departures from linearity\n\n-   Always avoid extrapolating beyond the range of the data (e.g. ages\n    under 65 or over 100)\n\n-   Defining \"Contrasts\"\n\n    -   Define a comparison across groups to use when answering\n        scientific questions\n\n    -   If the straight line relationship holds, the slope is the\n        difference in mean cholesterol levels between groups differing\n        by 1 year in age\n\n    -   Do we want to assume that comparisons of 65 to 66 year old\n        subjects are the same as comparisons of 95 to 96 year old\n        subjects?\n\n    -   If a non-linear relationship, the slope is still the average\n        difference in mean cholesterol levels between groups differing\n        by 1 year in age\n\n    -   Slope is a (first order or linear) test for trend\n\n### Linear Regression Inference\n\n-   Regression output provides\n\n    -   Estimates\n\n        -   Intercept: Estimated mean cholesterol when age is 0\n\n        -   Slope: Estimated average difference in average cholesterol\n            for two groups differing by 1 year in age\n\n        -   Standard errors\n\n    -   Confidence intervals\n\n    -   P-values for testing\n\n        -   Intercept is zero (usually unimportant)\n\n        -   Slope is zero (test for linear trend in means)\n\n-   Interpretation\n\nFrom linear regression analysis, we estimate that for each year\ndifference in age, the difference in mean cholesterol is 0.47 mg/dL. A\n95% confidence interval (CI) suggests that this observation is not\nunusual if the true difference in mean cholesterol per year difference\nin age were between 0.21 and 0.73 mg/dL. Because $p < 0.001$, we reject\nthe null hypothesis that there is no linear trend in the average\ncholesterol across age groups using a significance level, $\\alpha$, of\n$0.05$.\n\n## Simple Linear Regression\n\n### Ingredients\n\n-   Response\n\n    -   The distribution of this variable will be compared across groups\n\n    -   Linear regression models the mean of the response variable\n\n    -   Log transformation of the response corresponds to modeling the\n        geometric mean\n\n    -   Notation: Is is extremely common to use $Y$ to denote the\n        response variable when discussing general methods\n\n-   Predictor\n\n    -   Group membership is measured by this variable\n\n    -   Notation\n\n        -   When not using mnemonics, will be referred to as the $X$\n            variable in simple linear regression (linear regression with\n            one predictor)\n\n        -   Later, when we discuss multiple regression, will refer to\n            $X_1, X_2, \\ldots, X_p$ when there are up to $p$ predictors\n\n-   Regression Model\n\n    -   We typically consider a \"linear predictor function\" that is\n        linear in the modeled predictors\n\n    -   Expected value (i.e. mean) of $Y$ for a particular value of $X$\n\n    -   $E[Y | X] = \\beta_0 + \\beta_1 \\times X$\n\n    -   In a deterministic world, a line is of the form $y = mx + b$\n\n        -   With no variation in the data, each value of $y$ would like\n            exactly on a straight line\n\n        -   Intercept $b$ is values of $y$ when $x = 0$\n\n        -   Slope $m$ is the difference in $y$ for a one unit difference\n            in $x$\n\n    -   Statistics in not completely deterministic. The real world has\n        variability\n\n        -   Response within groups is variable (people born on the same\n            day will have different cholesterol levels!)\n\n        -   Randomness due to other variables impacting cholesterol\n\n        -   Inherent randomness\n\n-   The regression line thus describes the central tendency of the data\n    in a scatterplot of the response versus the predictor\n\n-   Interpretation of regression parameters\n\n    -   Intercept $\\beta_0$: Mean $Y$ for a group with $X=0$\n\n        -   Often $\\beta_0$ is not of scientific interest\n\n        -   May be out of the range of data, or even impossible to\n            observe $X=0$\n\n    -   Slope $\\beta_1$: Difference in mean $Y$ across groups differing\n        in $X$ by 1 unit\n\n        -   Usually measures association between $Y$ and $X$\n\n        -   $E[Y | X] = \\beta_0 + \\beta_1 \\times X$\n\n-   Derivation of interpretation\n\n    -   Simple linear regression of response $Y$ on predictor $X$\n\n    -   Mean of any arbitrary group can be derived from the $$\n         Y_i = \\beta_0 + \\beta_1 X_i\n         $$\n\n    -   Interpretation determined by considering possible values of $X$\n\n    -   Model: $E[Y_i | X_i] = \\beta_0 + \\beta_1 \\times X_i$\n\n        -   When $X_i = 0$, $E[Y_i | X_i = 0 ] = \\beta_0$\n\n        -   When $X_i = x$, $E[Y_i | X_i = x ] = \\beta_0 + \\beta_1 x$\n\n        -   When $X_i = x + 1$,\n            $E[Y_i | X_i = x + 1 ] = \\beta_0 + \\beta_1 x + \\beta_1$\n\n        -   We can use the above to get an equation for $\\beta_1$\n\n        $$\n         E[Y_i | X_i = x + 1 ] - E[Y_i | X_i = x ] = \\\\\n         (\\beta_0 + \\beta_1 x + \\beta_1) - (\\beta_0 + \\beta_1 x) \\\\\n         = \\beta_1\n         $$\n\n        -   Thus, $\\beta_1$ is the expected change in $Y$ per one unit\n            increase in $x$\n\n### Matrix Notation\n\n-   Using scalars, the simple linear regression model can be written as\n\n    -   $Y_i = \\beta_0 + \\beta_1 \\times X_i + \\epsilon_i$\n\n    -   $i = 1, \\ldots, n$\n\n    -   $i$ indexes the independent sampling units (e.g. subjects)\n\n    -   $n$ is the total number of independent sampling units\n\n    -   This formulauation drops the expected value notation, add in\n        $\\epsilon_i$\n\n        -   $\\epsilon_i$ are the \"Residuals\" or \"Errors\"\n\n        -   $E[\\epsilon_i] = 0$\n\n        -   $V[\\epsilon_i] = \\sigma^2$ (constant variance assumption)\n\n\n```{=tex}\n\\newcommand\\Ybd{{\\boldsymbol{Y}}}\n\\newcommand\\Xbd{{\\boldsymbol{X}}}\n\\newcommand\\betabd{{\\boldsymbol{\\beta}}}\n\\newcommand\\epsilonbd{{\\boldsymbol{\\epsilon}}}\n\\newcommand\\zerovec{{\\boldsymbol{0}}}\n\\newcommand\\onevec{{\\boldsymbol{1}}}\n\\newcommand\\eye{{\\boldsymbol{I}}}\n```\n\n-   Matrix notation:\n    ${\\boldsymbol{Y}}= {\\boldsymbol{X}}{\\boldsymbol{\\beta}}+ {\\boldsymbol{\\epsilon}}$\n\n${\\boldsymbol{Y}}= \\left( \\begin{array}{c} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{array} \\right)_{n\\times1}$\n${\\boldsymbol{X}}= \\left( \\begin{array}{cc} 1 & x_1 \\\\ 1 & x_2 \\\\ \\vdots & \\vdots \\\\ 1 & x_n \\end{array} \\right)_{n\\times2}$\n${\\boldsymbol{\\beta}}= \\left( \\begin{array}{c} \\beta_0 \\\\ \\beta_1 \\\\ \\end{array} \\right)_{2\\times1}$\n${\\boldsymbol{\\epsilon}}= \\left( \\begin{array}{c} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{array} \\right)_{n\\times1}$\n\n$E[{\\boldsymbol{\\epsilon}}] = {\\boldsymbol{0}}$, where\n${\\boldsymbol{0}}= \\left( \\begin{array}{c} 0 \\\\ 0 \\\\ \\vdots \\\\0 \\end{array} \\right)_{n\\times1}$\n\n$V[{\\boldsymbol{\\epsilon}}] = \\sigma^2 {\\boldsymbol{I}}$, where\n${\\boldsymbol{I}}= \\left( \\begin{array}{cccc} 1 & 0 & \\ldots & 0 \\\\ 0 & 1 & \\ldots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\ldots & 1 \\end{array} \\right)_{n\\times n}$\n\n$E[{\\boldsymbol{Y}}] = {\\boldsymbol{X}}{\\boldsymbol{\\beta}}$\n\n-   I am using standard notation to indicate matrices/vectors and\n    scalars\n\n-   Boldface indicates a vector or matrix (${\\boldsymbol{Y}}$,\n    ${\\boldsymbol{X}}$, ${\\boldsymbol{\\beta}}$,\n    ${\\boldsymbol{\\epsilon}}$, ${\\boldsymbol{0}}$, ${\\boldsymbol{I}}$)\n\n-   Normal typeface indicates a scalar ($Y_i$, $x_i$, $\\beta_0$,\n    $\\beta_1$, $\\epsilon_i$, $0$, $1$)\n\n## Example Analysis of BMI (Part 1)\n\n-   Example analysis conducted in class involving BMI (response) with\n    gender (predictor 1) and age (predictor 2)\n\n-   Stata and R code to reproduce the same analysis are provided below\n\n-   This is \"Lab 1\" and will serve as an example of how future labs will\n    proceed\n\n## Residuals\n\n-   Often linear regression models are specified in terms of the\n    response instead of the mean response\n\n    -   Include an error term in the model, $\\epsilon_i$\n\n    -   Model $Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i$\n\n-   The linear regression model is divided into two parts\n\n    -   The mean, or systematic, part (the \"signal\")\n\n    -   The error, or random, part (the \"noise\")\n\n-   Residuals\n\n    -   $\\hat{\\epsilon}_i = Y_i - \\left(\\hat{\\beta_0} + \\hat{\\beta_1} X_i\\right)$\n\n    -   $\\hat{{\\boldsymbol{\\epsilon}}} = {\\boldsymbol{Y}}- {\\boldsymbol{X}}\\hat{{\\boldsymbol{\\beta}}}$\n\n    -   $\\hat{{\\boldsymbol{\\beta}}} = \\left({\\boldsymbol{X}}'{\\boldsymbol{X}}\\right)^{-1} \\left({\\boldsymbol{X}}'{\\boldsymbol{Y}}\\right)$\n\n-   The mean of the residuals is $0$\n\n-   The standard deviation of the residuals is the \"Root Mean Square\n    Error\"\n\n    -   In our example analysis of BMI and gender, the RMSE is exactly\n        equal to the pooled estimate of the standard deviation from a\n        two-sample, equal variance t-test\n\n    -   In our example analysis of BMI and age, the RMSE is the square\n        root of the average variances across the age groups\n\n-   In many textbooks, $\\epsilon_i \\sim N(0, \\sigma^2)$\n\n    -   A common $\\sigma^2$ implies constant variance across all levels\n        of the grouping variable, \"homoscedasticity\"\n\n    -   Normality of the residuals is a nice property, but it is not\n        necessary (and rarely observed in practice)\n\n    -   We will discuss how lack of Normality and heteroscedasticity\n        impact statistical inference\n\n## Assumptions and Inference with Linear Regression\n\n### The Big Three Assumptions\n\n-   Most common uses of regression\n\n    -   Prediction: Estimating what a future value of $Y$ will be based\n        on observed $X$\n\n    -   Comparisons within groups: Describing the distribution of $Y$\n        across levels of the grouping variable $X$ by estimating the\n        mean $E[Y | X]$\n\n    -   Comparisons across groups: Differences appear across groups if\n        the regression parameter slope estimate $\\beta_1$ is non-zero\n\n-   Valid statistical inference (CIs, p-values) about *associations*\n    requires three general assumptions\n\n-   Assumption 1: Approximately Normal distributions for the parameter\n    estimates\n\n    -   Normal data or \"large\" N\n\n    -   It is often surprising how small \"large\" can be\n\n        -   Definition of large depends on the error distribution and\n            relative sample sizes within each group\n\n        -   With exactly Normally distributed errors, only need one\n            observation (or two to estimate a slope)\n\n        -   With very heavy tails, \"large\" can be very large\n\n        -   See Lumley, et al., Ann Rev Pub Hlth, 2002\n\n-   Assumption 2: Independence of observations\n\n    -   Classic regression: Independence of all observation (now)\n\n    -   Robust standard errors: Correlated observations within\n        identified clusters (later)\n\n-   Assumption 3: Assumption about the variance of observations within\n    groups\n\n    -   Classic regression: Homoscedasticity (equal variance across\n        groups)\n\n    -   Robust standard errors: Allows for unequal variance across\n        groups\n\n### Additional Assumptions for Additional Inference\n\n-   Note that some textbooks will claim there are more than three\n    assumptions. In truth, additional assumptions are not needed to make\n    the aforementioned statistical inference about *associations*.\n    However \\...\n\n-   Valid statistical inference (CIs, p-values) about *means responses\n    in specific groups* requires a further assumption\n\n-   Assumption 4: Adequacy of the linear model\n\n    -   If we are trying to borrow information about the mean from\n        neighboring groups, and we are assuming a straight line\n        relationship, the straight line needs to be true\n\n    -   No longer saying there is just a linear trend in the means, but\n        now need to believe that all the means lie on a straight line\n\n    -   Note that we can model transformations of the measured predictor\n\n-   For inference about *individual observations* (prediction intervals,\n    P-values) in specific groups requires another assumption\n\n-   Assumption 5: Assumptions about the distribution of the errors\n    within each group (a very strong assumption)\n\n    -   Classically: Errors have the same Normal distribution within\n        each grouping variable\n\n    -   Robust standard error will not help\n\n    -   Prediction intervals assume a common error distribution across\n        groups (homoscedasticity)\n\n-   Possible extension: Errors have the same distribution, but not\n    necessarily Normal (rarely implemented in software)\n\n    -   Bootstrapping\n\n    -   Bayesian analysis\n\n    -   Other flexible approaches\n\n#### Scenario 1: All model assumptions hold\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\nn <- 200\nregassumptions <- data.frame(x=seq(from=0, to=100, length=n))\n# Linear model correct, Normal errors\nregassumptions$y1 <- 5 + 0.2*regassumptions$x + rnorm(n,0,5)\n# Linear model correct, Skewed errors\nregassumptions$y2 <- 5 + 0.2*regassumptions$x + rlnorm(n,0,1)\n# Linear model correct, errors increasing with predictor (so increasing with Y too)\nregassumptions$y3 <- 5 + 0.2*regassumptions$x + rnorm(n,0,1+regassumptions$x*.1)\n# Linear model incorrect, Normal error\nregassumptions$y4<- 5 + 0.2*regassumptions$x + 0.02*(regassumptions$x)^2 + rnorm(n,0,5)\nggplot(regassumptions, aes(x=x, y=y1)) + geom_point() + geom_smooth(method=\"lm\")\n```\n\n::: {.cell-output-display}\n![Scatterplot with best fit line where assumptions of Normality, linearity, homoskedasticity all hold](Lec03.SimpleLinearRegression_files/figure-html/fig-assumptions-true-1.png){#fig-assumptions-true width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(gglm) # Load gglm\nmodel.y1 <- lm(y1 ~ x, data = regassumptions)\ngglm(model.y1)\n```\n\n::: {.cell-output-display}\n![Diagnostic plots where assumptions of Normality, linearity, homoskedasticity all hold](Lec03.SimpleLinearRegression_files/figure-html/fig-assumptions-true-diag-1.png){#fig-assumptions-true-diag width=672}\n:::\n:::\n\n\n-   Can estimate the mean of Y at a given value of X\n-   Can make inference about the association between X and Y (based on\n    the slope)\n-   Can predict new values of Y given X\n\n#### Scenario 2: Error distribution is not Normally distributed\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(regassumptions, aes(x=x, y=y2)) + geom_point() + geom_smooth(method=\"lm\")\n```\n\n::: {.cell-output-display}\n![Scatterplot with best fit line where the error distribution is skewed to the right](Lec03.SimpleLinearRegression_files/figure-html/fig-assumptions-error-distribution-1.png){#fig-assumptions-error-distribution width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel.y2 <- lm(y2 ~ x, data = regassumptions)\ngglm(model.y2)\n```\n\n::: {.cell-output-display}\n![Diagnostic plots where the error distribution is skewed to the right](Lec03.SimpleLinearRegression_files/figure-html/fig-assumptions-error-distribution-diag-1.png){#fig-assumptions-error-distribution-diag width=672}\n:::\n:::\n\n\n-   If the sample size is large enough, we can estimate the mean of Y at\n    a given value of X (Central Limit Theorem)\n-   If we use \"robust\" standard errors, we can make inference about the\n    association between X and Y (based on the slope)\n    -   Robust standard error relax the assumption of\n        homoskedasticity/common variance\n-   We *cannot* predict new values of Y given X. This requires the\n    correct error distribution.\n\n#### Scenario 3: Variance increasing with predictor and expected value of Y (heteroskedasticity)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(regassumptions, aes(x=x, y=y3)) + geom_point() + geom_smooth(method=\"lm\")\n```\n\n::: {.cell-output-display}\n![Scatterplot with best fit line where the error distribution variance is increasing](Lec03.SimpleLinearRegression_files/figure-html/fig-assumptions-heteroskedasticity-1.png){#fig-assumptions-heteroskedasticity width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel.y3 <- lm(y3 ~ x, data = regassumptions)\ngglm(model.y3)\n```\n\n::: {.cell-output-display}\n![Diagnostic plot where the error distribution variance is increasing](Lec03.SimpleLinearRegression_files/figure-html/fig-assumptions-heteroskedasticity-diag-1.png){#fig-assumptions-heteroskedasticity-diag width=672}\n:::\n:::\n\n\n-   If the sample size is large enough, we can estimate the mean of Y at\n    a given value of X (Central Limit Theorem)\n-   If we use \"robust\" standard errors, we can make inference about the\n    association between X and Y (based on the slope)\n    -   Robust standard error relax the assumption of\n        homoskedasticity/common variance\n-   We *cannot* predict new values of Y given X. This requires the\n    correct error distribution (and mean model, but that is correct)\n\n#### Scenario 4: Mean model incorrectly specified\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(regassumptions, aes(x=x, y=y4)) + geom_point() + geom_smooth(method=\"lm\")\n```\n\n::: {.cell-output-display}\n![Scatterplot with best fit line where the mean model is not correctly specified](Lec03.SimpleLinearRegression_files/figure-html/fig-assumptions-mean-incorrect-1.png){#fig-assumptions-mean-incorrect width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel.y4 <- lm(y4 ~ x, data = regassumptions)\ngglm(model.y4)\n```\n\n::: {.cell-output-display}\n![Diagnostic plots where the mean model is not correctly specified](Lec03.SimpleLinearRegression_files/figure-html/fig-assumptions-mean-incorrect-diag-1.png){#fig-assumptions-mean-incorrect-diag width=672}\n:::\n:::\n\n\n-   Because our model is wrong, we *cannot* estimate the mean of Y at a\n    given value of X. It will be biased, even if the sample size is\n    large, the model will still be incorrect\n    -   The good news is that we can fix our mean model to more flexibly\n        model the association.\n        -   Here, we have just included a single slope term to model the\n            association with $X$, $E[Y|X] = \\beta_0 + \\beta_1*X$.\n        -   The correct model in this simple example is\n            $E[Y|X] = \\beta_0 + \\beta_1*X + \\beta_2*X^2$. Note that this\n            model is still a \"linear model\". A \"linear model\" is linear\n            *in the parameters* ($\\beta_0$, $\\beta_1$, $beta_2$). We can\n            included the quadratic *predictor* $X^2$ in the model and\n            still have a model that is linear in the parameters.\n-   If we use \"robust\" standard errors, we can make inference about the\n    association between X and Y (based on the slope, $\\beta_1$)\n    -   Robust standard error relax the assumption of homoskedasticity\n    -   While the model is not a great fit, $\\beta_1$ may summarize a\n        first-order trend\n-   We *cannot* predict new values of Y given X. This requires the\n    correct mean model and error distribution.\n\n#### Summary\n\n-   Regression based inference about *associations* is far more\n    trustworthy than estimation of group means or individual\n    predictions.\n\n-   There is much to be gained by using robust variance estimates\n\n::: {#tbl-panel layout-ncol=\"2\"}\n| Linearity | Homoskedasticity | Normality |\n|-----------|------------------|-----------|\n| Y         | Y                | Y         |\n| Y         | Y                | N         |\n| Y         | N                | Y         |\n| Y         | N                | N         |\n| N         | Y                | Y         |\n| N         | Y                | N         |\n| N         | N                | Y         |\n| N         | N                | N         |\n\n: Assumption Holds (Yes/No)\n\n+---------------------+---------------------+---------------------+\n| $\\hat{\\beta}$       | $                   | $\\hat{\\textrm{V a   |\n|                     | \\hat{\\textrm{Var} } | r}}_R(\\hat{\\beta})$ |\n|                     | _{NR}(\\hat{\\beta})$ |                     |\n+=====================+=====================+=====================+\n| Y                   | Y                   | Y                   |\n+---------------------+---------------------+---------------------+\n| Y                   | M^2^                | M^2^                |\n+---------------------+---------------------+---------------------+\n| Y                   | N                   | Y                   |\n+---------------------+---------------------+---------------------+\n| Y                   | N                   | M^2^                |\n+---------------------+---------------------+---------------------+\n| M^1^                | N                   | M^3^                |\n+---------------------+---------------------+---------------------+\n| M^1^                | N                   | M^2,3^              |\n+---------------------+---------------------+---------------------+\n| M^1^                | N                   | M^2^                |\n+---------------------+---------------------+---------------------+\n| M^1^                | N                   | M^2,3^              |\n+---------------------+---------------------+---------------------+\n\n: Valid Inference (Yes/No/Maybe)\n\nSummary of what is required for valid estimates of $\\beta$ and the\nvariance of beta using the classical, non-robust (NR) and robust (R)\napproaches. The classic (non-robust) variance is\n$\\hat{\\textrm{Var}}_{NR}(\\hat{\\beta})$ and the robust variance is\n$\\hat{\\textrm{Var}}_R(\\hat{\\beta})$\n:::\n\n-   ^2^ Approximately valid under large sample sizes\n-   ^3^ If $X$ is random, but not if $X$ is fixed\n\n### Interpreting \"Positive\" Results\n\n-   Slope is statistically different from 0 using robust standard errors\n\n-   Observed data is atypical of a setting with no linear trend in mean\n    response across groups\n\n-   Data suggests evidence of a trend toward larger (or smaller) means\n    in groups having larger values of the predictor\n\n-   (To the extent the data appears linear, estimates of the group means\n    will be reliable)\n\n### Interpreting \"Negative\" Results\n\n-   Many possible reasons why the slope is not statistically different\n    from 0 using robust standard errors\n\n-   There may be no association between the response and predictor\n\n-   There may be an association, but not in the parameter considered\n    (the mean response)\n\n-   There may be an association in the parameter considered, but the\n    best fitting line has zero slope\n\n-   There may be a first order trend in the parameter considered, but we\n    lacked the precision to be confident that it truly exists (a type II\n    error)\n\n## Model Checking\n\n-   Much statistical literature has been devoted to methods for checking\n    the assumptions for regression models\n\n-   My philosophy: Model checking is generally fraught with peril as it\n    necessarily involves multiple comparisons\n\n    -   We cannot reliably use the sampled data to assess whether it\n        accurately portrays the population\n\n    -   We are more worried about the data from the population that we\n        might not have sampled\n\n    -   It is not so much the abnormal points that we see, but the ones\n        that are hiding in the population that will make our model\n        perform badly\n\n    -   But, do tend to worry more when we see a tendency to extreme\n        outliers in the sample or clear departures from model\n        assumptions\n\n    -   If we over-check our model and make adjustments to the model\n        based on our observed data, we will inflate the type I error\n        rate (i.e. will be more likely to claim statistical significance\n        when it doesn't really exist). We run the risk of creating a\n        model that fits our data well but does not generalize.\n\n        -   Estimates are biased away from the null\n\n        -   Reported standard errors are too small\n\n        -   If we fish through the data, we will always find significant\n            results\n\n        -   In clinical trials, often Phase II results are not able to\n            be replicated in Phase III trials\n\n-   Instead of extensive model checking, go back to our choices of\n    inference when planning our analysis\n\n    -   Best to plan for unusual data\n\n    -   There is often little to be lost and much to be gained by using\n        the robust standard error estimates\n\n    -   By using robust errors, avoids much of the need for model\n        checking\n\n    -   Model checking is almost entirely data driven\n\n    -   Robust standard errors is a more logical scientific approach\n\n        -   Minimize the need to presume more knowledge than the\n            question we are trying to answer\n\n        -   E.g., if we don't know how the means might differ, why\n            should we presume to know how the variances or the shape of\n            the distribution might behave?\n\n### Residuals versus fitted values\n\n-   Plot of $\\hat{\\epsilon_i}$ versus $\\hat{y_i}$\n\n-   If assumptions hold, should be a random pattern about zero. See\n    @fig-assumptions-true-diag for example.\n\n-   If you have *a priori* concerns about non-constant variance, this is\n    one potential check\n\n    -   E.g., it is fairly common for uncertainty to increase with\n        outcome. See @fig-assumptions-heteroskedasticity-diag for the\n        resulting \"fan\" pattern that would appear\n\n### Scale-location plot\n\n-   The scale-location plot is very similar to residuals vs fitted\n    values, and is used to evaluate the homoskedasticity assumption\n\n-   It uses the square root of the absolute value of standardized\n    residuals instead of plotting the residuals themselves\n\n-   We want to check two things\n\n    -   That the best fit line is approximately horizontal. If it is,\n        then the average magnitude of the standardized residuals isn't\n        changing much as a function of the fitted values.\n\n    -   That the spread around the fit line doesn't vary with the fitted\n        values. If so, then the variability of magnitudes doesn't vary\n        much as a function of the fitted values.\n\n### QQ plot\n\n-   Used to evaluate Normality of the residuals\n\n    -   If assumptions hold, points should lie on a 1:1 line\n\n-   Plot of standardized residuals versus theoretical quantities from a\n    N(0,1) distribution\n\n    -   If assumptions hold, the standardized residuals should be a\n        random sample from a N(0,1) distribution\n\n    -   We can use quantiles of the Normal distribution to check how\n        closely the observed matches the expected\n\n        -   The 50th percentile of the residuals should be\n            0\n        -   The 75th percentile of the residuals should be\n            0.67\n        -   The 90th percentile of the residuals should be\n            1.28\n        -   The 95th percentile of the residuals should be\n            1.64\n        -   The 97.5th percentil of the residuals should be\n            1.96\n\n-   Similar idea would be a histogram or density plot of the\n    (standardized) residuals that could be visually evaluated for\n    Normality\n\n### Residuals versus leverage\n\n-   Used to check for outliers\n\n-   Plot can help us to identify influential observations, if there are\n    any\n\n-   Influence differs from leverage. Not all outliers influence the\n    regression coefficient estimates\n\n## Linear Regression with Robust Standard Error estimates\n\n-   There are several libraries available in R for fitting models with\n    robust error estimates\n\n-   There are also several different flavors of robust estimators\n\n    -   For now, we are going to consider the default \"Huber-White\n        sandwich estimator\"\n\n    -   In Stata, the Huber-White robust estimate of the standard error\n        can be obtained using the 'robust' option\n\n        ``` stata\n        regress chol age, robust\n        ```\n\n-   In the following examples I provide estimates using classical linear\n    regression and linear regression estimate using robust standard\n    errors. For each of these example compare\n\n    -   Estimates of the intercept, $\\hat{\\beta_0}$\n\n    -   Estimates of the slope, $\\hat{\\beta_1}$\n\n    -   Estimates of the standard errors,\n        $\\hat{\\textrm{se}}(\\hat{\\beta_0})$ and\n        $\\hat{\\textrm{se}}(\\hat{\\beta_1})$\n\n### Comparison when Normality, constant variance holds\n\n#### Age and Cholesterol\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit.ols <- ols(chol ~ age, data=plotdata, x=TRUE)\nfit.ols\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear Regression Model\n \n ols(formula = chol ~ age, data = plotdata, x = TRUE)\n \n                  Model Likelihood    Discrimination    \n                        Ratio Test           Indexes    \n Obs      301    LR chi2      5.43    R2       0.018    \n sigma15.3405    d.f.            1    R2 adj   0.015    \n d.f.     299    Pr(> chi2) 0.0198    g        2.344    \n \n Residuals\n \n     Min      1Q  Median      3Q     Max \n -39.591 -10.524  -0.234  11.154  41.819 \n \n \n           Coef     S.E.    t     Pr(>|t|)\n Intercept 203.2259 10.3138 19.70 <0.0001 \n age         0.3209  0.1375  2.33 0.0203  \n \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrobcov(fit.ols)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear Regression Model\n \n ols(formula = chol ~ age, data = plotdata, x = TRUE)\n \n                  Model Likelihood    Discrimination    \n                        Ratio Test           Indexes    \n Obs      301    LR chi2      5.43    R2       0.018    \n sigma15.3405    d.f.            1    R2 adj   0.015    \n d.f.     299    Pr(> chi2) 0.0198    g        2.344    \n \n Residuals\n \n     Min      1Q  Median      3Q     Max \n -39.591 -10.524  -0.234  11.154  41.819 \n \n \n           Coef     S.E.    t     Pr(>|t|)\n Intercept 203.2259 10.3790 19.58 <0.0001 \n age         0.3209  0.1378  2.33 0.0205  \n \n```\n:::\n:::\n\n\n#### Scenario 1 dataset\n\n\n::: {.cell}\n\n```{.r .cell-code}\nols(y1 ~ x, data=regassumptions, x=TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear Regression Model\n \n ols(formula = y1 ~ x, data = regassumptions, x = TRUE)\n \n                 Model Likelihood    Discrimination    \n                       Ratio Test           Indexes    \n Obs     200    LR chi2    200.34    R2       0.633    \n sigma5.0531    d.f.            1    R2 adj   0.631    \n d.f.    198    Pr(> chi2) 0.0000    g        7.659    \n \n Residuals\n \n      Min       1Q   Median       3Q      Max \n -15.2533  -3.2042  -0.4963   3.0693  14.4377 \n \n \n           Coef   S.E.   t     Pr(>|t|)\n Intercept 3.3375 0.7119  4.69 <0.0001 \n x         0.2275 0.0123 18.47 <0.0001 \n \n```\n:::\n\n```{.r .cell-code}\nrobcov(ols(y1 ~ x, data=regassumptions, x=TRUE))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear Regression Model\n \n ols(formula = y1 ~ x, data = regassumptions, x = TRUE)\n \n                 Model Likelihood    Discrimination    \n                       Ratio Test           Indexes    \n Obs     200    LR chi2    200.34    R2       0.633    \n sigma5.0531    d.f.            1    R2 adj   0.631    \n d.f.    198    Pr(> chi2) 0.0000    g        7.659    \n \n Residuals\n \n      Min       1Q   Median       3Q      Max \n -15.2533  -3.2042  -0.4963   3.0693  14.4377 \n \n \n           Coef   S.E.   t     Pr(>|t|)\n Intercept 3.3375 0.7002  4.77 <0.0001 \n x         0.2275 0.0132 17.20 <0.0001 \n \n```\n:::\n:::\n\n\n### Comparison when variance is increasing with predictor (Scenario 3)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nols(y3 ~ x, data=regassumptions, x=TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear Regression Model\n \n ols(formula = y3 ~ x, data = regassumptions, x = TRUE)\n \n                 Model Likelihood    Discrimination    \n                       Ratio Test           Indexes    \n Obs     200    LR chi2     95.93    R2       0.381    \n sigma6.5232    d.f.            1    R2 adj   0.378    \n d.f.    198    Pr(> chi2) 0.0000    g        5.909    \n \n Residuals\n \n      Min       1Q   Median       3Q      Max \n -24.1947  -3.7861  -0.1145   4.1816  17.2286 \n \n \n           Coef   S.E.   t     Pr(>|t|)\n Intercept 5.6640 0.9191  6.16 <0.0001 \n x         0.1755 0.0159 11.04 <0.0001 \n \n```\n:::\n\n```{.r .cell-code}\nrobcov(ols(y3 ~ x, data=regassumptions, x=TRUE))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear Regression Model\n \n ols(formula = y3 ~ x, data = regassumptions, x = TRUE)\n \n                 Model Likelihood    Discrimination    \n                       Ratio Test           Indexes    \n Obs     200    LR chi2     95.93    R2       0.381    \n sigma6.5232    d.f.            1    R2 adj   0.378    \n d.f.    198    Pr(> chi2) 0.0000    g        5.909    \n \n Residuals\n \n      Min       1Q   Median       3Q      Max \n -24.1947  -3.7861  -0.1145   4.1816  17.2286 \n \n \n           Coef   S.E.   t     Pr(>|t|)\n Intercept 5.6640 0.5955  9.51 <0.0001 \n x         0.1755 0.0153 11.49 <0.0001 \n \n```\n:::\n:::\n\n\n### Comparison when error variance is skewed to the right (Scenario 2)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nols(y2 ~ x, data=regassumptions, x=TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear Regression Model\n \n ols(formula = y2 ~ x, data = regassumptions, x = TRUE)\n \n                 Model Likelihood    Discrimination    \n                       Ratio Test           Indexes    \n Obs     200    LR chi2    431.83    R2       0.885    \n sigma2.0635    d.f.            1    R2 adj   0.884    \n d.f.    198    Pr(> chi2) 0.0000    g        6.596    \n \n Residuals\n \n     Min      1Q  Median      3Q     Max \n -1.8206 -1.1341 -0.6103  0.1731 16.6448 \n \n \n           Coef   S.E.   t     Pr(>|t|)\n Intercept 6.9339 0.2907 23.85 <0.0001 \n x         0.1959 0.0050 38.95 <0.0001 \n \n```\n:::\n\n```{.r .cell-code}\nrobcov(ols(y2 ~ x, data=regassumptions, x=TRUE))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear Regression Model\n \n ols(formula = y2 ~ x, data = regassumptions, x = TRUE)\n \n                 Model Likelihood    Discrimination    \n                       Ratio Test           Indexes    \n Obs     200    LR chi2    431.83    R2       0.885    \n sigma2.0635    d.f.            1    R2 adj   0.884    \n d.f.    198    Pr(> chi2) 0.0000    g        6.596    \n \n Residuals\n \n     Min      1Q  Median      3Q     Max \n -1.8206 -1.1341 -0.6103  0.1731 16.6448 \n \n \n           Coef   S.E.   t     Pr(>|t|)\n Intercept 6.9339 0.3526 19.67 <0.0001 \n x         0.1959 0.0056 35.18 <0.0001 \n \n```\n:::\n:::\n\n\n### Comments\n\n\n```{=tex}\n\\newcommand\\seb0{{\\hat{\\textrm{se}}(\\hat{\\beta_0})}}\n\\newcommand\\seb1{\\hat{\\textrm{se}}(\\hat{\\beta_1})}\n```\n\n+-------------------------+------------+------------+------------+------------+\n| Model                   | Classical  | Robust     | Classical  | Robust    |\n|                         | Intercept  | Intercept  | Slope      | Slope      |\n+=========================+============+============+============+============+\n| Age - Cholesterol       |            |            |            |            |\n|                         |            |            |            |            |\n| Assumptions hold        |            |            |            |            |\n+-------------------------+------------+------------+------------+------------+\n| Scenario 1              |            |            |            |            |\n|                         |            |            |            |            |\n| Assumptions hold        |            |            |            |            |\n+-------------------------+------------+------------+------------+------------+\n| Scenario 3              |            |            |            |            |\n|                         |            |            |            |            |\n| Increasing variance     |            |            |            |            |\n+-------------------------+------------+------------+------------+------------+\n| Scenario 2              |            |            |            |            |\n|                         |            |            |            |            |\n| Skewed variance         |            |            |            |            |\n+-------------------------+------------+------------+------------+------------+\n\n: Comparison of standard error estimates\n\n-   Point estimates $\\hat{\\beta_0}$ and $\\hat{\\beta_1}$ were the same\n    for robust and classical standard error\n\n-   Point estimates \\$\\\\hat{\\\\textrm{se}}(\\\\hat{\\\\beta_0})\\$ and\n    \\$\\\\hat{\\\\textrm{se}}(\\\\hat{\\\\beta_1})\\$ were different for robust\n    and classical standard error\n\n    -   In general, it is hard to predict the effect of using robust\n        standard errors\n\n    -   These were just a a couple of examples and do not constitute a\n        thorough examination of the impact of using robust standard\n        errors\n\n-   When the model assumptions hold, for these two scenarios, robust\n    standard error estimates were larger than classical standard error\n    estimates\n\n    -   While this is not unexpected, to formally evaluate the amount of\n        increase would require a statistical simulation\n\n        -   Simulate many datasets where the Normality and constant\n            variance assumption holds\n\n        -   For each dataset, estimate\n            $\\hat{\\textrm{se}}(\\hat{\\beta_0})$ and\n            $\\hat{\\textrm{se}}(\\hat{\\beta_1})$\n\n        -   Summarize the results\n\n        -   Approach will be covered in a lab\n\n## Linear Regression and Correlation\n\n### Pearson's Correlation\n\n-   Notation\n\n    -   $\\rho$ signifies the population value\n\n    -   $r$ (or $\\hat{\\rho}$) is the estimated correlation from data\n\n-   Formula\n\n    -   $r = \\frac{\\Sigma(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\Sigma(x_i - \\bar{x})^2\\Sigma(y_i - \\bar{y})^2}}$\n\n    -   Range: $-1 \\leq r \\leq 1$\n\n-   Interpretation\n\n    -   Measures the linear relationship between $X$ and $Y$\n\n    -   Correlation coefficient is a unitless index of strength of\n        association between two variables (+ = positive association, - =\n        negative, 0 = no association)\n\n    -   Can test for significant association by testing whether the\n        population correlation is zero t = which is identical to the\n        $t$-test used to test whether the population $r$ is zero;\n        $\\textrm{d.f.} = n-2$\n\n    -   Use probability calculator for $t$ distribution to get a\n        2-tailed $P$-value\n\n    -   Confidence intervals for population $r$ calculated using\n        Fisher's $Z$ transformation\n\n    $$Z = \\frac{1}{2} \\textrm{log}_\\textrm{e} \\left( \\frac{1+r}{1-r} \\right)$$\n\n    -   For large $n$, $Z$ follows a Normal distribution with standard\n        error $\\frac{1}{\\sqrt{n-3}}$\n\n    -   To calculate a confidence interval for $r$, first find the\n        confidence interval for $Z$ then transform back to the $r$ scale\n\n$$\\begin{aligned}\n Z & = & \\frac{1}{2} \\textrm{log}_\\textrm{e} \\left( \\frac{1+r}{1-r} \\right) \\\\\n 2*Z & = & \\textrm{log}_\\textrm{e} \\left( \\frac{1+r}{1-r} \\right) \\\\\n \\textrm{exp}(2*Z) & = & \\left( \\frac{1+r}{1-r} \\right) \\\\\n \\textrm{exp}(2*Z) * (1-r) & = & 1 + r \\\\\n \\textrm{exp}(2*Z) - r * \\textrm{exp}(2*Z) & = & 1 + r \\\\\n \\textrm{exp}(2*Z) - 1 & = & r * \\textrm{exp}(2*Z) + r \\\\\n \\textrm{exp}(2*Z) - 1 & = & r \\left(\\textrm{exp}(2*Z) + 1\\right) \\\\\n \\frac{\\textrm{exp}(2*Z) - 1}{\\textrm{exp}(2*Z) + 1} & = & r \\\\\n\\end{aligned}$$\n\n-   Example (Altman 89-90): Pearson's $r$ for a study investigating the\n    association of basal metabolic rate with total energy expenditure\n    was calculated to be $0.7283$ in a study of $13$ women. Derive a 95%\n    confidence interval for $r$\n\n    $$Z = \\frac{1}{2} \\textrm{log}_\\textrm{e} \\left( \\frac{1+r}{1-r} \\right) = 0.9251$$\n\n-   The lower limit of a 95% CI for $Z$ is given by\n    $0.9251 + 1.96*\\frac{1}{\\sqrt{n-3}} = 0.3053$\n\n-   The upper limit is $0.9251 + 1.96*\\frac{1}{\\sqrt{n-3}} = 1.545$\n\n-   A 95% CI for the population correlation coefficient is given by\n    transforming these limits from the $Z$ scale back to the $r$ scale.\n\n$$\\frac{\\textrm{exp}(2*0.3053) - 1}{\\textrm{exp}(2*0.3053) + 1} \\hspace{.5cm} \\textrm{to} \\hspace{.5cm}  \\frac{\\textrm{exp}(2*1.545) - 1}{\\textrm{exp}(2*1.545) + 1}$$\n\n-   Which gives a 95% CI from 0.30 to 0.91 for the population\n    correlation\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfisher.z <- function(r) {.5 * log((1+r)/(1-r))}\nfisher.z.inv <- function(z) {(exp(2*z)-1) / (exp(2*z)+1)}\n\nfisher.z.se <- function(n) {1/sqrt(n-3)}\nfisher.z.inv(fisher.z(0.7283)) #Should be original value\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.7283\n```\n:::\n\n```{.r .cell-code}\nfisher.z(0.7283)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9250975\n```\n:::\n\n```{.r .cell-code}\nfisher.z(0.7283)-1.96*fisher.z.se(13)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.3052911\n```\n:::\n\n```{.r .cell-code}\nfisher.z(0.7283)+1.96*fisher.z.se(13)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.544904\n```\n:::\n\n```{.r .cell-code}\nfisher.z.inv(fisher.z(0.7283)+c(-1.96, 1.96)*fisher.z.se(13))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.2961472 0.9129407\n```\n:::\n:::\n\n\n### Relationship to regression\n\n-   Pearson's correlation ($\\rho$) is directly related to linear\n    regression\n\n-   Correlation treats $Y$ and $X$ symmetrically, but we can relate\n\n    -   $E[Y | X]$ as a function of $X$\n\n    -   $E[Y | X] = \\beta_0 + \\beta_1 X$\n\n    -   $\\beta_1 = \\rho \\frac{\\sigma_Y}{\\sigma_X}$\n\n    -   $E[Y | X]$: mean $Y$ withing groups having equal $X$\n\n    -   $\\beta_1$: difference in mean $Y$ per 1 unit difference in $X$\n\n    -   $\\rho$: true correlation between $Y$ and $X$\n\n    -   $\\sigma_Y$: standard deviation of $Y$\n\n    -   $\\sigma_X$: standard deviation of $X$\n\n-   More interpretable formulation of $\\rho$\n\n    -   $\\rho \\approx \\beta \\sqrt{\\frac{\\textrm{Var}(X)}{\\beta^2\\textrm{Var}(X) + \\textrm{Var}(Y | X = x)}}$\n\n    -   $\\beta$: slope between $Y$ and $X$\n\n    -   $\\textrm{Var}(X)$: variance of $X$ in the sample\n\n    -   $\\textrm{Var}(Y | X = x)$: variance of $Y$ in groups having the\n        same value of $X$ (the vertical spread of data)\n\n-   Correlation tends to increase in absolute value as\n\n    -   The absolute value of the slope of the line increases\n\n    -   The variance of data decreases within groups that share a common\n        value of $X$\n\n    -   The variance of $X$ increases\n\n-   Scientific uses of correlation\n\n    -   The slope between $X$ and $Y$ is of scientific interest\n\n    -   The variance of $Y$ given $X=x$ is partly of scientific\n        interest, but can also be impacted by restricting sampling to\n        certain values of another variable\n\n        -   The variance of height given age is less if sampling just\n            males than if sampling both sexes\n\n        -   The variance of $X$ is often set by study design (which is\n            often not of scientific interest)\n\n    -   Hypothesis tests for a nonzero correlation are exactly the same\n        as a test for a nonzero slope in classical linear regression\n\n    -   The statistical significance of a given value of $\\rho$ depends\n        only on the sample size\n\n-   Correlation is more of a statistical measure than a scientific\n    measure\n\n## Regression on Geometric Means: Log transformations\n\n-   Geometric means of distributions are typically analyzed by using\n    linear regression on the log transformed outcome\n\n-   Used for inference under the following circumstances\n\n    -   We are (scientifically) interested in multiplicative rather than\n        additive models\n\n    -   We want to down-weight the impact of outliers\n\n    -   The standard deviation of the response in a groups is\n        proportional to the mean\n\n        -   \"Error is $\\pm 10\\%$\" instead of \"Error is $\\pm 10$\"\n\n### Interpretation of Parameters\n\n-   Linear regression on the log transformed $Y$\n\n-   Note that in statistics $\\textrm{log}$ almost always refers the the\n    natural log ($\\textrm{log}_e$)\n\n-   Model: $E[\\textrm{log} Y_i | X_i] = \\beta_0 + \\beta_1 \\times X_i$\n\n    -   When $X_i = 0$: $E[\\textrm{log} Y_i | X_i = 0] = \\beta_0$\n\n    -   When $X_i = x$:\n        $E[\\textrm{log} Y_i | X_i = x] = \\beta_0 + \\beta_1 \\times x$\n\n    -   When $X_i = x+1$:\n        $E[\\textrm{log} Y_i | X_i = x + 1] = \\beta_0 + \\beta_1 \\times x + \\beta_1$\n\n    -   Restate the model as log link for geometric mean\n\n    -   Model:\n        $\\textrm{log GM}[Y_i | X_i] = \\beta_0 + \\beta_1 \\times X_i$\n\n    -   When $X_i = 0$: $\\textrm{log GM}[Y_i | X_i = 0] = \\beta_0$\n\n    -   When $X_i = x$:\n        $\\textrm{log GM}[Y_i | X_i = x] = \\beta_0 + \\beta_1 \\times x$\n\n    -   When $X_i = x+1$:\n        $\\textrm{log GM}[Y_i | X_i = x + 1] = \\beta_0 + \\beta_1 \\times x + \\beta_1$\n\n-   Interpretation of parameters by back-transforming model\n\n    -   Model:\n        $\\textrm{GM}[Y_i | X_i] = e^{\\beta_0} \\times e^{\\beta_1 \\times X_i}$\n\n    -   When $X_i = 0$: $\\textrm{GM}[Y_i | X_i = 0] = e^{\\beta_0}$\n\n    -   When $X_i = x$:\n        $\\textrm{GM}[Y_i | X_i = x] = e^{\\beta_0} \\times e^{\\beta_1 \\times x}$\n\n    -   When $X_i = x+1$:\n        $\\textrm{GM}[Y_i | X_i = x + 1] = e^{\\beta_0} \\times e^{\\beta_1 \\times x} \\times e^{\\beta_1}$\n\n### Example: FEV with Height\n\n-   Question: How does FEV differ across height groups?\n\n-   Scientific justification for summarizing with geometric mean\n\n    -   FEV is a volume\n\n    -   Height is a linear dimension\n\n    -   Each dimension of lung size is proportional to height\n\n    -   Standard deviation is likely proportional to height\n\n    -   Science: $FEV \\propto Height^3$ or\n        $\\sqrt[3]{FEV} \\propto Height$\n\n-   Statistics: $\\textrm{log}(FEV) \\propto 3 \\textrm{ log}(Height)$\n\n    -   Science dictates model choice\n\n-   Statistical preference for (any) transformations of response\n\n    -   May transform to equal variance across groups\n\n    -   Homoscedasticity allows for easier inference\n\n-   Statistical preference for a log transformation\n\n    -   Easier interpretation: multiplicative model\n\n    -   Compare groups using ratios\n\n### Log Transformed Predictors\n\n-   Interpretation of log predictor and log link for response\n\n-   Log link used to model the geometric mean of $Y$\n\n-   Exponentiated slope estimates ratio of geometric means across groups\n\n-   Log transformation of predictor compares groups with a $k$-fold\n    difference in there measured predictors\n\n-   Estimated ratio of geometric means\n\n    -   $\\exp (\\textrm{log}(k) \\times \\beta_1) = k^{\\beta_1}$\n\n-   Interpretation of model\n\n    -   $\\textrm{log GM}[FEV_i | log (height_i)] = -11.9 + 3.12 \\times log(height_i)$\n\n-   Estimate ratio of geometric mean FEV for two groups differing by\n    $10\\%$ in height (a 1.1 fold difference in height)\n\n-   To find: Exponentiate $1.1$ to the slope: $1.1^{3.12} = 1.35$\n\n    -   \"Group that is 10% taller is estimated to have a geometric mean\n        FEV that is 1.35 times higher (35% higher)\"\n\n-   Why transform the predictor?\n\n    -   Typically chosen according to whether the data are likely to\n        follow a straight line relationship\n\n    -   Linearity necessary to predict the values of the parameter in\n        individual groups\n\n    -   Linearity is not necessary to estimate existence of an\n        association\n\n    -   Linearity is not necessary to estimate a first order trend in\n        the parameter across groups having the sampled distribution of\n        the predictor\n\n    -   (Inference about these two questions will tend to be\n        conservative if linearity does not hold)\n\n    -   Rarely do we know which transformation of the predictor provides\n        the best linear fit\n\n    -   As always, there is significant danger in using the data to\n        estimate the best transformation to use\n\n    -   If there is no association of any kind between the response and\n        the predictor, a linear fit (with a zero slope) is correct\n\n    -   Trying to detect a transformation is thus an informal test for\n        an association (multiple testing, which inflates type I error\n        rate)\n\n    -   It is best to choose transformations of the predictor on\n        scientific grounds\n\n    -   In practice, it is often true that mean functions are well\n        approximated by a straight line over a small range of the data\n\n-   Example: In the FEV dataset, the logarithm of height is\n    approximately linear over the range of heights sampled (look at the\n    plot)\n\n-   We can often choose untransformed predictors when science would\n    suggest a non-linear association\n\n-   Can have major advantages in interpreting the results of the\n    analysis\n\n-   In the FEV example, it is far more natural to compare heights by\n    difference than by ratios\n\n    -   We would rather characterize two children as differing by 4\n        inches in height rather than as the 44 inch child being 10%\n        taller than the 40 inch child\n\n### Looking Ahead to Multivariable Models\n\n-   The relative importance of having the \"true\" transformation for a\n    predictor depends on the statistical role\n\n    -   Predictor of Interest\n\n    -   Effect Modifiers\n\n    -   Confounders\n\n    -   Precision variables\n\n-   Earlier I made a comment about treating these variables different in\n    an analysis\n\n    -   Predictor of Interest (should be pre-specified)\n\n    -   In general, don't worry about modeling the exact association\n        until you have even established an association (binary search)\n\n    -   Need to have a prespecified model for the POI\n\n-   Searching for the best fit can inflate the type I error\n\n-   Make most accurate, precise inference about the presence of the\n    association first\n\n-   Exploratory models can suggest models for future analyses\n\n-   Effect Modifiers (difficult to do right anyway)\n\n    -   Modeling of effect modifiers is invariably just a test for the\n        existence of an interaction\n\n    -   We rarely have a lot of precision to answer questions in\n        subgroups of the data\n\n    -   Patterns of interaction can be so complex that it is unlikely\n        that we will really capture the interactions across all\n        subgroups in a single model\n\n    -   Typically, we would restrict future studies to analyses treating\n        subgroups separately\n\n-   Confounders (matters the most)\n\n    -   It is important to have an appropriate model of the association\n        between the confounder and the response\n\n    -   Failure to accurately model the confounder means that some\n        residual confounding will exist\n\n    -   However, searching for the best model may inflate the type I\n        error for inference about the predictor of interest by\n        overstating the precision of the study\n\n    -   Luckily, we rarely care about inference for the confounder, so\n        we are free to use inefficient means of adjustment (e.g. a\n        stratified analysis)\n\n-   Precision Variables\n\n    -   When modeling precision variables, it is rarely worth the effort\n        to use the \"best\" transformation\n\n    -   We usually capture the largest part of the added precision using\n        crude models\n\n    -   We generally do not care about estimating association between\n        the response and the precision variable\n\n    -   Most often, precision variables represent known effects on the\n        response\n",
    "supporting": [
      "Lec03.SimpleLinearRegression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}