{
  "hash": "445e173b19d66f5210629ec10790b91e",
  "result": {
    "markdown": "---\ntitle: \"Logistic Regression\"\nsubtitle: \"Lecture 04\"\nauthor: \"Chris Slaughter\"\nfooter: \"Bios 6312\"\nformat:\n  html:\n    self-contained: true\n    number-sections: true\n    number-depth: 4\n    anchor-sections: true\n    smooth-scroll: true\n    theme: journal\n    toc: true\n    toc-depth: 4\n    toc-title: Contents\n    toc-location: left\n    code-link: false\n    code-tools: true\n    code-fold: true\n    code-block-bg: \"#f1f3f5\"\n    code-block-border-left: \"#31BAE9\"\n    reference-location: margin\n    fig-cap-location: margin\n    fontsize: medium\nexecute:\n   warning: false\n   message: false\neditor: \n  markdown: \n    wrap: 72\n---\n\n\n## General Regression Setting\n\n-   Types of variables\n\n    -   Binary data: e.g. sex, death\n\n    -   Nominal (unordered categorical) data: e.g. race, martial status\n\n    -   Ordinal (ordered categorical data): e.g. cancer stage, asthma\n        severity\n\n    -   Quantitative data: e.g. age, blood pressure\n\n    -   Right censored data: e.g. time to death\n\n-   The measures used to summarize and compare distributions vary\n    according to the type of variable\n\n    -   Means: Binary, quantitative\n\n    -   Medians: Ordered, quantitative, censored\n\n    -   Proportions: Binary, nominal, ordinal\n\n    -   Odds: Binary, nominal, ordinal\n\n    -   Hazards: Censored\n\n-   Which regression model you choose to use is based on the parameter\n    being compared across groups\n\n::: center\n| Parameter       | Approach                              |\n|:----------------|:--------------------------------------|\n| Means           | Linear regression                     |\n| Geometric means | Linear regression on log scale        |\n| Odds            | Logistic regression                   |\n| Rates           | Poisson regression                    |\n| Hazards         | Proportional Hazards (Cox) regression |\n:::\n\n-   General notation for variables and parameters\n\n::: center\n|            |                                                      |\n|:-----------|:-----------------------------------------------------|\n| $Y_i$      | Response measured on the $i$th subject               |\n| $X_i$      | Value of the predictor measured on the $i$th subject |\n| $\\theta_i$ | Parameter summarizing distribution of $Y_i | X_i$    |\n:::\n\n-   The parameter ($\\theta_i$) might be the mean, geometric mean, odds,\n    rate, instantaneous risk of an event (hazard), etc.\n\n-   In linear regression on means, $\\theta_i = E[Y_i | X_i]$\n\n-   Choice of correct $\\theta_i$ should be based on scientific\n    understanding of problem\n\n-   General notation for simple regression model\n\n$$g(\\theta_i) = \\beta_0 + \\beta_1 \\times X_i$$\n\n::: center\nGeneral notation for regression model with one predictor\n\n|           |                                 |\n|:----------|:--------------------------------|\n| $g( )$    | Link function used for modeling |\n| $\\beta_0$ | Intercept                       |\n| $\\beta_1$ | Slope for predictor $X$         |\n:::\n\n-   The link function is often either the identity function (for\n    modeling means) or log (for modeling geometric means, odds, hazards)\n\n    -   Identity function: $f(x) = x$\n\n### Uses of General Regression\n\n-   Borrowing information\n\n    -   Use other groups to make estimates in groups with sparse data\n\n    -   Intuitively, 67 and 69 year olds would provide some relevant\n        information about 68 year olds\n\n    -   Assuming a straight line relationship tells us about other, even\n        more distant, individuals\n\n    -   If we do not want to assume a straight line, we may only want to\n        borrow information from nearby groups\n\n-   Defining \"Contrasts\"\n\n    -   Define a comparison across groups to use when answering\n        scientific questions\n\n    -   If the straight line relationship holds, the slope is the\n        difference in parameter between groups differing by 1 unit in\n        $X$\n\n    -   If a non-linear relationship in parameter, the slope is still\n        the average difference in parameter between groups differing by\n        1 unit in $X$\n\n    -   Slope is a (first order or linear) test for trend in the\n        parameter\n\n    -   Statistical jargon: \"a contrast\" across groups\n\n-   The major difference between different regression models is the\n    interpretation of the parameters\n\n    -   How do I want to summarize the outcome?\n\n    -   Mean, geometric mean, odds, hazard\n\n-   How do I want to compare groups?\n\n    -   Difference, ratio\n\n-   Answering these two simple questions provides a starting road-map as\n    to which regression model to choose\n\n-   Issues related to the inclusion of covariates remains the same\n\n    -   Address the scientific question: Predictor of interest, effect\n        modification\n\n    -   Address confounding\n\n    -   Increase precision\n\n## Simple Logistic Regression\n\n### Uses of logistic regression\n\n-   Use logistic regression when you want to make inference about the\n    odds\n\n    -   Allows for continuous (or multiple) grouping variables\n\n    -   Is OK with binary grouping variables too\n\n    -   Compares odds of responses across groups using ratios\n\n        -   \"Odds ratio\"\n\n-   Binary response variable\n\n-   When using regression with binary response variables, we typically\n    model the (log) odds using logistic regression\n\n    -   Conceptually there should be no problem modeling the proportion\n        (which is the mean of the distribution)\n\n    -   However, there are several technical reasons why we do not use\n        linear regression very often with binary responses\n\n    -   Why not use linear regression for binary responses?\n\n-   Many misconceptions about the advantages and disadvantages of\n    analyzing the odds\n\n    -   Reasons I consider valid: Scientific basis\n\n        -   Uses of odds ratios in case control studies\n\n        -   Plausibility of linear trends and no effect modifiers\n\n    -   Reasons I consider valid: Statistical basis\n\n        -   There is a mean variance relationship (if not using robust\n            SE) that can be incorporate in the logistic regression model\n\n### Reasons to use logistic regression\n\n-   First (scientific) reason: Case-Control Studies\n\n    -   Studying a rare disease, so we do study in reverse\n\n        -   e.g. find subjects with cancer (and suitable controls) and\n            then ascertain exposure of interest\n\n        -   Estimate distribution of the \"effect\" across groups defined\n            by \"cause\"\n\n        -   Proportion (or odds) of smokers among people with or without\n            lung cancer\n\n    |            |               |               |\n    |------------|---------------|---------------|\n    |            | Lung Cancer + | Lung Cancer - |\n    | Smoker     | a             | b             |\n    | Non-Smoker | c             | d             |\n\n    : Case-Control or Cohort 2x2 Table. In the Case-Control design, the\n    total number of subjects with Cancer $(a+c)$ and without cancer\n    $(b+d)$ are fixed by design.\n\n    -   In contrast, a cohort study samples by exposure (smoking) and\n        then estimates the distribution of the effect in exposure groups\n\n    -   In a case-control study, we cannot estimate prevalence (without\n        knowing selection probabilities)\n\n        -   e.g. if doing a 1:1 case-control study, $(a+c) = b+d$ so it\n            would look like $50\\%$ of the subjects have cancer\n\n    -   Odds ratios are estimable in either case-control or cohort\n        sampling scheme\n\n        -   Cohort study: Odds of cancer among smoker compared to odds\n            of cancer among nonsmokers\n\n        -   Case-control study: Odds of smoking among cancer compared to\n            odds of smoking among non-cancer\n\n    -   Mathematically, these two odds ratios are the same\n\n        -   \n\n    -   Odds ratios are easy to interpret when investigating rare events\n\n        -   Odds = prob / (1 - prob)\n\n        -   For rare events, (1 - prob) is approximately 1\n\n            -   Odds is approximately the probability\n\n            -   Odds ratios are approximately risk ratios\n\n        -   Case-control studies usually used when events are rare\n\n-   Second (scientific) reason: Linearity\n\n    -   Proportions are bounded by 0 and 1\n\n    -   It is thus unlikely that a straight line relationship would\n        exists between a proportion and a predictor\n\n        -   Unless the predictor itself is bounded\n\n        -   Otherwise, there eventually must be a threshold above which\n            the probability does not increase (or only increases a\n            little)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexpit <- function(x) {exp(x)/(1+exp(x))}\nplot(function(x) expit(x), -4,4, ylab=\"Probabilty\", xlab=\"Predictor\")\n```\n\n::: {.cell-output-display}\n![Logistic function will bound probabilities between 0 and 1](Lec04.Logistic_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n-   Third (scientific) reason: Effect modification\n\n    -   The restriction on ranges for probabilities makes it likely that\n        effect modification *must* be present with proportions\n\n    -   Example: Is the association between 2-year relapse rates and\n        having a positive scan modified by gender?\n\n        -   Women relapse 40% of the time when the scan is negative, and\n            95% of the time when the scan is positive (an increase of\n            55%)\n\n            -   If men relapse 75% of the time when the scan is\n                negative, then a positive scan can increase the relapse\n                rate to at most 100%, which is only a 25% increase\n\n                |                 |       |              |\n                |-----------------|-------|--------------|\n                | **Proportions** |       |              |\n                |                 | Women | Men          |\n                | Negative Scan   | 40%   | 75%          |\n                | Positive Scan   | 95%   | (up to 100%) |\n                |                 |       |              |\n                | Difference      | 55%   | Up to 25%    |\n                | Ratio           | 1.64  | $\\leq 1.33$  |\n\n            -   With the odds, the association can hold without effect\n                modification\n\n                |               |       |                  |\n                |---------------|-------|------------------|\n                | **Odds**      |       |                  |\n                |               | Women | Men              |\n                | Negative Scan | 0.67  | 3                |\n                | Positive Scan | 19    | (up to $\\infty$) |\n                |               |       |                  |\n                | Ratio         | 28.5  | $< \\infty$       |\n\n-   If the o dds of positive scan in men was 85.5, then the odds ratio\n    would be exactly 28.5 (no effect modification)\n\n-   Fourth (statistics) reason:\n\n    -   Classical linear regression requires equal variances across each\n        predictor group\n\n    -   But, with binary data, the variance within a group depends on\n        the mean\n\n    -   For binary $Y$, $E(Y) = p$ and $Var(Y) = p(1-p)$\n\n    -   With robust standard errors, the mean-variance relationship is\n        not a major problem. However, a logistic model that correctly\n        models the mean-variance relationship will be more efficient.\n\n### The simple logistic regression model\n\n-   Modeling the odds of binary response variable $Y$ on predictor $X$\n\n    -   Distribution: $\\textrm{Pr}(Y_i = 1) = p_i$\n\n    -   Model:\n        $\\textrm{logit}(p_i) = \\textrm{log}\\left(\\frac{p_i}{1-p_i}\\right) = \\beta_0 + \\beta_1 \\times X_i$\n\n    -   When $X_i = 0$: log odds = $\\beta_0$\n\n    -   When $X_i = x$: log odds = $\\beta_0 + \\beta_1 \\times x$\n\n    -   When $X_i = x+1$: log odds =\n        $\\beta_0 + \\beta_1 \\times x + \\beta_1$\n\n-   To interpret as odds, exponentiate the regression parameters\n\n    -   Distribution: $\\textrm{Pr}(Y_i = 1) = p_i$\n    -   Model:\n        $\\frac{p_i}{1-p_i} = \\exp(\\beta_0 + \\beta_1 \\times X_i) = e^{\\beta_0} \\times e^{\\beta_1 \\times X_i}$\n    -   When $X_i = 0$: odds = $e^{\\beta_0}$\n    -   When $X_i = x$: odds = $e^{\\beta_0} \\times e^{\\beta_1 \\times x}$\n    -   When $X_i = x+1$: odds =\n        $e^{\\beta_0} \\times e^{\\beta_1 \\times x} \\times e^{\\beta_1}$\n\n-   To interpret as proportions (remember proportion = odds / (1 +\n    odds))\n\n    -   Distribution: $\\textrm{Pr}(Y_i = 1) = p_i$\n    -   Model:\n        $p_i = \\frac{e^{\\beta_0} e^{\\beta_1 \\times X_i}}{1 + e^{\\beta_0} e^{\\beta_1 \\times X_i}}$\n    -   When $X_i = 0$: $p_i = \\frac{e^{\\beta_0}}{1 + e^{\\beta_0}}$\\\n    -   When $X_i = x$:\n        $p_i = \\frac{e^{\\beta_0} e^{\\beta_1 \\times x}}{1 + e^{\\beta_0} e^{\\beta_1 \\times x}}$\n    -   When $X_i = x+1$:\n        $p_i = \\frac{e^{\\beta_0} e^{\\beta_1 \\times x} e^{\\beta_1}}{1 + e^{\\beta_0} e^{\\beta_1 \\times x}e^{\\beta_1}}$\n\n-   Most common interpretations found by exponentiating the coefficients\n\n    -   Odds when predictor is 0 found by exponentiating the intercept:\n        $\\exp(\\beta_0)$\n\n    -   Odds ratio between groups differing in the values of the\n        predictor by 1 unit found by exponentiating the slope:\n        $\\exp(\\beta_1)$\n\n-   Stata commands\n\n    ``` stata\n    logit respvar predvar, [robust]\n    ```\n\n    -   Provides regression parameter estimates an inference on the log\n        odds scale (both coefficients with CIs, SEs, p-values)\n\n    ``` stata\n    logistic respvar predvar, [robust]\n    ```\n\n    -   Provides regression parameter estimates and inference on the\n        odds ratio scale (only slope with CIs, SEs, p-values)\n\n-   R Commands\n\n    -   With rms package, `lrm(respvar ~ predvar, ...)`\n\n    -   In general, `glm(respvar ~ predvar, family=“binomial”)`\n\n## Example: Survival on the Titanic and Age\n\n-   Dataset at <https://biostat.app.vumc.org/DataSets>\n\n-   Describes the survival status of individual passengers on the\n    Titanic\n\n-   Data on age available for many, but not all, subjects (data\n    continually being updated)\n\n-   Response variables is Survival\n\n    -   Binary variable: 1=Survived, 0=Died\n\n-   Predictor variable is Age\n\n    -   Continuous grouping variable\n\n-   Possibly different age effects by sex (effect modification by sex)\n\n### Descriptive Plots\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# | fig-cap: Missing data patterns in the Tianic dataset\nlibrary(rms)\ntitanic <- read.csv(file=\"data/titanic3.csv\")\nplot(naclus(titanic)) # study patterns of missing values\n```\n\n::: {.cell-output-display}\n![](Lec04.Logistic_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# | fig-cap: Scatterplot of age versus survival in the Titanic data with lowess smooth.  This simple plot is not very useful because survival is either 0 or 1, making it hard to visualize any trends.\nggplot(titanic, aes(x=age, y=survived)) + geom_jitter(width=0, height=.02, alpha=.5) + geom_smooth()\n```\n\n::: {.cell-output-display}\n![](Lec04.Logistic_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# | fig-cap: Age versus survival by sex in the Titanic data by age using a super smoother.  The trends are clearer with this smoothing approach.\nwith(titanic, \n     plsmo(age, survived, group=sex, datadensity=T, ylab=\"Survived (1=Yes, 0=No)\", xlab=\"Age (years)\")\n)\n```\n\n::: {.cell-output-display}\n![](Lec04.Logistic_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n-   Comments on the plots\n\n    -   Age is missing for many subjects, which we will not worry about\n        in the following analysis\n\n    -   The simple scatterplot, even with superimposes lowess smooth, is\n        worthless. I have jittered the point and altered their opacity\n        to help visualize overlapping point.\n\n    -   More advanced plotting available in R (in this case, the plsmo()\n        function) can help to visualize the data\n\n### Regression Model\n\n-   Regression model for survival on age (ignoring possible effect\n    modification for now)\n\n-   Answer question by assessing linear trends in log odds of survival\n    by age\n\n-   Estimate the best fitting line to log odds of survival within age\n    groups\n\n$$\\textrm{logodds}(\\textrm{Survival} | \\textrm{Age}) = \\beta_0 + \\beta_1 \\times \\textrm{Age}$$\n\n-   An association will exist if the slope $\\beta_1$ is nonzero\n\n-   In that case, the odds (and probability) of survival will be\n    different across different age groups\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm.titanic <- glm(survived ~ age, data=titanic, family = \"binomial\")\nsummary(m.titanic)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = survived ~ age, family = \"binomial\", data = titanic)\n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)  \n(Intercept) -0.136534   0.144715  -0.943   0.3454  \nage         -0.007899   0.004407  -1.792   0.0731 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1414.6  on 1045  degrees of freedom\nResidual deviance: 1411.4  on 1044  degrees of freedom\n  (263 observations deleted due to missingness)\nAIC: 1415.4\n\nNumber of Fisher Scoring iterations: 4\n```\n:::\n:::\n\n\n$\\textrm{logodds}(\\textrm{Survival} | \\textrm{Age}) = -0.1365 - 0.007899 \\times \\textrm{Age}$\n\n-   General interpretation\n\n    -   Intercept is labeled \"(Intercept)\"\n\n    -   Slope for age is labeled \"age\"\n\n-   Interpretation of intercept\n\n    \\*Estimated log odds for newborns (age=0) is $-0.136534$\n\n    -   Odds of survival for newborns is $e^{-0.136534} = 0.8724$\n\n    -   Probability of survival\n\n        -   Prob = odds / (1 + odds)\n\n        -   $0.8724 / (1 + .8724) = 0.4659$\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(m.titanic, newdata=data.frame(age=0), type='response')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        1 \n0.4659194 \n```\n:::\n:::\n\n\n-   Interpretation of slope\n\n    -   Estimate difference in the log odds of survival for two groups\n        differing by one year in age is $-0.0078985$\n\n    -   This estimate averages over males and females\n\n    -   Older groups tend to have lower log odds\n\n    -   Odds Ratio: $e^{-0.0078985} = 0.9921$\n\n    -   For five year difference in age:\n        $e^{-0.0078985 \\times 5} = 0.9612$\n\n    -   In Stata use \"lincom age, or\" or \"lincom 5\\*age, or\"\n\nNote that if the straight line relationship does not hold true, we\ninterpret the slope as an average difference in the log odds of survival\nper one year difference in age\n\nThere are several ways to get the odds ratio and confidence interval in\nR\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# The coefficient and confidence interval (on the log-odds scale)\ncoef(m.titanic)[\"age\"]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         age \n-0.007898504 \n```\n:::\n\n```{.r .cell-code}\nconfint.default(m.titanic, \"age\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          2.5 %       97.5 %\nage -0.01653509 0.0007380868\n```\n:::\n\n```{.r .cell-code}\n# Odds ratio for age and confidence interval for age (1 year increase)\nexp(coef(m.titanic)[\"age\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      age \n0.9921326 \n```\n:::\n\n```{.r .cell-code}\nexp(confint.default(m.titanic, \"age\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        2.5 %   97.5 %\nage 0.9836009 1.000738\n```\n:::\n\n```{.r .cell-code}\n# Odds ratio for age and confidence interval for age (5 year increase)\nexp(5*coef(m.titanic)[\"age\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      age \n0.9612771 \n```\n:::\n\n```{.r .cell-code}\nexp(5*confint.default(m.titanic, \"age\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        2.5 %   97.5 %\nage 0.9206499 1.003697\n```\n:::\n:::\n\n\n-   Using finalfit to create a nicer output table of the coefficients\n    and confidence intervals\n\n    -   For finalfit to use a logistic regression model by default,\n        survived must be defined as a factor variable with two levels\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmykable = function(x){\n  knitr::kable(x, row.names = FALSE, align = c(\"l\", \"l\", \"r\", \"r\", \"r\", \"r\", \"r\", \"r\", \"r\"),\n               booktabs=TRUE)\n}\n\n\nlibrary(finalfit)\nlibrary(dplyr)\nlibrary(rms)\nexplanatory = c(\"age\")\n\ntitanic$survived.factor <- factor(titanic$survived, levels=0:1, labels=c(\"Died\",\"Survived\"))\ndependent = 'survived.factor'\n\nlabel(titanic$age) <- \"Age (years)\"\ntitanic %>% \n  finalfit(dependent, explanatory) %>% mykable()\n```\n\n::: {.cell-output-display}\n|Dependent: survived.factor |          |        Died|    Survived|          OR (univariable)|        OR (multivariable)|\n|:--------------------------|:---------|-----------:|-----------:|-------------------------:|-------------------------:|\n|Age (years)                |Mean (SD) | 30.5 (13.9)| 28.9 (15.1)| 0.99 (0.98-1.00, p=0.073)| 0.99 (0.98-1.00, p=0.073)|\n:::\n:::\n\n\n### Comments on Interpretation\n\n-   The slope for age is expressed as a difference in group means, not\n    the difference due to aging. We did not do a longitudinal study in\n    which repeated measurements were taken on the same subject.\n\n-   If the group log odds are truly linear, then the slope has an exact\n    interpretation as the change in survival due to a one year change in\n    (any) age\n\n-   Otherwise, the slope estimates the first order trend of the sample\n    data and we should not treat the estimates of group odds or\n    probabilities as accurate\n\n-   It is difficult to see in the above example, but the CIs around the\n    odds ratios are not symmetric\n\n    -   (Symmetric) CIs are calculated on the log odds scale, and then\n        transformed to the odds scale by expoenentiating the lower and\n        upper limits of the CI\n\n-   \"From logistic regression analysis, we estimate that for each 5 year\n    difference in age, the odds of survival on the Titanic decreased by\n    3.9%, though this estimate is not statistically significant\n    ($p = 0.07$). A 95% CI suggests that this observation is not unusual\n    if a group that is five years older might have an odds of survival\n    that was anywhere between 7.9% lower and 0.4% higher than the\n    younger group.\"\n\n-   The confidence interval and statistical test given in the output is\n    called a Wald test. Other tests (Score, Likelihood Ratio) are also\n    possible.\n\n    -   All tests are asymptotically equivalent\n\n    -   The Wald test is easiest to obtain, but generally performs the\n        poorest in small sample sizes\n\n    -   The Likelihood Ratio test performs the best in small samples. We\n        will discuss it later, including how to obtain the test using\n        post-estimation commands.\n\n    -   The Score test is not bad in small samples, but is often hard to\n        obtain from software. It is exactly equal to the Chi-squared\n        test for binary outcomes and categorical predictors.\n\n#### Bayesian Estimates and Interpretation\n\n-   Bayesian approach to the logistic model requires specifying\n\n    -   The model\n\n    -   e.g. a model for the log odds of survival that is linear in the\n        parameters with an intercept and slope for age\n\n    -   Prior distributions on parameters\n\n        -   For the simple logistic regression model, we have parameters\n            $\\beta_0$, and $\\beta_1$.\n\n        -   For now, we will use default prior distributions that are\n            are intended to be *weakly informative* in that they provide\n            moderate regularization and help stabilize computation. See\n            the [STAN\n            documentation](https://mc-stan.org/rstanarm/reference/priors.html)\n            for more details\n\n        -   Appropriate priors can be based on scientific considerations\n\n        -   Sensitivity analyses can evaluate the the robustness of\n            finding to different prior assumptions\n\n-   The likelihood\n\n    -   For a binomial GLM the likelihood for one observation $y$ can be\n        written as a conditionally binomial probability mass function\n\n$$\\binom{n}{y} \\pi^{y} (1 - \\pi)^{n - y},$$\n\n-   $n$ is the known number of trials, $\\pi = g^{-1}(\\eta)$ is the\n    probability of success and\n    $\\eta = \\alpha + \\mathbf{x}^\\top \\boldsymbol{\\beta}$ is a linear\n    predictor\n\n-   For a sample of size $N$, the likelihood of the entire sample is the\n    product of $N$ individual likelihood contributions.\n\n-   Because $\\pi$ is a probability, for a binomial model the *link*\n    function $g$ maps between the unit interval (the support of $\\pi$)\n    and the set of all real numbers $\\mathbb{R}$. When applied to a\n    linear predictor $\\eta$ with values in $\\mathbb{R}$, the inverse\n    link function $g^{-1}(\\eta)$ therefore returns a valid probability\n    between 0 and 1.\n\n-   The two most common link functions used for binomial GLMs are the\n\n    -   [logit](https://en.wikipedia.org/wiki/Logit) and\n    -   [probit](https://en.wikipedia.org/wiki/Probit)\n\n-   With the logit (or log-odds) link function\n    $g(x) = \\ln{\\left(\\frac{x}{1-x}\\right)}$, the likelihood for a\n    single observation becomes\n\n$$\\binom{n}{y}\\left(\\text{logit}^{-1}(\\eta)\\right)^y \n\\left(1 - \\text{logit}^{-1}(\\eta)\\right)^{n-y} = \n\\binom{n}{y} \\left(\\frac{e^{\\eta}}{1 + e^{\\eta}}\\right)^{y}\n\\left(\\frac{1}{1 + e^{\\eta}}\\right)^{n - y}$$\n\n-   With the probit link function $g(x) = \\Phi^{-1}(x)$ yields the\n    likelihood\n\n$$\\binom{n}{y} \\left(\\Phi(\\eta)\\right)^{y}\n\\left(1 - \\Phi(\\eta)\\right)^{n - y},$$\n\nwhere $\\Phi$ is the CDF of the standard normal distribution.\n\n-   Output from Bayesian logistic regression using logit link function\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rstanarm)\nlibrary(bayesplot)\n\nfit2 <- stan_glm(survived ~ age,\n                 data=titanic, family=binomial(),\n                 seed=1234,\n                 refresh=0)\nsummary(fit2, digits=4, prob=c(.025, .5, .975))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nModel Info:\n function:     stan_glm\n family:       binomial [logit]\n formula:      survived ~ age\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 1046\n predictors:   2\n\nEstimates:\n              mean    sd      2.5%    50%     97.5%\n(Intercept) -0.1343  0.1459 -0.4135 -0.1336  0.1560\nage         -0.0079  0.0044 -0.0169 -0.0078  0.0004\n\nFit Diagnostics:\n           mean   sd     2.5%   50%    97.5%\nmean_PPD 0.4089 0.0217 0.3652 0.4092 0.4512 \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse   Rhat   n_eff\n(Intercept)   0.0029 0.9994 2533 \nage           0.0001 0.9996 2570 \nmean_PPD      0.0004 1.0004 3073 \nlog-posterior 0.0284 1.0047 1340 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n```\n:::\n:::\n\n\n-   And a summary of the prior distributions used\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprior_summary(fit2, digits = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPriors for model 'fit2' \n------\nIntercept (after predictors centered)\n ~ normal(location = 0, scale = 2.5)\n\nCoefficients\n  Specified prior:\n    ~ normal(location = 0, scale = 2.5)\n  Adjusted prior:\n    ~ normal(location = 0, scale = 0.17)\n------\nSee help('prior_summary.stanreg') for more details\n```\n:::\n:::\n\n\n-   Interpretation\n\n    -   Slope for age is of primary scientific importance\n\n    -   *A priori* we assume that no association between age and\n        cholesterol. Specifically, we assumed a Normal prior with\n        location (mean) of 0 and scale (standard devation) of 0.17 for\n        $\\beta_1$.\n\n    -   Conditional on the data, we estimate that for every 1 year\n        increase in age, the log odds of decreases by -0.0079 (95%\n        credible interval -0.0169 to 0.0004).\n\n    -   To obtain the posterior odds ratio and 95% credible intervals,\n        some additional commands are needed\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 1 year change in age\nexp(coef(fit2)[\"age\"]) # Posterior Odds Ratio\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      age \n0.9922461 \n```\n:::\n\n```{.r .cell-code}\nexp(posterior_interval(fit2, prob = 0.95)) # 95% credible interval\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 2.5%    97.5%\n(Intercept) 0.6613115 1.168821\nage         0.9832330 1.000353\n```\n:::\n\n```{.r .cell-code}\n# 5 year change in age\nexp(5*coef(fit2)[\"age\"]) # Posterior Odds Ratio\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      age \n0.9618272 \n```\n:::\n\n```{.r .cell-code}\nexp(5*posterior_interval(fit2, prob = 0.95))[2,] # 95% credible interval\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     2.5%     97.5% \n0.9189294 1.0017684 \n```\n:::\n:::\n\n\n## Inference with Logistic Regression\n\n-   The ideas of Signal and Noise found in simple linear regression do\n    not translate well to logistic regression\n\n-   We do not tend to quantify an error distribution with logistic\n    regression\n\n-   Valid statistical inference (CIs, p-values) about *associations*\n    requires three general assumptions\n\n-   Assumption 1: Approximately Normal distributions for the parameter\n    estimates\n\n    -   Large N\n\n    -   Need for either robust standard errors or classical logistic\n        regression\n\n    -   Definition of large depends on the underlying probabilities\n        (odds)\n\n    -   Recall the rule of thumb for chi-squared tests based on the\n        expected number of events\n\n-   Assumption 2: Assumptions about the independence of observations\n\n    -   Classical regression: Independence of all observation\n\n    -   Robust standard errors: Correlated observations within\n        identified clusters\n\n    -   Assumption 3: Assumptions about variance of observations within\n        groups\n\n-   Classical regression: Mean-variance relationship for binary data\n\n    -   Classical logistic regression estimates SE using model based\n        estimates\n\n    -   Hence in order to satisfy this requirement, linearity of log\n        odds across groups must hold\n\n-   Robust standard errors\n\n    -   Allows unequal variance across groups\n\n    -   Hence, do not need linearity of log odds across groups to hold\n\n    -   Valid statistical inference (CIs, p-values) about *odds of\n        response in specific groups* requires a further assumption\n\n-   Assumption 4: Adequacy of the linear model\n\n    -   If we are trying to borrow information about the log odds from\n        neighboring groups, and we are assuming a straight line\n        relationship, the straight line needs to be true\n\n    -   Needed for either classical or robust standard errors\n\n    -   Note that we can model transformations of the measured predictor\n        if we feel a straight line is not appropriate\n\n    -   Inference about *individual observations* (prediction intervals,\n        P-values) in specific groups requires no further assumptions\n        because we have binary data\n\n        -   For binary data, if we know the mean (proportion), we know\n            everything about the distribution including the variance\n\n        -   This differs from linear regression where we can have a\n            correct model for the mean, but the assumption about the\n            error distribution (Normality, homoskedasticity) can be\n            incorrect\n\n### Interpreting \"Positive\" Results\n\n-   Slope is statistically different from 0 using robust standard errors\n\n-   Observed data is atypical of a setting with no linear trend in odds\n    of response across groups\n\n-   Data suggests evidence of a trend toward larger (or smaller) odds in\n    groups having larger values of the predictor\n\n-   (To the extent the data appears linear, estimates of the group odds\n    or probabilities will be reliable)\n\n### Interpreting \"Negative\" Results\n\n-   Many possible reasons why the slope is not statistically different\n    from 0 using robust standard errors\n\n    -   There may be no association between the response and predictor\n\n    -   There may be an association in the parameter considered, but the\n        best fitting line has zero slope\n\n    -   There may be a first order trend in the log odds, but we lacked\n        the precision to be confident that it truly exists (a type II\n        error)\n\n## Example analysis revisited: Effect Modification\n\n-   Recall in our Titanic example that the effect of age appeared to\n    differ by sex\n\n    -   We ignored this difference earlier, so our estimated age effect\n        was a (weighted) average of the age effect in males and the age\n        effect in female\n\n    -   Here is the plot again describing the trends we see in survival\n        by age and sex (using plsmo).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# | fig-cap: Age versus survival by gender in the Titanic data by age using a super smoother.\nwith(titanic, \n     plsmo(age, survived, group=sex, datadensity=T, ylab=\"Survived (1=Yes, 0=No)\", xlab=\"Age (years)\")\n)\n```\n\n::: {.cell-output-display}\n![](Lec04.Logistic_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n:::\n\n\n-   We could describe the observed differences in two way, both being\n    correct\n\n-   Gender modifies the age effect\n\n    -   In males, the probability of survival worsened with age while in\n        female the probabilty of survival improved with age\n    -   Emphasizes that the female age slope is positive while the male\n        age slope is negative\n\n-   Age modifies the gender effect\n\n    -   The survival rates of male and females were more similar at\n        younger ages than older ages\n    -   Could specify the odds ratio of survival comparing females to\n        males at specific ages\n\n### Stratified analysis by sex\n\n-   The log odds of survival in females\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit.titanic.female <- glm(survived ~ age, data=titanic, subset=sex==\"female\")\nfit.titanic.female\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:  glm(formula = survived ~ age, data = titanic, subset = sex == \n    \"female\")\n\nCoefficients:\n(Intercept)          age  \n   0.637644     0.004006  \n\nDegrees of Freedom: 387 Total (i.e. Null);  386 Residual\n  (78 observations deleted due to missingness)\nNull Deviance:\t    72.25 \nResidual Deviance: 70.93 \tAIC: 447.7\n```\n:::\n:::\n\n\n-   The log odds of survival in males\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit.titanic.male <- glm(survived ~ age, data=titanic, subset=sex==\"male\")\nfit.titanic.male\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:  glm(formula = survived ~ age, data = titanic, subset = sex == \n    \"male\")\n\nCoefficients:\n(Intercept)          age  \n   0.316337    -0.003635  \n\nDegrees of Freedom: 657 Total (i.e. Null);  656 Residual\n  (185 observations deleted due to missingness)\nNull Deviance:\t    107.3 \nResidual Deviance: 105.5 \tAIC: 669.1\n```\n:::\n:::\n\n\n#### Odds ratios and confidence intervals for age effect by sex\n\n-   Consider a 5 year change in age\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Females\nexp(5*coef(fit.titanic.female)[\"age\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     age \n1.020234 \n```\n:::\n\n```{.r .cell-code}\nexp(5*confint.default(fit.titanic.female,\"age\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       2.5 %  97.5 %\nage 1.005398 1.03529\n```\n:::\n\n```{.r .cell-code}\n# Males\nexp(5*coef(fit.titanic.male)[\"age\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      age \n0.9819903 \n```\n:::\n\n```{.r .cell-code}\nexp(5*confint.default(fit.titanic.male,\"age\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        2.5 %    97.5 %\nage 0.9715019 0.9925919\n```\n:::\n:::\n\n\n### Effect modification using interaction terms\n\n-   Instead of fitting two seaparte models for male and females, we\n    could estimate all parameters in a single regression model\n    -   Let $p_i$ be the probability of survival for passenger $i$ and\n        $\\textrm{logit}(p)= \\textrm{log}\\left(\\frac{p}{1-p}\\right)$\n    -   Let $X_{1i}$ be the age of subject $i$\n    -   Let $X_{2i}$ be an indicator variable for female sex. $X_{2i}=1$\n        if a subject is female and $X_{2i} = 0$ if a subject is male\n\n$$\\textrm{logit}(p | X_{1i},X_{2i}) = \\beta_0 + \\beta_1 * X_{1i} + \\beta_{2i} * X_{2i} + \\beta_3*X_{1i}*X_{2i}$$\n\n-   In males, $X_{2i} = 0$, this model reduces to\n\n$$\\textrm{logit}(p | X_{1i}, X_{2i}=0) = \\beta_0 + \\beta_1 * X_{1i}$$\n\n-   In females, $X_{2i} = 1$, this model can be expressed as\n\n$$\\textrm{logit}(p | X_{1i}, X_{2i}=1) = (\\beta_0+\\beta_2) + (\\beta_1+\\beta_3) * X_{1i}$$\n\n-   $\\hat{\\beta_1}$ is the estimate age effect in males\n-   $\\hat{\\beta_1} + \\hat{\\beta_3}$ is the estimated age effect in\n    females\n-   $\\hat{\\beta_3}$ is the estimated *difference* between the age effect\n    in male and the age effect in females\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# female has already been defined in the dataset, but if I wanted to create this variable I could do so\ntitanic$female <- (titanic$sex==\"female\")+0\n\nfit.titanic.interact <- glm(survived ~ age + female + age*female, data=titanic)\nsummary(fit.titanic.interact)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = survived ~ age + female + age * female, data = titanic)\n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.316337   0.037944   8.337 2.39e-16 ***\nage         -0.003635   0.001124  -3.233  0.00126 ** \nfemale       0.321307   0.059757   5.377 9.35e-08 ***\nage:female   0.007641   0.001823   4.192 3.01e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.1693472)\n\n    Null deviance: 252.69  on 1045  degrees of freedom\nResidual deviance: 176.46  on 1042  degrees of freedom\n  (263 observations deleted due to missingness)\nAIC: 1116.9\n\nNumber of Fisher Scoring iterations: 2\n```\n:::\n:::\n\n\n-   We can see that the parameter estimates from the interaction model\n    are the same as the estimates from the two stratified models\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Interaction model\ncoef(fit.titanic.interact)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n (Intercept)          age       female   age:female \n 0.316337448 -0.003634770  0.321306890  0.007641206 \n```\n:::\n\n```{.r .cell-code}\n# Model fit just on male subjects\ncoef(fit.titanic.male)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept)         age \n 0.31633745 -0.00363477 \n```\n:::\n\n```{.r .cell-code}\n# Model fit just on female subjects\ncoef(fit.titanic.female)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept)         age \n0.637644338 0.004006436 \n```\n:::\n\n```{.r .cell-code}\n# Linear combinations from the interaction model give the female intercept and age slope\ncoef(fit.titanic.interact)[1] + coef(fit.titanic.interact)[3]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept) \n  0.6376443 \n```\n:::\n\n```{.r .cell-code}\ncoef(fit.titanic.interact)[2] + coef(fit.titanic.interact)[4]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        age \n0.004006436 \n```\n:::\n:::\n",
    "supporting": [
      "Lec04.Logistic_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}