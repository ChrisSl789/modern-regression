{
  "hash": "b8562bc4010555a7e8ca6e5b3e6d9625",
  "result": {
    "markdown": "---\ntitle: \"Multivariable Models\"\nsubtitle: \"Lecture 07\"\nauthor: \"Chris Slaughter\"\nfooter: \"Bios 6312\"\ndate: last-modified\nformat:\n  html:\n    embed-resources: true\n    standalone: true\n    number-sections: true\n    number-depth: 4\n    anchor-sections: true\n    smooth-scroll: true\n    theme: journal\n    toc: true\n    toc-depth: 4\n    toc-title: Contents\n    toc-location: left\n    code-link: false\n    code-tools: true\n    code-fold: true\n    code-block-bg: \"#f1f3f5\"\n    code-block-border-left: \"#31BAE9\"\n    reference-location: margin\n    fig-cap-location: margin\n    fontsize: medium\n\nexecute:\n   warning: false\n   message: false\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rms)\nlibrary(ggplot2)\n```\n:::\n\n\n## Overview\n\n-   Scientific questions\n\n    -   Most often scientific questions are translated into comparing the distribution of some response variable across groups of interest\n\n    -   Groups are defined by the predictor of interest (POI)\n\n        -   Categorical predictors of interest: Treatment or control, knockout or wild type, ethnic group\n\n        -   Continuous predictors of interest: Age, BMI, cholesterol, blood pressure\n\n-   If we only considered the response and POI, this is referred to as a simple (linear, logistic, PH, etc.) regression model\n\n-   Often we need to consider additional variables other than POI because\\...\n\n    -   We want to make comparisons in different strata\n\n        -   e.g if we stratify by gender, we may get different answers to our scientific question in men and women\n\n    -   Groups being compared differ in other ways\n\n        -   Confounding: A variable that is related to both the outcome and predictor of interest\n\n    -   Less variability in the response if we control for other variables\n\n        -   Precision: If we restrict to looking within certain strata, may get smaller $\\sigma^2$\n\n-   Statistics: Covariates other than the Predictor of Interest are included in the model as\\...\n\n    -   Effect modifiers\n\n    -   Confounders\n\n    -   Precision variables\n\n-   Two main statistical methods to adjust for covariates\n\n    -   Stratified analyses\n\n        -   Combines information about associations between response across strata\n\n        -   Will not borrow information about (or even estimate) associations between response and adjustment variables\n\n    -   Adjustment in multiple regression\n\n        -   Can (but does not have to) borrow information about associations between response and all modeled variables\n\n        -   Could conduct a stratified analysis using regression\n\n        -   In practice, when researchers say they are using regression, they are almost certainly doing so to borrow information\n\n-   Example: Is smoking associated with FEV in teenagers?\n\n    -   Stratified Analysis\n\n        -   Separately estimate mean FEV in 19 year olds, 18 year olds, 17 year olds, etc. by smoking status\n\n        -   Average means (using weights) to come up with overall effect of smoking on FEV\n\n        -   Key: Not trying to estimate a common effect of age across strata (not borrowing information across age)\n\n        -   No estimate of the age effect in this analysis\n\n    -   Multiple regression\n\n        -   Fit a regression model with FEV as the outcome, smoking as the POI, and age as an adjustment variable\n\n        -   Will provide you an estimate of the association between FEV and age (but do you care?)\n\n        -   Can borrow information across ages to estimate the age effect\n\n            -   Linear/spline function for age would borrow information\n\n            -   Separate indicator variable for each age would borrow less information (would still assume that all 19.1 and 19.2 year olds are the same)\n\n-   Adjustment for two factors: Age and Sex\n\n    -   Stratified analyses\n\n        -   Calculate separate means by age and sex, combine using weight averages as before\n\n        -   This method adjusts for the interaction of age and sex (in addition to age and sex main effects)\n\n    -   Multiple regression\n\n        -   \"We adjusted for age and sex\\...\" or \"Holding age and sex constant, we found \\...\"\n\n        -   Almost certainly the research adjusted for age and sex, but not the interaction of the two variables (but they could have)\n\n## Stratified Analysis\n\n### Methods\n\n-   General approach to conducting a stratified analysis\n\n    -   Divide the data into strata based on all combinations of the \"adjustment\" covariates\n\n        -   e.g. every combination of age, gender, race, SES, etc.\n\n    -   Within each strata, perform an analysis comparing responses across POI groups\n\n    -   Use (weighted) average of estimated associations across groups\n\n-   Combining responses: Easy if estimates are independent and approximately Normally distributed\n\n    -   For independent strata $k$, $k = 1, \\ldots, K$\n\n        -   Estimate in stratum $k$: $\\hat{\\theta}_k \\sim N (\\theta_k, se^2_k)$\n\n        -   Weight in stratum $k$: $w_k$\n\n        -   Stratified estimate is\n\n            $$\\hat{\\theta} = \\frac{\\sum_{k=1}^K w_k \\hat{\\theta}_k}{\\sum_{k=1}^K w_k} \\sim N\\left(\\frac{\\sum_{k=1}^K w_k \\theta_k}{\\sum_{k=1}^K w_k}, \\frac{\\sum_{k=1}^K w_k^2 se^2_k}{\\left(\\sum_{k=1}^K w_k\\right)^2} \\right)$$\n\n-   How to choose the weights?\n\n    -   Scientific role of the stratified estimate\n\n        -   Just because I have more women in my sample than men, does that mean I should weight my estimate towards women? Maybe, maybe not.\n\n    -   Statistical precision of the stratified estimate\n\n        -   Just because the data are more variable in women than men, does that mean I should down-weight women? Maybe, maybe not.\n\n        -   Weight usually chosen on statistical criteria\n\n-   Weights should be chosen based on the statistical role of the adjustment variable\n\n    -   Effect modifiers\n\n    -   Confounding\n\n    -   Precision\n\n### Weights for Effect Modification\n\n-   Scientific criteria\n\n    -   Sometimes we anticipate effect modification by some variables, but\n\n        -   We do not choose to report estimates of the association between the response and POI in each stratum separately\n\n            -   e.g. political polls, age adjusted incidence rates\n\n        -   We are interested in estimating the \"average association\" for a population\n\n-   Choosing weights according to scientific importance\n\n    -   Want to estimate the average effect in some population of interest\n\n        -   The real population, or,\n\n        -   Some standard population used for comparisons\n\n    -   Example: Ecologic studies comparing incidence of hip fractures across countries\n\n        -   Hip fracture rates increase with age\n\n        -   Industrialized countries and developing world have very different age distributions\n\n        -   Choose a standard age distribution to remove confounding by age\n\n-   Comment on oversampling\n\n    -   In political polls or epidemiologic studies we sometimes oversample some strata in order to gain precision\n\n        -   For fixed maximal sample size, we gain most precision if stratum samples size is proportional to weight times standard deviation of measurements in stratum\n\n        -   Example: Oversample swing-voters relative to individuals who we can be more certain about their voting preferences\n\n    -   For independent strata $k$, $k = 1, \\ldots, K$\n\n        -   Sample size in stratum $k$: $n_k$\n\n        -   Estimate in stratum $k$: $\\hat{\\theta}_k \\sim N\\left(\\theta_k, se^2_k = \\frac{V_k}{n_k} \\right)$\n\n        -   Importance weight for stratum $k$: $w_k$\n\n        -   Optimal sample size when $N = \\sum_{k=1}^K n_k$ is:\n\n            $$\\frac{w_1 \\sqrt{V_1}}{n_1} = \\frac{w_2 \\sqrt{V_2}}{n_2} = \\ldots = \\frac{w_k \\sqrt{V_k}}{n_k}$$\n\n### Weights for Confounders and Precision Variables\n\n-   If the true association is the same in each stratum, we are free to consider statistical criteria\n\n    -   It is very unlikely that there is *no* effect modification in truth, but is it small enough to ignore?\n\n-   Statistical criteria\n\n    -   Maximize precision of stratified estimates by minimizing the standard error\n\n-   Optimal statistical weights\n\n    -   For independent strata $k$, $k = 1, \\ldots, K$\n\n        -   Sample size in stratum $k$: $n_k$\n\n        -   Estimate in stratum $k$: $\\hat{\\theta}_k \\sim N\\left(\\theta_k, se^2_k = \\frac{V_k}{n_k} \\right)$\n\n        -   Importance weight for stratum $k$: $w_k$\n\n        -   Optimal sample size when $N = \\sum_{k=1}^K n_k$ is:\n\n            $$\\frac{w_1 \\sqrt{V_1}}{n_1} = \\frac{w_2 \\sqrt{V_2}}{n_2} = \\ldots = \\frac{w_k \\sqrt{V_k}}{n_k}$$\n\n-   We often ignore the aspect that variability may differ across strata\n\n    -   Simplifies so that we choose weight by sample size for each stratum\n\n-   Example: Mantel-Haenszel Statistic\n\n    -   Popular method used to create a common odds ratio estimate across strata\n\n        -   One way to combine a binary response variable and binary predictor across various strata\n\n    -   Hypothesis test comparing odds (proportions) across two groups\n\n        -   Adjust for confounding in a stratified analysis\n\n        -   Weights chosen for statistical precision\n\n    -   Approximate weighting of difference in proportions based on harmonic means of sample sizes in each stratum\n\n        -   Usually viewed as a weighted odds ratio\n\n        -   (Why not weight by log odds or probabilities?)\n\n    -   For independent strata $k$, $k = 1, \\ldots, K$\n\n        -   Sample size in stratum $k$: $n_{1k}, n_{0k}$\n\n        -   Estimates in stratum $k$: $\\hat{p}_{1k}, \\hat{p}_{2k}$\n\n        -   Precision weight for stratum $k$:\n\n            $$w_k = \\frac{n_{1k} n_{0k}}{n_{1k} + n_{0k}} \\div \\sum_{k=1}^K \\frac{n_{1k} n_{0k}}{n_{1k} + n_{0k}}$$\n\n        -   These weights work well in practice, and are not as complicated as some other weighting systems\n\n-   Odds of being full professor by sex\n\n<!-- -->\n\n```         \n. cc full female if year==95, by(field)\n\n           field |       OR       [95% Conf. Interval]   M-H Weight\n-----------------+-------------------------------------------------\n            Arts |   .5384615      .2927119   .9835152     16.54545 (exact)\n           Other |   .2540881      .1870044   .3440217      91.6448 (exact)\n            Prof |   .3434705      .1640076   .7048353     14.42581 (exact)\n-----------------+-------------------------------------------------\n           Crude |    .290421       .226544   .3715365              (exact)\n    M-H combined |   .3029764      .2378934    .385865              \n-------------------------------------------------------------------\nTest of homogeneity (M-H)      chi2(2) =     5.47  Pr>chi2 = 0.0648\n\n                   Test that combined OR = 1:\n                                Mantel-Haenszel chi2(1) =     99.10\n                                                Pr>chi2 =    0.0000\n```\n\n-   Questions about output\n\n    -   What hypothesis is being tested by the \"Test of Homogeneity\"?\n\n    -   Should we use the test of homogeneity to decide if we need to use the M-H adjustment?\n\n    -   Compare the Crude and M-H combined OR\n\n        -   Is there evidence of confounding?\n\n        -   Would there be evidence of confounding if the M-H estimate was further from the null (got smaller in this case)?\n\n    -   How would you determine if field is a precision variable from the output?\n\n-   R code for similar output as above\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsalary <- read.csv(file=\"https://biostat.app.vumc.org/wiki/pub/Main/CourseBios312/salary.csv\")\nsalary.95 <- subset(salary, year==95)\nsalary.95$female <- factor((salary.95$sex==\"F\")+0,levels=0:1, labels=c(\"Male\",\"Female\"))\nsalary.95$full <- factor((salary.95$rank==\"Full\")+0,levels=0:1, labels=c(\"Assist/Assoc\",\"Full\"))\nsalary.95$field <- factor(salary.95$field)\nlibrary(rms)\npartial.tables <- xtabs(~female + full + field, salary.95)\npartial.tables\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n, , field = Arts\n\n        full\nfemale   Assist/Assoc Full\n  Male             70   70\n  Female           52   28\n\n, , field = Other\n\n        full\nfemale   Assist/Assoc Full\n  Male            303  477\n  Female          205   82\n\n, , field = Prof\n\n        full\nfemale   Assist/Assoc Full\n  Male             96  172\n  Female           26   16\n```\n:::\n\n```{.r .cell-code}\nmarginal.table <- xtabs(~female + full, salary.95)\nmarginal.table\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        full\nfemale   Assist/Assoc Full\n  Male            469  719\n  Female          283  126\n```\n:::\n\n```{.r .cell-code}\nlibrary(epiR)\nepi.2by2(partial.tables)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             Outcome +    Outcome -      Total                 Inc risk *\nExposed +          469          719       1188     39.48 (36.69 to 42.32)\nExposed -          283          126        409     69.19 (64.47 to 73.64)\nTotal              752          845       1597     47.09 (44.62 to 49.57)\n\n\nPoint estimates and 95% CIs:\n-------------------------------------------------------------------\nInc risk ratio (crude)                         0.57 (0.52, 0.63)\nInc risk ratio (M-H)                           0.58 (0.53, 0.64)\nInc risk ratio (crude:M-H)                     0.98\nInc odds ratio (crude)                         0.29 (0.23, 0.37)\nInc odds ratio (M-H)                           0.30 (0.24, 0.39)\nInc odds ratio (crude:M-H)                     0.96\nAttrib risk in the exposed (crude) *           -29.72 (-34.98, -24.45)\nAttrib risk in the exposed (M-H) *             -28.77 (-39.96, -17.59)\nAttrib risk (crude:M-H)                        1.03\n-------------------------------------------------------------------\n M-H test of homogeneity of IRRs: chi2(2) = 6.957 Pr>chi2 = 0.031\n M-H test of homogeneity of ORs: chi2(2) = 5.604 Pr>chi2 = 0.061\n Test that M-H adjusted OR = 1:  chi2(1) = 99.101 Pr>chi2 = <0.001\n Wald confidence limits\n M-H: Mantel-Haenszel; CI: confidence interval\n * Outcomes per 100 population units \n```\n:::\n:::\n\n\n-   Can compare the M-H results to results obtained running logistic regression\n\n#### Unadjusted OR from logistic regression {#unadjusted-or-from-logistic-regression .unnumbered}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm.unadj <- lrm(full ~ female, data=salary.95)\ndd <- datadist(salary.95)\noptions(datadist='dd')\nm.unadj\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Model\n \n lrm(formula = full ~ female, data = salary.95)\n \n                        Model Likelihood    Discrimination    Rank Discrim.    \n                              Ratio Test           Indexes          Indexes    \n Obs          1597    LR chi2     109.43    R2       0.088    C       0.614    \n  Assist/Assoc 752    d.f.             1    g        0.471    Dxy     0.227    \n  Full         845    Pr(> chi2) <0.0001    gr       1.602    gamma   0.550    \n max |deriv| 4e-14                          gp       0.113    tau-a   0.113    \n                                            Brier    0.232                     \n \n               Coef    S.E.   Wald Z Pr(>|Z|)\n Intercept      0.4273 0.0594   7.20 <0.0001 \n female=Female -1.2364 0.1224 -10.10 <0.0001 \n \n```\n:::\n\n```{.r .cell-code}\nhtml(summary(m.unadj))\n```\n\n::: {.cell-output-display}\n```{=html}\n<table class='gmisc_table' style='border-collapse: collapse; margin-top: 1em; margin-bottom: 1em;' >\n<thead>\n<tr><td colspan='8' style='text-align: left;'>\nEffects          &emsp;&emsp;Response: <code>full</code></td></tr>\n<tr><th style='border-bottom: 1px solid grey; font-weight: 900; border-top: 2px solid grey; text-align: center;'></th>\n<th style='font-weight: 900; border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;'>Low</th>\n<th style='font-weight: 900; border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;'>High</th>\n<th style='font-weight: 900; border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;'>&Delta;</th>\n<th style='font-weight: 900; border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;'>Effect</th>\n<th style='font-weight: 900; border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;'>S.E.</th>\n<th style='font-weight: 900; border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;'>Lower 0.95</th>\n<th style='font-weight: 900; border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;'>Upper 0.95</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style='text-align: left;'>female --- Female:Male</td>\n<td style='padding-left:4ex; text-align: right;'>1</td>\n<td style='padding-left:4ex; text-align: right;'>2</td>\n<td style='padding-left:4ex; text-align: right;'></td>\n<td style='padding-left:4ex; text-align: right;'>-1.2360</td>\n<td style='padding-left:4ex; text-align: right;'>0.1224</td>\n<td style='padding-left:4ex; text-align: right;'>-1.4760</td>\n<td style='padding-left:4ex; text-align: right;'>-0.9964</td>\n</tr>\n<tr>\n<td style='border-bottom: 2px solid grey; text-align: left;'>&emsp;<em>Odds Ratio</em></td>\n<td style='padding-left:4ex; border-bottom: 2px solid grey; text-align: right;'>1</td>\n<td style='padding-left:4ex; border-bottom: 2px solid grey; text-align: right;'>2</td>\n<td style='padding-left:4ex; border-bottom: 2px solid grey; text-align: right;'></td>\n<td style='padding-left:4ex; border-bottom: 2px solid grey; text-align: right;'> 0.2904</td>\n<td style='padding-left:4ex; border-bottom: 2px solid grey; text-align: right;'></td>\n<td style='padding-left:4ex; border-bottom: 2px solid grey; text-align: right;'> 0.2285</td>\n<td style='padding-left:4ex; border-bottom: 2px solid grey; text-align: right;'> 0.3692</td>\n</tr>\n</tbody>\n</table>\n```\n:::\n:::\n\n\n```         \nLogistic regression                               Number of obs   =       1597\n                                                  LR chi2(1)      =     109.43\n                                                  Prob > chi2     =     0.0000\nLog likelihood =  -1049.533                       Pseudo R2       =     0.0495\n\n------------------------------------------------------------------------------\n        full | Odds Ratio   Std. Err.      z    P>|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n      female |    .290421    .035561   -10.10   0.000     .2284555    .3691939\n------------------------------------------------------------------------------\n```\n\n#### Unadjusted ORs by field {#unadjusted-ors-by-field .unnumbered}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm.unadj.arts <- lrm(full ~ female, data=salary.95[salary.95$field==\"Arts\",])\nm.unadj.other <- lrm(full ~ female, data=salary.95[salary.95$field==\"Other\",])\nm.unadj.prof <- lrm(full ~ female, data=salary.95[salary.95$field==\"Prof\",])\n```\n:::\n\n\n#### In Arts Strata\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm.unadj.arts\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Model\n \n lrm(formula = full ~ female, data = salary.95[salary.95$field == \n     \"Arts\", ])\n \n                       Model Likelihood    Discrimination    Rank Discrim.    \n                             Ratio Test           Indexes          Indexes    \n Obs           220    LR chi2      4.69    R2       0.028    C       0.570    \n  Assist/Assoc 122    d.f.            1    g        0.288    Dxy     0.141    \n  Full          98    Pr(> chi2) 0.0304    gr       1.333    gamma   0.300    \n max |deriv| 2e-09                         gp       0.070    tau-a   0.070    \n                                           Brier    0.242                     \n \n               Coef    S.E.   Wald Z Pr(>|Z|)\n Intercept      0.0000 0.1690  0.00  1.0000  \n female=Female -0.6190 0.2890 -2.14  0.0322  \n \n```\n:::\n\n```{.r .cell-code}\nsummary(m.unadj.arts)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             Effects              Response : full \n\n Factor               Low High Diff. Effect   S.E.    Lower 0.95 Upper 0.95\n female - Female:Male 1   2    NA    -0.61904 0.28899 -1.18550   -0.052625 \n  Odds Ratio          1   2    NA     0.53846      NA  0.30561    0.948740 \n```\n:::\n:::\n\n\n#### In Other Strata\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm.unadj.other\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Model\n \n lrm(formula = full ~ female, data = salary.95[salary.95$field == \n     \"Other\", ])\n \n                        Model Likelihood    Discrimination    Rank Discrim.    \n                              Ratio Test           Indexes          Indexes    \n Obs          1067    LR chi2      91.17    R2       0.109    C       0.628    \n  Assist/Assoc 508    d.f.             1    g        0.539    Dxy     0.257    \n  Full         559    Pr(> chi2) <0.0001    gr       1.715    gamma   0.595    \n max |deriv| 2e-13                          gp       0.128    tau-a   0.128    \n                                            Brier    0.229                     \n \n               Coef    S.E.   Wald Z Pr(>|Z|)\n Intercept      0.4538 0.0735  6.18  <0.0001 \n female=Female -1.3701 0.1499 -9.14  <0.0001 \n \n```\n:::\n\n```{.r .cell-code}\nhtml(summary(m.unadj.other))\n```\n\n::: {.cell-output-display}\n```{=html}\n<table class='gmisc_table' style='border-collapse: collapse; margin-top: 1em; margin-bottom: 1em;' >\n<thead>\n<tr><td colspan='8' style='text-align: left;'>\nEffects          &emsp;&emsp;Response: <code>full</code></td></tr>\n<tr><th style='border-bottom: 1px solid grey; font-weight: 900; border-top: 2px solid grey; text-align: center;'></th>\n<th style='font-weight: 900; border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;'>Low</th>\n<th style='font-weight: 900; border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;'>High</th>\n<th style='font-weight: 900; border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;'>&Delta;</th>\n<th style='font-weight: 900; border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;'>Effect</th>\n<th style='font-weight: 900; border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;'>S.E.</th>\n<th style='font-weight: 900; border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;'>Lower 0.95</th>\n<th style='font-weight: 900; border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;'>Upper 0.95</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style='text-align: left;'>female --- Female:Male</td>\n<td style='padding-left:4ex; text-align: right;'>1</td>\n<td style='padding-left:4ex; text-align: right;'>2</td>\n<td style='padding-left:4ex; text-align: right;'></td>\n<td style='padding-left:4ex; text-align: right;'>-1.3700</td>\n<td style='padding-left:4ex; text-align: right;'>0.1499</td>\n<td style='padding-left:4ex; text-align: right;'>-1.6640</td>\n<td style='padding-left:4ex; text-align: right;'>-1.0760</td>\n</tr>\n<tr>\n<td style='border-bottom: 2px solid grey; text-align: left;'>&emsp;<em>Odds Ratio</em></td>\n<td style='padding-left:4ex; border-bottom: 2px solid grey; text-align: right;'>1</td>\n<td style='padding-left:4ex; border-bottom: 2px solid grey; text-align: right;'>2</td>\n<td style='padding-left:4ex; border-bottom: 2px solid grey; text-align: right;'></td>\n<td style='padding-left:4ex; border-bottom: 2px solid grey; text-align: right;'> 0.2541</td>\n<td style='padding-left:4ex; border-bottom: 2px solid grey; text-align: right;'></td>\n<td style='padding-left:4ex; border-bottom: 2px solid grey; text-align: right;'> 0.1894</td>\n<td style='padding-left:4ex; border-bottom: 2px solid grey; text-align: right;'> 0.3409</td>\n</tr>\n</tbody>\n</table>\n```\n:::\n:::\n\n\n#### In Prof(essional) Strata\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm.unadj.arts\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Model\n \n lrm(formula = full ~ female, data = salary.95[salary.95$field == \n     \"Arts\", ])\n \n                       Model Likelihood    Discrimination    Rank Discrim.    \n                             Ratio Test           Indexes          Indexes    \n Obs           220    LR chi2      4.69    R2       0.028    C       0.570    \n  Assist/Assoc 122    d.f.            1    g        0.288    Dxy     0.141    \n  Full          98    Pr(> chi2) 0.0304    gr       1.333    gamma   0.300    \n max |deriv| 2e-09                         gp       0.070    tau-a   0.070    \n                                           Brier    0.242                     \n \n               Coef    S.E.   Wald Z Pr(>|Z|)\n Intercept      0.0000 0.1690  0.00  1.0000  \n female=Female -0.6190 0.2890 -2.14  0.0322  \n \n```\n:::\n\n```{.r .cell-code}\nhtml(summary(m.unadj.other))\n```\n\n::: {.cell-output-display}\n```{=html}\n<table class='gmisc_table' style='border-collapse: collapse; margin-top: 1em; margin-bottom: 1em;' >\n<thead>\n<tr><td colspan='8' style='text-align: left;'>\nEffects          &emsp;&emsp;Response: <code>full</code></td></tr>\n<tr><th style='border-bottom: 1px solid grey; font-weight: 900; border-top: 2px solid grey; text-align: center;'></th>\n<th style='font-weight: 900; border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;'>Low</th>\n<th style='font-weight: 900; border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;'>High</th>\n<th style='font-weight: 900; border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;'>&Delta;</th>\n<th style='font-weight: 900; border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;'>Effect</th>\n<th style='font-weight: 900; border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;'>S.E.</th>\n<th style='font-weight: 900; border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;'>Lower 0.95</th>\n<th style='font-weight: 900; border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;'>Upper 0.95</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style='text-align: left;'>female --- Female:Male</td>\n<td style='padding-left:4ex; text-align: right;'>1</td>\n<td style='padding-left:4ex; text-align: right;'>2</td>\n<td style='padding-left:4ex; text-align: right;'></td>\n<td style='padding-left:4ex; text-align: right;'>-1.3700</td>\n<td style='padding-left:4ex; text-align: right;'>0.1499</td>\n<td style='padding-left:4ex; text-align: right;'>-1.6640</td>\n<td style='padding-left:4ex; text-align: right;'>-1.0760</td>\n</tr>\n<tr>\n<td style='border-bottom: 2px solid grey; text-align: left;'>&emsp;<em>Odds Ratio</em></td>\n<td style='padding-left:4ex; border-bottom: 2px solid grey; text-align: right;'>1</td>\n<td style='padding-left:4ex; border-bottom: 2px solid grey; text-align: right;'>2</td>\n<td style='padding-left:4ex; border-bottom: 2px solid grey; text-align: right;'></td>\n<td style='padding-left:4ex; border-bottom: 2px solid grey; text-align: right;'> 0.2541</td>\n<td style='padding-left:4ex; border-bottom: 2px solid grey; text-align: right;'></td>\n<td style='padding-left:4ex; border-bottom: 2px solid grey; text-align: right;'> 0.1894</td>\n<td style='padding-left:4ex; border-bottom: 2px solid grey; text-align: right;'> 0.3409</td>\n</tr>\n</tbody>\n</table>\n```\n:::\n:::\n\n\n```         \n. sort field\n. by field: logistic full female if year==95\n\n----------------------------------------------------------------------------------\n-> field = Arts\n\nLogistic regression                               Number of obs   =        220\n                                                  LR chi2(1)      =       4.69\n                                                  Prob > chi2     =     0.0304\nLog likelihood = -148.83634                       Pseudo R2       =     0.0155\n\n------------------------------------------------------------------------------\n        full | Odds Ratio   Std. Err.      z    P>|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n      female |   .5384615   .1556108    -2.14   0.032      .305608    .9487343\n------------------------------------------------------------------------------\n\n----------------------------------------------------------------------------------\n-> field = Other\n\nLogistic regression                               Number of obs   =       1067\n                                                  LR chi2(1)      =      91.17\n                                                  Prob > chi2     =     0.0000\nLog likelihood = -692.78622                       Pseudo R2       =     0.0617\n\n------------------------------------------------------------------------------\n        full | Odds Ratio   Std. Err.      z    P>|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n      female |   .2540881   .0380877    -9.14   0.000     .1894041    .3408624\n------------------------------------------------------------------------------\n\n----------------------------------------------------------------------------------\n-> field = Prof\n\nLogistic regression                               Number of obs   =        310\n                                                  LR chi2(1)      =      10.10\n                                                  Prob > chi2     =     0.0015\nLog likelihood = -202.74823                       Pseudo R2       =     0.0243\n\n------------------------------------------------------------------------------\n        full | Odds Ratio   Std. Err.      z    P>|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n      female |   .3434705   .1175803    -3.12   0.002      .175589    .6718641\n------------------------------------------------------------------------------\n```\n\n#### OR from multivariable logistic regression controlling for field {#or-from-multivariable-logistic-regression-controlling-for-field .unnumbered}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm.adj <- lrm(full ~ female + field, data=salary.95, x=TRUE, y=TRUE)\nm.adj\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Model\n \n lrm(formula = full ~ female + field, data = salary.95, x = TRUE, \n     y = TRUE)\n \n                        Model Likelihood    Discrimination    Rank Discrim.    \n                              Ratio Test           Indexes          Indexes    \n Obs          1597    LR chi2     114.38    R2       0.092    C       0.632    \n  Assist/Assoc 752    d.f.             3    g        0.542    Dxy     0.264    \n  Full         845    Pr(> chi2) <0.0001    gr       1.719    gamma   0.373    \n max |deriv| 1e-13                          gp       0.130    tau-a   0.132    \n                                            Brier    0.232                     \n \n               Coef    S.E.   Wald Z Pr(>|Z|)\n Intercept      0.1902 0.1461  1.30  0.1930  \n female=Female -1.1998 0.1236 -9.71  <0.0001 \n field=Other    0.2220 0.1547  1.43  0.1513  \n field=Prof     0.4110 0.1856  2.21  0.0268  \n \n```\n:::\n\n```{.r .cell-code}\nhtml(summary(m.adj))\n```\n\n::: {.cell-output-display}\n```{=html}\n<table class='gmisc_table' style='border-collapse: collapse; margin-top: 1em; margin-bottom: 1em;' >\n<thead>\n<tr><td colspan='8' style='text-align: left;'>\nEffects          &emsp;&emsp;Response: <code>full</code></td></tr>\n<tr><th style='border-bottom: 1px solid grey; font-weight: 900; border-top: 2px solid grey; text-align: center;'></th>\n<th style='font-weight: 900; border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;'>Low</th>\n<th style='font-weight: 900; border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;'>High</th>\n<th style='font-weight: 900; border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;'>&Delta;</th>\n<th style='font-weight: 900; border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;'>Effect</th>\n<th style='font-weight: 900; border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;'>S.E.</th>\n<th style='font-weight: 900; border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;'>Lower 0.95</th>\n<th style='font-weight: 900; border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;'>Upper 0.95</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style='text-align: left;'>female --- Female:Male</td>\n<td style='padding-left:4ex; text-align: right;'>1</td>\n<td style='padding-left:4ex; text-align: right;'>2</td>\n<td style='padding-left:4ex; text-align: right;'></td>\n<td style='padding-left:4ex; text-align: right;'>-1.2000</td>\n<td style='padding-left:4ex; text-align: right;'>0.1236</td>\n<td style='padding-left:4ex; text-align: right;'>-1.44200</td>\n<td style='padding-left:4ex; text-align: right;'>-0.95760</td>\n</tr>\n<tr>\n<td style='text-align: left;'>&emsp;<em>Odds Ratio</em></td>\n<td style='padding-left:4ex; text-align: right;'>1</td>\n<td style='padding-left:4ex; text-align: right;'>2</td>\n<td style='padding-left:4ex; text-align: right;'></td>\n<td style='padding-left:4ex; text-align: right;'> 0.3013</td>\n<td style='padding-left:4ex; text-align: right;'></td>\n<td style='padding-left:4ex; text-align: right;'> 0.23650</td>\n<td style='padding-left:4ex; text-align: right;'> 0.38380</td>\n</tr>\n<tr>\n<td style='text-align: left;'>field --- Arts:Other</td>\n<td style='padding-left:4ex; text-align: right;'>2</td>\n<td style='padding-left:4ex; text-align: right;'>1</td>\n<td style='padding-left:4ex; text-align: right;'></td>\n<td style='padding-left:4ex; text-align: right;'>-0.2220</td>\n<td style='padding-left:4ex; text-align: right;'>0.1547</td>\n<td style='padding-left:4ex; text-align: right;'>-0.52530</td>\n<td style='padding-left:4ex; text-align: right;'> 0.08126</td>\n</tr>\n<tr>\n<td style='text-align: left;'>&emsp;<em>Odds Ratio</em></td>\n<td style='padding-left:4ex; text-align: right;'>2</td>\n<td style='padding-left:4ex; text-align: right;'>1</td>\n<td style='padding-left:4ex; text-align: right;'></td>\n<td style='padding-left:4ex; text-align: right;'> 0.8009</td>\n<td style='padding-left:4ex; text-align: right;'></td>\n<td style='padding-left:4ex; text-align: right;'> 0.59140</td>\n<td style='padding-left:4ex; text-align: right;'> 1.08500</td>\n</tr>\n<tr>\n<td style='text-align: left;'>field --- Prof:Other</td>\n<td style='padding-left:4ex; text-align: right;'>2</td>\n<td style='padding-left:4ex; text-align: right;'>3</td>\n<td style='padding-left:4ex; text-align: right;'></td>\n<td style='padding-left:4ex; text-align: right;'> 0.1889</td>\n<td style='padding-left:4ex; text-align: right;'>0.1353</td>\n<td style='padding-left:4ex; text-align: right;'>-0.07635</td>\n<td style='padding-left:4ex; text-align: right;'> 0.45420</td>\n</tr>\n<tr>\n<td style='border-bottom: 2px solid grey; text-align: left;'>&emsp;<em>Odds Ratio</em></td>\n<td style='padding-left:4ex; border-bottom: 2px solid grey; text-align: right;'>2</td>\n<td style='padding-left:4ex; border-bottom: 2px solid grey; text-align: right;'>3</td>\n<td style='padding-left:4ex; border-bottom: 2px solid grey; text-align: right;'></td>\n<td style='padding-left:4ex; border-bottom: 2px solid grey; text-align: right;'> 1.2080</td>\n<td style='padding-left:4ex; border-bottom: 2px solid grey; text-align: right;'></td>\n<td style='padding-left:4ex; border-bottom: 2px solid grey; text-align: right;'> 0.92650</td>\n<td style='padding-left:4ex; border-bottom: 2px solid grey; text-align: right;'> 1.57500</td>\n</tr>\n</tbody>\n</table>\n```\n:::\n:::\n\n\n```         \n. xi: logistic full female i.field if year==95\ni.field           _Ifield_1-3         (_Ifield_1 for field==Arts omitted)\n\nLogistic regression                               Number of obs   =       1597\n                                                  LR chi2(3)      =     114.38\n                                                  Prob > chi2     =     0.0000\nLog likelihood = -1047.0583                       Pseudo R2       =     0.0518\n\n------------------------------------------------------------------------------\n        full | Odds Ratio   Std. Err.      z    P>|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n      female |   .3012563   .0372301    -9.71   0.000     .2364515    .3838222\n   _Ifield_2 |   1.248611   .1932157     1.43   0.151     .9219526    1.691009\n   _Ifield_3 |   1.508263   .2798894     2.21   0.027     1.048381    2.169877\n------------------------------------------------------------------------------\n```\n\n-   M-H OR (.303) differs slightly from the multiple logistic regression OR for (.301).\n\n-   Conceptually, the are attempting to do the same thing, but are using different weights\n\n-   Multivariable logistic regression would allow you to control for continuous covariates without complete stratification\n\n#### Logistic regression with robust standard error estimates\n\n-   We could also fit the logistic regression models using robust standard error estimate\n\n-   We did not fit a saturated model to exactly predict the probability of being a full professor in all 6 combinations of field and female. So, it is possible that we have a poorly fitting mean model, and because of the mean-variance relationship, our variance estimates could be poor too.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrobcov(m.adj)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Model\n \n lrm(formula = full ~ female + field, data = salary.95, x = TRUE, \n     y = TRUE)\n \n                        Model Likelihood    Discrimination    Rank Discrim.    \n                              Ratio Test           Indexes          Indexes    \n Obs          1597    LR chi2     114.38    R2       0.092    C       0.632    \n  Assist/Assoc 752    d.f.             3    g        0.542    Dxy     0.264    \n  Full         845    Pr(> chi2) <0.0001    gr       1.719    gamma   0.373    \n max |deriv| 1e-13                          gp       0.130    tau-a   0.132    \n                                            Brier    0.232                     \n \n               Coef    S.E.   Wald Z Pr(>|Z|)\n Intercept      0.1902 0.1498  1.27  0.2043  \n female=Female -1.1998 0.1236 -9.71  <0.0001 \n field=Other    0.2220 0.1589  1.40  0.1624  \n field=Prof     0.4110 0.1893  2.17  0.0300  \n \n```\n:::\n\n```{.r .cell-code}\nhtml(summary(robcov(m.adj)))\n```\n\n::: {.cell-output-display}\n```{=html}\n<table class='gmisc_table' style='border-collapse: collapse; margin-top: 1em; margin-bottom: 1em;' >\n<thead>\n<tr><td colspan='8' style='text-align: left;'>\nEffects          &emsp;&emsp;Response: <code>full</code></td></tr>\n<tr><th style='border-bottom: 1px solid grey; font-weight: 900; border-top: 2px solid grey; text-align: center;'></th>\n<th style='font-weight: 900; border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;'>Low</th>\n<th style='font-weight: 900; border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;'>High</th>\n<th style='font-weight: 900; border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;'>&Delta;</th>\n<th style='font-weight: 900; border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;'>Effect</th>\n<th style='font-weight: 900; border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;'>S.E.</th>\n<th style='font-weight: 900; border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;'>Lower 0.95</th>\n<th style='font-weight: 900; border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;'>Upper 0.95</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style='text-align: left;'>female --- Female:Male</td>\n<td style='padding-left:4ex; text-align: right;'>1</td>\n<td style='padding-left:4ex; text-align: right;'>2</td>\n<td style='padding-left:4ex; text-align: right;'></td>\n<td style='padding-left:4ex; text-align: right;'>-1.2000</td>\n<td style='padding-left:4ex; text-align: right;'>0.1236</td>\n<td style='padding-left:4ex; text-align: right;'>-1.44200</td>\n<td style='padding-left:4ex; text-align: right;'>-0.95750</td>\n</tr>\n<tr>\n<td style='text-align: left;'>&emsp;<em>Odds Ratio</em></td>\n<td style='padding-left:4ex; text-align: right;'>1</td>\n<td style='padding-left:4ex; text-align: right;'>2</td>\n<td style='padding-left:4ex; text-align: right;'></td>\n<td style='padding-left:4ex; text-align: right;'> 0.3013</td>\n<td style='padding-left:4ex; text-align: right;'></td>\n<td style='padding-left:4ex; text-align: right;'> 0.23640</td>\n<td style='padding-left:4ex; text-align: right;'> 0.38380</td>\n</tr>\n<tr>\n<td style='text-align: left;'>field --- Arts:Other</td>\n<td style='padding-left:4ex; text-align: right;'>2</td>\n<td style='padding-left:4ex; text-align: right;'>1</td>\n<td style='padding-left:4ex; text-align: right;'></td>\n<td style='padding-left:4ex; text-align: right;'>-0.2220</td>\n<td style='padding-left:4ex; text-align: right;'>0.1589</td>\n<td style='padding-left:4ex; text-align: right;'>-0.53350</td>\n<td style='padding-left:4ex; text-align: right;'> 0.08943</td>\n</tr>\n<tr>\n<td style='text-align: left;'>&emsp;<em>Odds Ratio</em></td>\n<td style='padding-left:4ex; text-align: right;'>2</td>\n<td style='padding-left:4ex; text-align: right;'>1</td>\n<td style='padding-left:4ex; text-align: right;'></td>\n<td style='padding-left:4ex; text-align: right;'> 0.8009</td>\n<td style='padding-left:4ex; text-align: right;'></td>\n<td style='padding-left:4ex; text-align: right;'> 0.58650</td>\n<td style='padding-left:4ex; text-align: right;'> 1.09400</td>\n</tr>\n<tr>\n<td style='text-align: left;'>field --- Prof:Other</td>\n<td style='padding-left:4ex; text-align: right;'>2</td>\n<td style='padding-left:4ex; text-align: right;'>3</td>\n<td style='padding-left:4ex; text-align: right;'></td>\n<td style='padding-left:4ex; text-align: right;'> 0.1889</td>\n<td style='padding-left:4ex; text-align: right;'>0.1357</td>\n<td style='padding-left:4ex; text-align: right;'>-0.07701</td>\n<td style='padding-left:4ex; text-align: right;'> 0.45490</td>\n</tr>\n<tr>\n<td style='border-bottom: 2px solid grey; text-align: left;'>&emsp;<em>Odds Ratio</em></td>\n<td style='padding-left:4ex; border-bottom: 2px solid grey; text-align: right;'>2</td>\n<td style='padding-left:4ex; border-bottom: 2px solid grey; text-align: right;'>3</td>\n<td style='padding-left:4ex; border-bottom: 2px solid grey; text-align: right;'></td>\n<td style='padding-left:4ex; border-bottom: 2px solid grey; text-align: right;'> 1.2080</td>\n<td style='padding-left:4ex; border-bottom: 2px solid grey; text-align: right;'></td>\n<td style='padding-left:4ex; border-bottom: 2px solid grey; text-align: right;'> 0.92590</td>\n<td style='padding-left:4ex; border-bottom: 2px solid grey; text-align: right;'> 1.57600</td>\n</tr>\n</tbody>\n</table>\n```\n:::\n:::\n\n\n-   A saturated model would include the interaction of female with field\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm.saturated <- lrm(full ~ field + female + field*female, data=salary.95)\nm.saturated\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Model\n \n lrm(formula = full ~ field + female + field * female, data = salary.95)\n \n                        Model Likelihood    Discrimination    Rank Discrim.    \n                              Ratio Test           Indexes          Indexes    \n Obs          1597    LR chi2     119.75    R2       0.096    C       0.635    \n  Assist/Assoc 752    d.f.             5    g        0.561    Dxy     0.269    \n  Full         845    Pr(> chi2) <0.0001    gr       1.753    gamma   0.379    \n max |deriv| 4e-13                          gp       0.134    tau-a   0.134    \n                                            Brier    0.231                     \n \n                             Coef    S.E.   Wald Z Pr(>|Z|)\n Intercept                    0.0000 0.1690  0.00  1.0000  \n field=Other                  0.4538 0.1843  2.46  0.0138  \n field=Prof                   0.5831 0.2117  2.76  0.0059  \n female=Female               -0.6190 0.2890 -2.14  0.0322  \n field=Other * female=Female -0.7510 0.3256 -2.31  0.0211  \n field=Prof * female=Female  -0.4496 0.4480 -1.00  0.3156  \n \n```\n:::\n:::\n\n\n-   We can compare the saturated to the adjusted model using a likelihood ratio test\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrms::lrtest(m.saturated, m.adj)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nModel 1: full ~ field + female + field * female\nModel 2: full ~ female + field\n\nL.R. Chisq       d.f.          P \n5.37507001 2.00000000 0.06804847 \n```\n:::\n:::\n\n\n-   We can also compare the predicted probabilities directly\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Predict probability of being a full professor at all 6 combinations of field and salary\nnewdata <- expand.grid(female=levels(salary.95$female),\n                       field=levels(salary.95$field)\n                       )\nnewdata$phat.adj <- predict(m.adj, newdata, type = \"fitted\")\nnewdata$phat.sat <- predict(m.saturated, newdata, type = \"fitted\")\nnewdata\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  female field  phat.adj  phat.sat\n1   Male  Arts 0.5473986 0.5000000\n2 Female  Arts 0.2670525 0.3500000\n3   Male Other 0.6016146 0.6115385\n4 Female Other 0.3126850 0.2857143\n5   Male  Prof 0.6459134 0.6417910\n6 Female  Prof 0.3546480 0.3809524\n```\n:::\n:::\n\n\n-   Recall the partial tables of full by female by field given above. Here is the Arts Table. Note that the saturated model matches the observed probability of being a full professor in the Arts group exactly.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npartial.tables[,,'Arts']\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        full\nfemale   Assist/Assoc Full\n  Male             70   70\n  Female           52   28\n```\n:::\n\n```{.r .cell-code}\n70/(70+70)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.5\n```\n:::\n\n```{.r .cell-code}\n28/(28+52)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.35\n```\n:::\n:::\n\n\n-   Also matches for Other and Professional\n\n\n::: {.cell}\n\n```{.r .cell-code}\npartial.tables[,,'Other']\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        full\nfemale   Assist/Assoc Full\n  Male            303  477\n  Female          205   82\n```\n:::\n\n```{.r .cell-code}\n477/(477+303)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.6115385\n```\n:::\n\n```{.r .cell-code}\n82/(82+205)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.2857143\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npartial.tables[,,'Prof']\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        full\nfemale   Assist/Assoc Full\n  Male             96  172\n  Female           26   16\n```\n:::\n\n```{.r .cell-code}\n172/(172+96)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.641791\n```\n:::\n\n```{.r .cell-code}\n16/(16+26)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.3809524\n```\n:::\n:::\n\n\n## Multivariable Regression\n\n### General Regression Setting\n\n-   Types of variables\n\n    -   Binary data: e.g. sex, death\n\n    -   Nominal (unordered categorical) data: e.g. race, martial status\n\n    -   Ordinal (ordered categorical data): e.g. cancer stage, asthma severity\n\n    -   Quantitative data: e.g. age, blood pressure\n\n    -   Right censored data: e.g. time to death\n\n-   Which regression model you choose to use is based on the parameter being compared across groups\n\n    | Means           | $\\rightarrow$ Linear regression                     |     |\n    |:----------------|:----------------------------------------------------|-----|\n    | Geometric means | $\\rightarrow$ Linear regression on log scale        |     |\n    | Odds            | $\\rightarrow$ Logistic regression                   |     |\n    | Rates           | $\\rightarrow$ Poisson regression                    |     |\n    | Hazards         | $\\rightarrow$ Proportional Hazards (Cox) regression |     |\n\n-   General notation for variables and parameters\n\n    | $Y_i$                    | **Response measured on the** $i$**th subject**                   |\n    |:-------------------------|:-----------------------------------------------------------------|\n    | $X_i$                    | Value of the predictor of interest measured on the $i$th subject |\n    | $W_{1i}, W_{2i}, \\ldots$ | Value of the adjustment variable for the $i$th subject           |\n    | $\\theta_i$               | Parameter summarizing distribution of $Y_i$                      |\n\n    -   The parameter ($\\theta_i$) might be the mean, geometric mean, odds, rate, instantaneous risk of an event (hazard), etc.\n\n    -   In multiple linear regression on means, $\\theta_i = E[Y_i | X_i, W_{1i}, W_{1i}, \\ldots]$\n\n    -   Choice of correct $\\theta_i$ should be based on scientific understanding of problem\n\n-   General notation for multiple regression model\n\n    -   $g(\\theta_i) = \\beta_0 + \\beta_1 \\times X_i + \\beta_2 \\times W_{1i} + \\beta_3 \\times W_{2i} + \\ldots$\n\n        | $g( )$    | Link function used for modeling     |\n        |:----------|:------------------------------------|\n        | $\\beta_0$ | Intercept                           |\n        | $\\beta_1$ | Slope for predictor of interest $X$ |\n        | $\\beta_j$ | Slope for covariate $W_{j-1}$       |\n\n    -   The link function is often either none (for modeling means) or log (for modeling geometric means, odds, hazards)\n\n### General Uses of Multivariable Regression\n\n-   Borrowing information\n\n    -   Use other groups to make estimates in groups with sparse data\n\n        -   Intuitively, 67 and 69 year olds would provide some relevant information about 68 year olds\n\n        -   Assuming a straight line relationship tells us about other, even more distant, individuals\n\n        -   If we do not want to assume a straight line, we may only want to borrow information from nearby groups\n\n-   Defining \"Contrasts\"\n\n    -   Define a comparison across groups to use when answering scientific questions\n\n    -   If the straight line relationship holds, the slope for the POI is the difference in parameter between groups differing by 1 unit in $X$ *when all other covariates are held constant*\n\n    -   If a non-linear relationship in parameter, the slope is still the average difference in parameter between groups differing by 1 unit in $X$ *when all other covariates are held constant*\n\n        -   Slope is a (first order or linear) test for trend in the parameter\n\n        -   Statistical jargon: \"a contrast\" across groups\n\n-   The major difference between different regression models is the interpretation of the parameters\n\n    -   How do I want to summarize the outcome?\n\n        -   Mean, geometric mean, odds, hazard\n\n    -   How do I want to compare groups?\n\n        -   Difference, ratio\n\n-   Issues related to the inclusion of covariates remains the same\n\n    -   Address the scientific question: Predictor of interest, effect modification\n\n    -   Address confounding\n\n    -   Increase precision\n\n-   Interpretation of parameters\n\n    -   Intercept\n\n        -   Corresponds to a population with all modeled covariates equal to zero\n\n            -   Quite often, this will be outside the range of the data so that the intercept has no meaningful interpretation by itself\n\n    -   Slope\n\n        -   A comparison between groups differing by 1 unit in corresponding covariate, but agreeing on all other modeled covariates\n\n            -   Sometimes impossible to use this interpretation when modeling interactions or complex dose-response curves (e.g. a model with age and age-squared)\n\n-   Stratification versus regression\n\n    -   Generally, any stratified analysis could be performed as a regression model\n\n        -   Stratification adjusts for covariates and all interactions among those covariates\n\n        -   Our habit in regression is to just adjust for covariates as main effects, and consider interactions less often\n\n### Software\n\n-   In Stata or R, we use the same commands as were used for simple regression models\n\n    -   We just list more variable names\n\n    -   Interpretations of the CIs, p-values for coefficients estimates now relate to new scientific interpretations of intercept and slopes\n\n    -   Test of entire regression model also provided (a test that all slopes are equal to zero)\n\n### Example: FEV and Smoking\n\n-   Scientific question: Is the maximum forced expiatory volume (FEV) related to smoking status in children?\n\n-   Age ranges from 3 to 19, but no child under 9 smokes in the sample\n\n-   Models we will compare\n\n    -   Unadjusted (simple) model: FEV and smoking\n\n    -   Adjusted for age: FEV and smoking with age (confounder)\n\n    -   Adjusted for age, height: FEV and smoking with age (confounder) and height (precision)\n\n<!-- -->\n\n```         \n. regress logfev smoker if age>=9, robust\n\nLinear regression                                      Number of obs =     439\n                                                       F(  1,   437) =   10.45\n                                                       Prob > F      =  0.0013\n                                                       R-squared     =  0.0212\n                                                       Root MSE      =  .24765\n\n------------------------------------------------------------------------------\n             |               Robust\n      logfev |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n      smoker |   .1023056   .0316539     3.23   0.001     .0400927    .1645184\n       _cons |    1.05817   .0129335    81.82   0.000     1.032751     1.08359\n------------------------------------------------------------------------------\n\n. \n. regress logfev smoker age if age>=9, robust\n\nLinear regression                                      Number of obs =     439\n                                                       F(  2,   436) =   82.28\n                                                       Prob > F      =  0.0000\n                                                       R-squared     =  0.3012\n                                                       Root MSE      =  .20949\n\n------------------------------------------------------------------------------\n             |               Robust\n      logfev |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n      smoker |  -.0513495   .0343822    -1.49   0.136     -.118925     .016226\n         age |   .0635957   .0051401    12.37   0.000     .0534932    .0736981\n       _cons |   .3518165   .0575011     6.12   0.000     .2388028    .4648303\n------------------------------------------------------------------------------\n\n. \n. regress logfev smoker age loght if age >=9, robust\n\nLinear regression                                      Number of obs =     439\n                                                       F(  3,   435) =  284.22\n                                                       Prob > F      =  0.0000\n                                                       R-squared     =  0.6703\n                                                       Root MSE      =  .14407\n\n------------------------------------------------------------------------------\n             |               Robust\n      logfev |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n      smoker |  -.0535896   .0241879    -2.22   0.027    -.1011293     -.00605\n         age |   .0215295   .0034817     6.18   0.000     .0146864    .0283725\n       loght |   2.869658   .1279943    22.42   0.000     2.618093    3.121222\n       _cons |  -11.09461   .5153323   -21.53   0.000    -12.10746   -10.08176\n------------------------------------------------------------------------------\n```\n\n## Unadjusted versus Adjusted Models\n\n### General comparison\n\n-   Adjusting for covariates changes the scientific question\n\n    -   Unadjusted models: Slope compares parameters across groups differing by 1 unit in the modeled predictor\n\n        -   Groups may also differ with respect to other variables\n\n    -   Adjusted models: Slope compares parameters across groups differing by 1 unit in the modeled predictor but similar with respect to other model covariates\n\n-   Interpretation of Slopes\n\n    -   Unadjusted model: $g(\\theta | X_i) = \\beta_0 + \\beta_1 \\times X_i$\n\n        -   $\\beta_1$: Compares $\\theta$ for groups differing by 1 unit in $X$\n\n            -   (The distribution of $W$ might differ across groups being compared)\n\n    -   Adjusted model: $g(\\theta | X_i, W_i) = \\gamma_0 + \\gamma_1 \\times X_i + \\gamma_2 \\times W_i$\n\n        -   $\\gamma_1$: Compares $\\theta$ for groups differing by 1 unit in $X$, but agreeing on their values of $W$\n\n-   Comparing unadjusted and adjusted models\n\n    -   Science questions\n\n        -   When does $\\gamma_1 = \\beta_1$?\n\n        -   When does $\\hat{\\gamma}_1 = \\hat{\\beta}_1$?\n\n    -   Statistics questions\n\n        -   When does $se(\\hat{\\gamma_1}) = se(\\hat{\\beta_1})$?\n\n        -   When does $\\hat{se}(\\hat{\\gamma_1}) = \\hat{se}(\\hat{\\beta_1})$\n\n    -   In above, note placement of the $\\hat{ }$ (\"hat\") which signifies estimates of population parameters\n\n    -   Lack of a hat indicates \"the truth\" in the population\n\n    -   When $\\hat{\\gamma}_1 = \\hat{\\beta}_1$ (the formulas are the same), then it must be the case that $se(\\hat{\\gamma_1}) = se(\\hat{\\beta_1})$\n\n        -   But our estimates of the standard errors may not be the same\n\n    -   Example of when $\\gamma_1 = \\beta_1$\n\n        -   Want to compare smokers to non-smokers (POI) with respect to their FEV (response) and have conducted a randomized experiment in which boys and girls ($W$) are equally represented as smokers and non-smokers\n\n        -   When we compare a random smoker to a random non-smoker, that average difference will be the same if we adjust for gender or not\n\n        -   Numbers in unadjusted and adjusted analyses are the same, but interpretation is different\n\n-   Answering these four questions cannot be done in the general case\n\n    -   However, in linear regression we can derive exact results\n\n    -   These exact results can serve as a basis for examination of other regression models\n\n        -   Logistic regression\n\n        -   Poisson regression\n\n        -   Proportional hazards regression\n\n### Comparison of Adjusted and Unadjusted Models in Linear Regression\n\n#### Interpretation of Slopes\n\n-   Unadjusted model: $E[Y_i | X_i] = \\beta_0 + \\beta_1 \\times X_i$\n\n    -   $\\beta_1$: Difference in mean $Y$ for groups differing by 1 unit in $X$\n\n        -   (The distribution of $W$ might differ across groups being compared)\n\n-   Adjusted model: $E[Y_i | X_i, W_i] = \\gamma_0 + \\gamma_1 \\times X_i + \\gamma_2 \\times W_i$\n\n    -   $\\gamma_1$: Difference in mean $Y$ for groups differing by 1 unit in $X$, but agreeing in their values of $W$\n\n#### Relationships: True Slopes\n\n-   The slope of the unadjusted model will tend to be\n\n    $$\\beta_1 = \\gamma_1 + \\rho_{XW} \\frac{\\sigma_W}{\\sigma_X} \\gamma_2$$\n\n-   Hence, true adjusted and unadjusted slopes for $X$ are estimating the same quantity on if\n\n    -   $\\rho_{XW} = 0$ ($X$ and $W$ are truly uncorrelated), OR\n\n    -   $\\gamma_2 = 0$ (no association between $W$ and $Y$ after adjusting for $X$)\n\n#### Relationships: Estimated Slopes\n\n-   The estimated slope of the unadjusted model will be\n\n    $$\\hat{\\beta}_1 = \\hat{\\gamma}_1\\left(1 + \\hat{\\gamma}_2 r_{XW} \\left[\\frac{s_W}{s_X(r_{YX} - r_{YW} r_{XW}} \\right] \\right)$$\n\n-   Hence, estimated adjusted and unadjusted slopes for $X$ are equal only if\n\n    -   $r_{XW} = 0$ ($X$ and $W$ are uncorrelated in the sample, which can be arranged by experimental design, OR\n\n    -   $\\hat{\\gamma}_2 = 0$ (which cannot be predetermined because $Y$ is random)\n\n#### Relationships: True SE\n\n-   Unadjusted model: $[se(\\hat{\\beta}_1)]^2 = \\frac{Var(Y|X)}{n Var(X)}$\n\n-   Adjusted model: $[se(\\hat{\\gamma}_1)]^2 = \\frac{Var(Y|X, W)}{n Var(X)(1 - r^2_{XW})}$\n\n    $$\\begin{aligned}\n        Var(Y|X) & = & \\gamma_2^2 Var(W|X) + Var(Y | X, W) \\\\\n        \\sigma^2_{Y|X} & = & \\gamma_2^2 \\sigma^2_{W|X} + \\sigma^2_{Y|X,W}\n       \\end{aligned}$$\n\n-   Hence, $se(\\hat{\\beta}_1) = se(\\hat{\\gamma}_1)$ if,\n\n    -   $r_{XW} = 0$ AND\n\n    -   $\\gamma_2 = 0$ OR $Var(W|X) = 0$\n\n#### Relationships: Estimated SE\n\n-   Unadjusted model: $[\\hat{se}(\\hat{\\beta}_1)]^2 = \\frac{SSE(Y|X)/(n-2)}{(n-1) s^2_X}$\n\n-   Adjusted model: $[\\hat{se}(\\hat{\\gamma}_1)]^2 = \\frac{SSE(Y|X, W)/(n-3)}{(n-1) s^2_X (1 - r^2_{XW})}$\n\n    $$\\begin{aligned}\n        SSE(Y|X) & = & \\sum(Y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 \\times X_i)^2 \\\\\n        SSE(Y|X, W) & = & \\sum(Y_i - \\hat{\\gamma}_0 - \\hat{\\gamma}_1 \\times X_i - \\hat{\\gamma}_2 \\times W_i)^2\n       \\end{aligned}$$\n\n-   Hence, $\\hat{se}(\\hat{\\beta}_1) = \\hat{se}(\\hat{\\gamma}_1)$ if,\n\n    -   $r_{XW} = 0$ AND\n\n    -   $SSE(Y|X)/(n-2) = SSE(Y|X,W)/(n-3)$\n\n-   Note than when calculated on the same data,\n\n    -   $SSE(Y|X) \\geq SSE(Y | X, W)$\n\n-   Now $\\hat{\\beta}_1 = \\hat{\\gamma}_1$ if\n\n    -   $\\hat{\\gamma}_2 = 0$, in which case $SSE(Y | X) = SSE(Y | X, W)$, OR\n\n    -   $r_{XW} = 0$, and $SSE(Y | X) > SSE(Y | X, W)$ if $\\hat{\\gamma}_2 \\neq 0$\n\n#### Summary: Special Cases\n\n-   We are interested in knowing the behavior of unadjusted and adjusted models according to whether\n\n    -   $X$ and $W$ are uncorrelated\n\n    -   $W$ is associated with $Y$ after adjustment for $X$\n\n-   The 4 key cases are summarized below\n\n    |                   | $r_{XW} = 0$ |  $r_{XW} \\neq 0$   |\n    |-------------------|:------------:|:------------------:|\n    | $\\gamma_2 \\neq 0$ |  Precision   |    Confounding     |\n    | $\\gamma_2 = 0$    |  Irrelevant  | Variance Inflation |\n\n-   When $X$ is associated with $W$, and $W$ is associated with $Y$ after we control for $X$, that is what we call confounding\n\n-   When $X$ is associated with $W$, and $W$ is not associated with $Y$ after we control for $X$, this inflates the variance of the association between $X$ and $Y$ (more on this follows)\n\n-   When $X$ is not associated with $W$, and $W$ is associated with $Y$ after we control for $X$, this increases the precision of our estimate of the association between $X$ and $Y$\n\n-   When $X$ is not associated with $W$, and $W$ is not associated with $Y$ after we control for $X$, there is no reason to be concerned with modeling $W$\n\n### Precision variables\n\n#### Precision in Linear Regression\n\n-   Adjusting for a true precision variable should not impact the point estimate of the association between the POI and the response, but will decrease the standard error\n\n-   $X, W$ independent in the population (or a completely randomized experiment) AND $W$ is associated with $Y$ independent of $X$\n\n    -   $\\rho_{XW} = 0$\n\n    -   $\\gamma_2 \\neq 0$\n\n        |            |                True Value                |                      Estimates                       |\n        |------------|:----------------------------------------:|:----------------------------------------------------:|\n        | Slopes     |           $\\beta_1 = \\gamma_1$           |        $\\hat{\\beta}_1 \\approx \\hat{\\gamma}_1$        |\n        | Std Errors | $se(\\hat{\\beta}_1) > se(\\hat{\\gamma}_1)$ | $\\hat{se}(\\hat{\\beta}_1) > \\hat{se}(\\hat{\\gamma}_1)$ |\n\n#### Precision in Logistic Regression\n\n-   Can no longer use the formulas for linear regression\n\n-   Adjusting for a precision variable\n\n    -   Deattenuates slope away from the null\n\n    -   Standard errors reflect the mean-variance relationship\n\n        |                         |                True Value                |                      Estimates                       |\n        |-------------------------|:----------------------------------------:|:----------------------------------------------------:|\n        | Slopes if $\\beta_1 > 0$ |           $\\beta_1 < \\gamma_1$           |           $\\hat{\\beta}_1 < \\hat{\\gamma}_1$           |\n        | Slopes if $\\beta_1 < 0$ |           $\\beta_1 > \\gamma_1$           |           $\\hat{\\beta}_1 > \\hat{\\gamma}_1$           |\n        | Std Errors              | $se(\\hat{\\beta}_1) < se(\\hat{\\gamma}_1)$ | $\\hat{se}(\\hat{\\beta}_1) < \\hat{se}(\\hat{\\gamma}_1)$ |\n\n-   Note that the standard errors will be smaller in the unadjusted model due to the mean-variance relationship\n\n    -   Proportions have minimum variance when $p$ gets close to 0 or 1, and maximum variance at $p = 0.5$\n\n    -   Odds have minimum variance when $p$ is $0.5$\n\n    -   Precision variables should be driving probabilities toward 0 or 1\n\n#### Precision in Poisson Regression\n\n-   Adjusting for a precision variable will\n\n    -   Have no effect on the slope (log ratios are linear in log means)\n\n    -   Standard errors reflect the mean-variance relationship (virtually no effect on power)\n\n        |            |                   True Value                   |                         Estimates                          |\n        |------------|:----------------------------------------------:|:----------------------------------------------------------:|\n        | Slopes     |              $\\beta_1 = \\gamma_1$              |           $\\hat{\\beta}_1 \\approx \\hat{\\gamma}_1$           |\n        | Std Errors | $se(\\hat{\\beta}_1) \\approx se(\\hat{\\gamma}_1)$ | $\\hat{se}(\\hat{\\beta}_1) \\approx \\hat{se}(\\hat{\\gamma}_1)$ |\n\n#### Precision in PH Regression\n\n-   Adjusting for a precision variable\n\n    -   Deattenuates slope away from the null\n\n    -   Standard errors stay fairly constant (complicated result of binomial mean-variance relationship)\n\n        |                         |                   True Value                   |                         Estimates                          |\n        |-------------------------|:----------------------------------------------:|:----------------------------------------------------------:|\n        | Slopes if $\\beta_1 > 0$ |              $\\beta_1 < \\gamma_1$              |              $\\hat{\\beta}_1 < \\hat{\\gamma}_1$              |\n        | Slopes if $\\beta_1 < 0$ |              $\\beta_1 > \\gamma_1$              |              $\\hat{\\beta}_1 > \\hat{\\gamma}_1$              |\n        | Std Errors              | $se(\\hat{\\beta}_1) \\approx se(\\hat{\\gamma}_1)$ | $\\hat{se}(\\hat{\\beta}_1) \\approx \\hat{se}(\\hat{\\gamma}_1)$ |\n\n-   Will get some gain in power due to deattenuation of $\\beta_1$ while standard errors remain similar\n\n-   However, it is rare to have PH assumption hold for both adjusted and unadjusted models\n\n#### Stratified Randomization in Linear Regression\n\n-   Stratified randomization in a designed experiment\n\n    -   $r_{XW} = 0$\n\n    -   $\\gamma_2 \\neq 0$\n\n        |            |                True Value                |                      Estimates                       |\n        |------------|:----------------------------------------:|:----------------------------------------------------:|\n        | Slopes     |           $\\beta_1 = \\gamma_1$           |           $\\hat{\\beta}_1 = \\hat{\\gamma}_1$           |\n        | Std Errors | $se(\\hat{\\beta}_1) = se(\\hat{\\gamma}_1)$ | $\\hat{se}(\\hat{\\beta}_1) > \\hat{se}(\\hat{\\gamma}_1)$ |\n\n-   Need to adjust for the blocking variable in regression analysis to get improved standard error\n\n### Confounding variables\n\n#### Confounding in Linear Regression\n\n-   Causally associated with the response and associated with POI in the sample\n\n    -   $r_{XW} \\neq 0$\n\n    -   $\\gamma_2 \\neq 0$\n\n        |            |                             True Value                              |                                                          Estimates                                                           |\n        |------------|:-------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------------------------------------:|\n        | Slopes     | $\\beta_1 = \\gamma_1 + \\rho_{XW} \\frac{\\sigma_W}{\\sigma_X} \\gamma_2$ | $\\hat{\\beta}_1 = \\hat{\\gamma}_1\\left(1 + \\hat{\\gamma}_2 r_{XW} \\left[\\frac{s_W}{s_X(r_{YX} - r_{YW} r_{XW}} \\right] \\right)$ |\n        | Std Errors |        $se(\\hat{\\beta}_1) \\{>, =, or <\\} se(\\hat{\\gamma}_1)$        |                              $\\hat{se}(\\hat{\\beta}_1) \\{>, =, or <\\} \\hat{se}(\\hat{\\gamma}_1)$                               |\n\n-   Slopes could increase or decrease\n\n-   Standard errors could increase, decrease, or stay the same\n\n    -   Competition of greater precision with variance inflation\n\n#### Confounding in Other Regressions\n\n-   With logistic, Poisson, PH regression we cannot write down formula, but\n\n    -   As in linear regression, anything can happen\n\n        |            |                      True Value                       |                             Estimates                             |\n        |------------|:-----------------------------------------------------:|:-----------------------------------------------------------------:|\n        | Slopes     |           $\\beta_1 \\{>, =, or <\\} \\gamma_1$           |           $\\hat{\\beta}_1 \\{>, =, or <\\} \\hat{\\gamma}_1$           |\n        | Std Errors | $se(\\hat{\\beta}_1) \\{>, =, or <\\} se(\\hat{\\gamma}_1)$ | $\\hat{se}(\\hat{\\beta}_1) \\{>, =, or <\\} \\hat{se}(\\hat{\\gamma}_1)$ |\n\n-   Slopes could increase or decrease\n\n-   Standard errors could increase, decrease, or stay the same\n\n### Variance Inflation\n\n#### Variance Inflation in Linear Regression\n\n-   Associated with POI in the sample, but not associated with response\n\n    -   $r_{XW} \\neq 0$\n\n    -   $\\gamma_2 = 0$\n\n        |            |                True Value                |                                                          Estimates                                                           |\n        |------------|:----------------------------------------:|:----------------------------------------------------------------------------------------------------------------------------:|\n        | Slopes     |           $\\beta_1 = \\gamma_1$           | $\\hat{\\beta}_1 = \\hat{\\gamma}_1\\left(1 + \\hat{\\gamma}_2 r_{XW} \\left[\\frac{s_W}{s_X(r_{YX} - r_{YW} r_{XW}} \\right] \\right)$ |\n        | Std Errors | $se(\\hat{\\beta}_1) < se(\\hat{\\gamma}_1)$ |                                     $\\hat{se}(\\hat{\\beta}_1) < \\hat{se}(\\hat{\\gamma}_1)$                                     |\n\n#### Variance Inflation in Other Regressions\n\n-   With logistic, Poisson, PH regression we cannot write down formula, but\n\n    -   Similar to linear regression\n\n        |            |                True Value                |                      Estimates                       |\n        |------------|:----------------------------------------:|:----------------------------------------------------:|\n        | Slopes     |           $\\beta_1 = \\gamma_1$           |    $\\hat{\\beta}_1 \\{>, =, or <\\} \\hat{\\gamma}_1$     |\n        | Std Errors | $se(\\hat{\\beta}_1) < se(\\hat{\\gamma}_1)$ | $\\hat{se}(\\hat{\\beta}_1) < \\hat{se}(\\hat{\\gamma}_1)$ |\n\n### Irrelevant variables\n\n-   Uncorrelated with POI in sample, and not associated with response\n\n    -   $r_{XW} = 0$\n\n    -   $\\gamma_2 = 0$\n\n-   Inclusion of irrelevant variables results in slight loss in precision in all regressions\n\n    |            |                True Value                |                      Estimates                       |\n    |------------|:----------------------------------------:|:----------------------------------------------------:|\n    | Slopes     |           $\\beta_1 = \\gamma_1$           |           $\\hat{\\beta}_1= \\hat{\\gamma}_1$            |\n    | Std Errors | $se(\\hat{\\beta}_1) = se(\\hat{\\gamma}_1)$ | $\\hat{se}(\\hat{\\beta}_1) < \\hat{se}(\\hat{\\gamma}_1)$ |\n\n## Example: FEV and Smoking in Children\n\n### Linear model for geometric means\n\n-   Association between lung function and self-reported smoking in children\n\n-   Compare geometric means of FEV of children who smoke to comparable non-smokers\n\n-   Restrict analysis to children 9 years and older\n\n    -   No smokers less than 9\n\n    -   Still about 6 : 1 ratio of non-smokers to smokers\n\n        -   Little precision gained by keeping younger children\n\n        -   Borrowing information from young kids problematic if not a linear relationship between log(FEV) and predictors\n\n        -   With confounding, want to get the model correct\n\n-   Academic Exercise: Compare alternative models\n\n    -   In real life, we should choose a single model in advance of looking at the data\n\n    -   Here we will observe what happens to parameter estimates and SE across models\n\n        -   Smoking\n\n        -   Smoking adjusted for age\n\n        -   Smoking adjusted for age and height\n\n#### Unadjusted Model\n\n```         \n. regress logfev smoker if age>=9, robust\n\nLinear regression                                      Number of obs =     439\n                                                       F(  1,   437) =   10.45\n                                                       Prob > F      =  0.0013\n                                                       R-squared     =  0.0212\n                                                       Root MSE      =  .24765\n\n------------------------------------------------------------------------------\n             |               Robust\n      logfev |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n      smoker |   .1023056   .0316539     3.23   0.001     .0400927    .1645184\n       _cons |    1.05817   .0129335    81.82   0.000     1.032751     1.08359\n------------------------------------------------------------------------------\n```\n\n-   Intercept\n\n    -   Geometric mean of FEV in nonsmokers is 2.88 l/sec\n\n        -   The scientific relevance is questionable here because we do not really know the population our sample represents\n\n        -   Calculations $e^{1.05817} = 2.88$\n\n        -   The p-value is completely unimportant here as it is testing that the log geometric mean is 0, or that the geometric mean is 1. Why would we care?\n\n    -   Because smoker is a binary variable, the estimate corresponds to a geometric mean. In many regression models, the intercept will have not interpretation\n\n-   Smoking effect\n\n    -   Geometric mean of FEV is $10.8\\%$ higher in smokers than in nonsmokers (95% CI: $4.1\\%$ to $17.9\\%$ higher)\n\n        -   These results are atypical of what we might expect with no true difference between groups ($p = 0.001$)\n\n        -   Calculations: $e^{.102} = 1.108; e^{.040} = 1.041; e^{0.165} = 1.179$\n\n        -   (Note that $e^x$ is approximately $(1+x)$ for $x$ close to 1)\n\n    -   Because smoker is a binary variable, this analysis is nearly identical to a two sample t-test allowing for unequal variances\n\n#### Adjusted for Age\n\n```         \n. regress logfev smoker age if age>=9, robust\n\nLinear regression                                      Number of obs =     439\n                                                       F(  2,   436) =   82.28\n                                                       Prob > F      =  0.0000\n                                                       R-squared     =  0.3012\n                                                       Root MSE      =  .20949\n\n------------------------------------------------------------------------------\n             |               Robust\n      logfev |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n      smoker |  -.0513495   .0343822    -1.49   0.136     -.118925     .016226\n         age |   .0635957   .0051401    12.37   0.000     .0534932    .0736981\n       _cons |   .3518165   .0575011     6.12   0.000     .2388028    .4648303\n------------------------------------------------------------------------------\n```\n\n-   Intercept\n\n    -   Geometric mean of FEV in newborn nonsmokers is 1.42 l/sec\n\n        -   Intercept corresponds to the log geometric mean in a group having all predictors equal to $0$\n\n        -   There is no scientific relevance here as we are extrapolating beyond the range of our data\n\n        -   Calculations $e^{0.352} = 1.422$\n\n-   Age effect\n\n    -   Geometric mean of FEV is $6.6\\%$ higher for each year difference in age between two groups with the same smoking status (95% CI: $5.5\\%$ to $7.6\\%$ higher)\n\n        -   These results are highly atypical of what we might expect with no true difference in geometric means between age groups having similar smoking status ($p < 0.001$)\n\n-   Smoking effect (age adjusted interpretation)\n\n    -   Geometric mean of FEV is $5.0\\%$ lower in smokers than in nonsmokers of the same age (95% CI: $12.2\\%$ lower to $1.6\\%$ higher)\n\n        -   These results are not atypical of what we might expect with no true difference between groups of the same age ($p = 0.136$)\n\n        -   Lack of statistical significance can also be noted by the fact that the CI for the ratio contain 1 or the CI for the percent difference contains 0\n\n        -   Calculations: $e^{-.051} = 0.950; e^{-.119} = 0.888; e^{0.016} = 1.016$\n\n-   Comparing unadjusted and age adjusted analyses\n\n    -   Marked differences in effect of smoking suggests that there was indeed confounding\n\n        -   Age is a relatively strong predictor of FEV\n\n        -   Age is associated with smoking in the sample\n\n            -   Mean (SD) of age in analyzed smokers: 11.1 (2.04)\n\n            -   Mean (SD) of age in analyzed nonsmokers: 13.5 (2.34)\n\n    -   Effect of age adjustment on precision\n\n        -   Lower Root MSE (0.209 vs 0.248) would tend to increase precision of estimate of smoking effect\n\n        -   Association between smoking and age tends to lower precision\n\n        -   Net effect: Less precision (adj SE 0.034 vs unadj SE 0.031)\n\n#### Adjusted for Age and Height\n\n```         \n. regress logfev smoker age loght if age >=9, robust\n\nLinear regression                                      Number of obs =     439\n                                                       F(  3,   435) =  284.22\n                                                       Prob > F      =  0.0000\n                                                       R-squared     =  0.6703\n                                                       Root MSE      =  .14407\n\n------------------------------------------------------------------------------\n             |               Robust\n      logfev |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n      smoker |  -.0535896   .0241879    -2.22   0.027    -.1011293     -.00605\n         age |   .0215295   .0034817     6.18   0.000     .0146864    .0283725\n       loght |   2.869658   .1279943    22.42   0.000     2.618093    3.121222\n       _cons |  -11.09461   .5153323   -21.53   0.000    -12.10746   -10.08176\n------------------------------------------------------------------------------\n```\n\n-   Intercept\n\n    -   Geometric mean of FEV in newborn nonsmokers who are 1 inch in height is 0.000015 l/sec\n\n        -   Intercept corresponds to the log geometric mean in a group having all predictors equal to $0$ (nonsmokers, age 0, log height 0)\n\n        -   There is no scientific relevance because there are no such people in our data in either our sample or the population\n\n-   Age effect\n\n    -   Geometric mean of FEV is $2.2\\%$ higher for each year difference in age between two groups with the same height and smoking status (95% CI: $1.5\\%$ to $2.9\\%$ higher for each year difference in age)\n\n        -   These results are highly atypical of what we might expect with no true difference in geometric means between age groups having similar smoking status and height ($p < 0.001$)\n\n    -   Note that there is clear evidence that height confounded the age effect estimated in the analysis that only considered age and smoking, but there is still a clearly independent effect of age on FEV\n\n-   Height effect\n\n    -   Geometric mean of FEV is $31.5\\%$ higher for each $10\\%$ difference in height between two groups with the same age and smoking status (95% CI: $28.3\\%$ to $34.6\\%$ higher for each $10\\%$ difference in height)\n\n        -   These results are highly atypical of what we might expect with no true difference in geometric means between height groups having similar smoking status and age ($p < 0.001$)\n\n        -   Calculations: $1.1^{2.867} = 1.315$\n\n    -   Note that the CI for the regression coefficient is consistent with the scientifically-hypothesize value of 3\n\n-   Smoking effect (age, height adjusted interpretation)\n\n    -   Geometric mean of FEV is $5.2\\%$ lower in smokers than in nonsmokers of the same age and height (95% CI: $9.6\\%$ to $0.6\\%$ lower)\n\n        -   These results are atypical of what we might expect with no true difference between groups of the same age and height ($p = 0.027$)\n\n        -   Calculations: $e^{-.054} = 0.948; e^{-.101} = 0.904; e^{-0.006} = .994$\n\n-   Comparing age-adjusted to age- and height-adjusted analyses\n\n    -   No difference in effect of smoking suggests that there was no more confounding after age adjustment\n\n    -   Effect of height adjustment on precision\n\n        -   Lower Root MSE (0.144 vs 0.209) would tend to increase precision of estimate of smoking effect\n\n        -   Little association between smoking and height after adjusting for age will not tend to lower precision\n\n        -   Net effect: Higher precision (adj SE 0.024 vs unadj SE 0.034)\n\n### Logistic model\n\n-   Continue our academic exercise using logistic regression\n\n    -   Dichotomize FEV at median (2.93)\n\n    -   In real life, we would not want to make a continuous variable like FEV into a binary variable\n\n    -   Here we will observe what happens to parameter estimates and SE across models\n\n        -   Smoking\n\n        -   Smoking adjusted for age\n\n        -   Smoking adjusted for height\n\n        -   Smoking adjusted for age and height\n\n<!-- -->\n\n```         \n. logit fevbin smoker if age>=9\nLogistic regression                               Number of obs   =        439\n                                                  LR chi2(1)      =      15.81\n                                                  Prob > chi2     =     0.0001\nLog likelihood = -296.38409                       Pseudo R2       =     0.0260\n\n------------------------------------------------------------------------------\n      fevbin |      Coef.   Std. Err.      z    P>|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n      smoker |   1.120549   .2959672     3.79   0.000      .540464    1.700634\n       _cons |  -.1607732   .1037519    -1.55   0.121    -.3641231    .0425767\n------------------------------------------------------------------------------\n\n. \n. logit fevbin smoker age if age>=9\nLogistic regression                               Number of obs   =        439\n                                                  LR chi2(2)      =      88.68\n                                                  Prob > chi2     =     0.0000\nLog likelihood = -259.95032                       Pseudo R2       =     0.1457\n\n------------------------------------------------------------------------------\n      fevbin |      Coef.   Std. Err.      z    P>|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n      smoker |   .1971403   .3402482     0.58   0.562     -.469734    .8640146\n         age |    .470792   .0637495     7.39   0.000     .3458453    .5957386\n       _cons |  -5.360912   .7056265    -7.60   0.000    -6.743914   -3.977909\n------------------------------------------------------------------------------\n\n. \n. logit fevbin smoker loght if age>=9\nLogistic regression                               Number of obs   =        439\n                                                  LR chi2(2)      =     224.11\n                                                  Prob > chi2     =     0.0000\nLog likelihood = -192.23646                       Pseudo R2       =     0.3682\n\n------------------------------------------------------------------------------\n      fevbin |      Coef.   Std. Err.      z    P>|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n      smoker |   .4159311   .3646506     1.14   0.254    -.2987709    1.130633\n       loght |   33.08872   3.169516    10.44   0.000     26.87659    39.30086\n       _cons |  -137.6043   13.16723   -10.45   0.000    -163.4116    -111.797\n------------------------------------------------------------------------------\n\n. \n. logit fevbin smoker age loght if age>=9\nLogistic regression                               Number of obs   =        439\n                                                  LR chi2(3)      =     230.19\n                                                  Prob > chi2     =     0.0000\nLog likelihood = -189.19487                       Pseudo R2       =     0.3782\n\n------------------------------------------------------------------------------\n      fevbin |      Coef.   Std. Err.      z    P>|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n      smoker |    .047695   .3933191     0.12   0.903    -.7231963    .8185863\n         age |   .1753759   .0723176     2.43   0.015      .033636    .3171157\n       loght |   31.04791   3.289031     9.44   0.000     24.60152    37.49429\n       _cons |  -131.0647    13.5393    -9.68   0.000    -157.6013   -104.5282\n------------------------------------------------------------------------------\n```\n\n#### Results Summarized\n\n-   Coefficients (logit scale)\n\n    | Model        | Smoker | Age  | Log(Height) |\n    |:-------------|:------:|:----:|:-----------:|\n    | Smoke        |  1.12  |      |             |\n    | Smoke+Age    |  0.19  | 0.47 |             |\n    | Smoke+Ht     |  0.42  |      |    33.1     |\n    | Smoke+Age+Ht |  0.47  | 0.18 |    31.0     |\n\n-   Std Errors (logit scale)\n\n    | Model        | Smoker | Age  | Log(Height) |\n    |:-------------|:------:|:----:|:-----------:|\n    | Smoke        |  0.30  |      |             |\n    | Smoke+Age    |  0.34  | 0.06 |             |\n    | Smoke+Ht     |  0.36  |      |     3.2     |\n    | Smoke+Age+Ht |  0.39  | 0.07 |     3.3     |\n\n-   Adjusting for the height\n\n    -   Comparing Smoke+Age to Smoke+Age+Ht\n\n        -   Height is being added as a precision variable\n\n        -   Deattenuated the smoking effect (estimate changes from 0.19 to 0.47, which is further from 0)\n\n        -   Increased the standard error estimate (0.34 to 0.39)\n\n        -   Both of these change are predicted by the previous discusssion\n\n    -   Comparing Smoke to Smoke+Ht\n\n        -   Attenuated the smoking effect (1.12 to 0.42, closer to 0)\n\n        -   Increased the standard error estimate\n\n        -   Height now also acting acting as a confounding variable (because we are not modeling age), so these changes were not predictable\n\n-   Adjusting for the confounding variable (age)\n\n    -   Comparing Smoke+Ht to Smoke+Age+Ht\n\n        -   Deattenuated the estimated coefficient (0.42 to 0.47)\n\n        -   Increased the standard error\n\n    -   Comparing Smoke to Smoke+Age\n\n        -   Attenuated the coefficient estimate (1.12 to 0.19)\n\n        -   Increased the standard error\n\n    -   Changes in estimates and standard errors were not predictable for this, or any, confounder\n",
    "supporting": [
      "Lec07.MultivariableModels_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}