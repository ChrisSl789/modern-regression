{
  "hash": "294616864fc7b88ed7849149ac71b3db",
  "result": {
    "markdown": "---\ntitle: \"Modeling Effect Modification\"\nsubtitle: \"Lecture 10\"\nauthor: \"Chris Slaughter\"\nfooter: \"Bios 6312\"\ndate: last-modified\nformat:\n  html:\n    embed-resources: true\n    standalone: true\n    number-sections: true\n    number-depth: 4\n    anchor-sections: true\n    smooth-scroll: true\n    theme: journal\n    toc: true\n    toc-depth: 4\n    toc-title: Contents\n    toc-location: left\n    code-link: false\n    code-tools: true\n    code-fold: true\n    code-block-bg: \"#f1f3f5\"\n    code-block-border-left: \"#31BAE9\"\n    reference-location: margin\n    fig-cap-location: margin\n    fontsize: medium\n\nexecute:\n   warning: false\n   message: false\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rms)\nlibrary(ggplot2)\nlibrary(biostat3)\nlibrary(car)\n```\n:::\n\n\n## Overview\n\n-   Scientific questions\n\n    -   Most often scientific questions are translated into comparing the distribution of some response variable across groups of interest\n\n    -   Groups are defined by the predictor of interest (POI)\n\n    -   Effect modification evaluates if the association between the outcome and POI is modified by strata defined by a third, modifying variable\n\n    -   Examples of effect modification\n\n        -   Binary effect modification: If we stratify by gender, we may get different answers to our scientific question in men and women\n\n        -   Continuous effect modification: If we stratify by age, we way get different answers to our scientific question in young and old\n\n-   The association between the Response and the Predictor of Interest differs in strata defined by the effect modifier\n\n    -   Will get different answers to the scientific question within different strata defined by the effect modifier\n\n    -   Statistical term: \"Interaction\" between the effect modifier and the POI\n\n    -   Effect modification depends on the measure of effect that you choose\n\n        -   Choice of summary measure: mean, median, geometric mean, odds, hazard\n\n        -   Choice of comparisons across groups: differences, ratios\n\n-   We will also do some model diagnostics through examples\n\n    -   When looking at effect modification, I am also worried that one point may be driving the interaction\n\n    -   Will go over case diagnostics in an example analysis of somatosensory evoked potential (SEP)\n\n-   Graphical displays for effect modification\n\n    -   When analyzing difference of means of continuous data, stratified smooth curves of the data are non-parallel\n\n    -   Graphical techniques more difficult for odds and hazards\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1231)\nn <- 200\ngrp <- rep(c(0,1),each=n/2)\nX <- runif(n)\nemplot <- data.frame(grp=grp,\n                     X=X,\n                     Y=.1*grp + 2*X -4*grp*X + rnorm(n)\n)\nemplot$group <- factor(emplot$grp, levels=0:1, labels=c(\"Trt\",\"Ctrl\"))\nggplot(emplot, aes(x=X, y=Y)) + geom_point() + geom_smooth(se=FALSE) + theme_bw()\n```\n\n::: {.cell-output-display}\n![Unadjusted association between outcome and predictor shows a flat slope.](Lec10.Modeling.Effect.Modification_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(emplot, aes(x=X, y=Y, color=group, grp=group)) + geom_point() + geom_smooth(se=FALSE,) + theme_bw()\n```\n\n::: {.cell-output-display}\n![Adjusted association between outcome and predictor shows an increasing slope in one group and a decreasing slope in the other.  This demonstrates effect modification of group.](Lec10.Modeling.Effect.Modification_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n## Analysis of Effect Modification\n\n-   When the scientific question involves effect modification, analyses must be performed within each stratum separately\n\n-   If we want to test the degree of effect modification or test statistically, the regression model will typically include\n\n    -   Predictor of interest\n\n    -   Effect modifier\n\n    -   A covariate modeling the interaction (usually a product)\n\n### Example: Is blood pressure by gender modified by smoking?\n\n|       |           |          |           |          |\n|-------|-----------|----------|-----------|----------|\n|       | Non-Smoke | Smoke    | Non-smoke | Smoke    |\n| Women | 107.4     | 135.2    | 105.6     | 134.7    |\n| Men   | 122.7     | 174.7    | 121.1     | 174.1    |\n|       |           |          |           |          |\n| Diff  | **15.3**  | **39.5** | 15.5      | 39.4     |\n| Ratio | 1.14      | 1.28     | **1.15**  | **1.29** |\n\n#### Model for the Mean\n\n-   Model: $Y = \\beta_0 + \\beta_M * \\textrm{Male} + \\beta_S * \\textrm{Smoke} + \\beta_{MS} * \\textrm{Male} * \\textrm{Smoke} + \\epsilon$\n\n    -   Male and Smoke are modeled using indicator variables\n\n        -   Male is 1 for males, 0 for females\n\n        -   Smoke is 1 for smokers, 0 for non-smokers\n\n    -   Estimates of the mean from the regression model are given in the following table\n\n    |                |                      |                                            |\n    |-------------------|-------------------|----------------------------------|\n    |                | Non-smoker (smoke=0) | Smoker (smoke=1)                           |\n    | Women (male=0) | $\\beta_0$            | $\\beta_0 + \\beta_S$                        |\n    | Men (male=1)   | $\\beta_0 + \\beta_M$  | $\\beta_0 + \\beta_S + \\beta_M + \\beta_{MS}$ |\n    | Difference     | $\\beta_M$            | $\\beta_M + \\beta_{MS}$                     |\n\n-   What is the scientific interpretation of $\\beta_{MS} = 0$?\n\n    -   If true, the Male to Female difference in smokers $= \\beta_M$\n\n    -   And, the Male to Female difference in non-smokers $= \\beta_M$\n\n    -   In other words, the effect of gender is *the same* in smoker and non-smokers\n\n        -   A test of $H_0$: $\\beta_{MS} = 0$ is a test of effect modification (for means, differences)\n\n#### Model for the Geometric Mean\n\n-   Model: $\\textrm{log}(Y) = \\beta_0 + \\beta_M * \\textrm{Male} + \\beta_S * \\textrm{Smoke} + \\beta_{MS} * \\textrm{Male} * \\textrm{Smoke} + \\epsilon$\n\n    -   Male and Smoke are modeled using indicator variables\n\n        -   Male is 1 for males, 0 for females\n\n        -   Smoke is 1 for smokers, 0 for non-smokers\n\n    -   Estimates of the geometric mean from the regression model are given in the following table\n\n    |                |                         |                                                |\n    |-------------------|-------------------|----------------------------------|\n    |                | Non-smoker (smoke=0)    | Smoker (smoke=1)                               |\n    | Women (male=0) | $e^{\\beta_0}$           | $e^{\\beta_0 + \\beta_S}$                        |\n    | Men (male=1)   | $e^{\\beta_0 + \\beta_M}$ | $e^{\\beta_0 + \\beta_S + \\beta_M + \\beta_{MS}}$ |\n    | Ratio          | $e^{\\beta_M}$           | $e^{\\beta_M + \\beta_{MS}}$                     |\n\n-   What is the scientific interpretation of $\\beta_{MS} = 0$?\n\n    -   If true, the Male to Female ratio in smokers $= e^{\\beta_M}$\n\n    -   And, the Male to Female ratio in non-smokers $= e^{\\beta_M}$\n\n    -   In other words, the effect of gender is *the same* in smoker and non-smokers\n\n        -   A test of $H_0$: $\\beta_{MS} = 0$ is a test of effect modification (for geometric means, ratios)\n\n#### R Output\n\n##### Means\n\nWe can obtain the means by male and smoking using summary statistics or from regression ouput\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbp <- read.csv(\"data/bp.csv\")\nbp$logbp <- log(bp$bp)\nbp$malesmoke <- bp$male * bp$smoke\n\nlibrary(dplyr)\nbp %>% group_by(smoke, male) %>% summarize(meanbp=mean(bp))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 3\n# Groups:   smoke [2]\n  smoke  male meanbp\n  <int> <int>  <dbl>\n1     0     0   107.\n2     0     1   123.\n3     1     0   135.\n4     1     1   175.\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nm1 <- lm(bp ~ male + smoke + male*smoke, data=bp)\n\nnewdata <- expand.grid(male=0:1, smoke=0:1)\n\n# Predicted means from the regression model.  Matches above table\ncbind(newdata, predict(m1, newdata))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  male smoke predict(m1, newdata)\n1    0     0                107.4\n2    1     0                122.7\n3    0     1                135.2\n4    1     1                174.7\n```\n:::\n\n```{.r .cell-code}\n# Four linear combinations using biostat3 function lincom and robust standard error\n#  to obtain the 4 means\nlincom(m1, c(\"(Intercept)\",\n             \"(Intercept) + male\",\n             \"(Intercept) + smoke\",\n             \"(Intercept) + male + smoke + male:smoke\"),\n       vcov=hccm(m1, type = \"hc3\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                                        Estimate 2.5 %    97.5 %   F       \n(Intercept)                             107.4    94.53401 120.266  267.6815\n(Intercept) + male                      122.7    109.3391 136.0609 323.9761\n(Intercept) + smoke                     135.2    126.4956 143.9044 926.7665\n(Intercept) + male + smoke + male:smoke 174.7    164.3974 185.0026 1104.565\n                                        Pr(>F)      \n(Intercept)                             2.996149e-18\n(Intercept) + male                      1.389668e-19\n(Intercept) + smoke                     2.745825e-27\n(Intercept) + male + smoke + male:smoke 1.296132e-28\n```\n:::\n:::\n\n\n\n\n###### Difference in mean bp, males versus females, in non-smokers\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Male effect in non-smokers\nlincom(m1, \"male\", vcov=hccm(m1, type = \"hc3\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     Estimate     2.5 %   97.5 %        F    Pr(>F)\nmale     15.3 -3.248523 33.84852 2.613728 0.1146731\n```\n:::\n:::\n\n\n###### Difference in mean bp, males versus females, in smokers\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Male effect in smokers\nlincom(m1, \"male + male:smoke\", vcov=hccm(m1, type = \"hc3\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                  Estimate    2.5 %   97.5 %        F       Pr(>F)\nmale + male:smoke     39.5 26.01261 52.98739 32.94842 1.546644e-06\n```\n:::\n:::\n\n\n\n###### Difference in mean bp, smokers versus non-smokers, in females\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Smoking effect in females\nlincom(m1, \"smoke\", vcov=hccm(m1, type = \"hc3\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      Estimate    2.5 %   97.5 %        F      Pr(>F)\nsmoke     27.8 12.26615 43.33385 12.30347 0.001232434\n```\n:::\n:::\n\n\n\n###### Difference in mean bp, smokers versus non-smokers, in males\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Smoking effect in males\nlincom(m1, \"smoke + male:smoke\", vcov=hccm(m1, type = \"hc3\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                   Estimate    2.5 %   97.5 %        F       Pr(>F)\nsmoke + male:smoke       52 35.12822 68.87178 36.49062 6.131127e-07\n```\n:::\n:::\n\n\n###### Test for effect modification\n\nIs the effect of smoking on mean bp modified by male?  Is the effect of male on mean bp modified by smoking?  Effect modification is symmetric.  The answer to these two questions is answered by testing for the significance of the interaction term between male and smoking.  The following answers both questions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Effect modification\nlincom(m1, \"male:smoke\", vcov=hccm(m1, type = \"hc3\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           Estimate   2.5 %   97.5 %        F     Pr(>F)\nmale:smoke     24.2 1.26624 47.13376 4.277366 0.04586541\n```\n:::\n:::\n\n\n\n\n\n\n##### Geometric Means\n\nWe can obtain the geometric means by male and smoking using summary statistics or from regression ouput\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nbp %>% group_by(smoke, male) %>% summarize(geomeanbp=exp(mean(log(bp))))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 3\n# Groups:   smoke [2]\n  smoke  male geomeanbp\n  <int> <int>     <dbl>\n1     0     0      106.\n2     0     1      121.\n3     1     0      135.\n4     1     1      174.\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nm2 <- lm(logbp ~ male + smoke + male*smoke, data=bp)\n\nnewdata <- expand.grid(male=0:1, smoke=0:1)\n\n# Predicted means from the regression model.  Matches above table\ncbind(newdata, exp(predict(m2, newdata)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  male smoke exp(predict(m2, newdata))\n1    0     0                  105.5572\n2    1     0                  121.0690\n3    0     1                  134.6539\n4    1     1                  174.0794\n```\n:::\n\n```{.r .cell-code}\n# Four linear combinations using biostat3 function lincom and robust standard error\n#  to obtain the 4 means\nlincom(m2, c(\"(Intercept)\",\n             \"(Intercept) + male\",\n             \"(Intercept) + smoke\",\n             \"(Intercept) + male + smoke + male:smoke\"),\n       vcov=hccm(m2, type = \"hc3\"),\n       eform=TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                                        Estimate 2.5 %    97.5 %   F       \n(Intercept)                             105.5572 92.42845 120.5509 4727.281\n(Intercept) + male                      121.069  107.9158 135.8254 6681.152\n(Intercept) + smoke                     134.6539 126.706  143.1003 24946.97\n(Intercept) + male + smoke + male:smoke 174.0794 164.3235 184.4145 30743.14\n                                        Pr(>F)      \n(Intercept)                             8.57959e-40 \n(Intercept) + male                      1.761937e-42\n(Intercept) + smoke                     9.482449e-53\n(Intercept) + male + smoke + male:smoke 2.217396e-54\n```\n:::\n:::\n\n\n\n\n###### Ratio of geometric mean bp, males versus females, in non-smokers\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Male effect in non-smokers\nlincom(m2, \"male\", vcov=hccm(m2, type = \"hc3\"), eform=TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     Estimate     2.5 %   97.5 %        F   Pr(>F)\nmale 1.146951 0.9621492 1.367248 2.339419 0.134877\n```\n:::\n:::\n\n\n###### Ratio of geometric mean bp, males versus females, in smokers\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Male effect in smokers\nlincom(m2, \"male + male:smoke\", vcov=hccm(m2, type = \"hc3\"), eform=TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                  Estimate    2.5 %  97.5 %        F       Pr(>F)\nmale + male:smoke 1.292792 1.188834 1.40584 36.04897 6.862878e-07\n```\n:::\n:::\n\n\n\n###### Ratio of geometric mean bp, smokers versus non-smokers, in females\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Smoking effect in females\nlincom(m2, \"smoke\", vcov=hccm(m2, type = \"hc3\"), eform=TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      Estimate    2.5 %   97.5 %       F      Pr(>F)\nsmoke 1.275648 1.102263 1.476307 10.6683 0.002396985\n```\n:::\n:::\n\n\n\n###### Ratio of geometric mean bp, smokers versus non-smokers, in males\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Smoking effect in males\nlincom(m2, \"smoke + male:smoke\", vcov=hccm(m2, type = \"hc3\"), eform=TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                   Estimate    2.5 %   97.5 %        F       Pr(>F)\nsmoke + male:smoke 1.437853 1.264265 1.635276 30.60412 2.934134e-06\n```\n:::\n:::\n\n\n###### Test for effect modification\n\nIs the effect of smoking on geometric mean bp modified by male?  Is the effect of male on geometric mean bp modified by smoking?  Effect modification is symmetric.  The answer to these two questions is answered by testing for the significance of the interaction term between male and smoking.  The following answers both questions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Effect modification\nlincom(m2, \"male:smoke\", vcov=hccm(m2, type = \"hc3\"), eform=TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           Estimate     2.5 %   97.5 %        F    Pr(>F)\nmale:smoke 1.127155 0.9277704 1.369389 1.452356 0.2360127\n```\n:::\n:::\n\n\n\n#### Stata Output\n\nSiimilar Stata output to obtain the results.\n\n```         \n.  insheet using \"/home/slaughjc/docs/teaching/b312/doc/effectmod/bp.csv\", clear\n(3 vars, 40 obs)\n. gen logbp = log(bp)\n. tabulate male smoke, summarize(bp) nostandard nofreq noobs\n\n                                Means of bp\n\n           |        smoke\n      male |         0          1 |     Total\n-----------+----------------------+----------\n         0 |     107.4      135.2 |     121.3\n         1 |     122.7      174.7 |     148.7\n-----------+----------------------+----------\n     Total |    115.05     154.95 |       135\n\n\n. di 122.7 - 107.4\n15.3\n\n. di 122.7 / 107.4\n1.1424581\n. \n. di 174.7 - 135.2\n39.5\n\n. di 174.7 / 135.2\n1.2921598\n\n. gen malesmoke = male*smoke\n. \n. ************************************************\n. * Linear Regression: Inference about means *\n. ************************************************\n. regress bp male smoke malesmoke, robust\n\nLinear regression                                      Number of obs =      40\n                                                       F(  3,    36) =   28.05\n                                                       Prob > F      =  0.0000\n                                                       R-squared     =  0.6918\n                                                       Root MSE      =  17.552\n\n------------------------------------------------------------------------------\n             |               Robust\n          bp |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n        male |       15.3    8.97806     1.70   0.097    -2.908349    33.50835\n       smoke |       27.8   7.518865     3.70   0.001     12.55103    43.04897\n   malesmoke |       24.2   11.10065     2.18   0.036     1.686837    46.71316\n       _cons |      107.4   6.227537    17.25   0.000     94.76997      120.03\n------------------------------------------------------------------------------\n\n. \n. * Get the means presented in the Table \n. *   Method 1: The adjust command         \n. *   Method 2: Four linear combos            \n. \n. adjust, by(male smoke)\n\n---------------------------------------------------------------------------------------\n     Dependent variable: bp     Command: regress\n    Variable left as is: malesmoke\n---------------------------------------------------------------------------------------\n\n------------------------\n          |    smoke    \n     male |     0      1\n----------+-------------\n        0 | 107.4  135.2\n        1 | 122.7  174.7\n------------------------\n     Key:  Linear Prediction\n\n. lincom _cons\n\n ( 1)  _cons = 0\n\n------------------------------------------------------------------------------\n          bp |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n         (1) |      107.4   6.227537    17.25   0.000     94.76997      120.03\n------------------------------------------------------------------------------\n\n. lincom _cons + male\n\n ( 1)  male + _cons = 0\n\n------------------------------------------------------------------------------\n          bp |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n         (1) |      122.7   6.467096    18.97   0.000     109.5841    135.8159\n------------------------------------------------------------------------------\n\n. lincom _cons + smoke\n\n ( 1)  smoke + _cons = 0\n\n------------------------------------------------------------------------------\n          bp |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n         (1) |      135.2   4.213207    32.09   0.000     126.6552    143.7448\n------------------------------------------------------------------------------\n\n. lincom _cons + male + smoke + malesmoke\n\n ( 1)  male + smoke + malesmoke + _cons = 0\n\n------------------------------------------------------------------------------\n          bp |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n         (1) |      174.7    4.98676    35.03   0.000     164.5864    184.8136\n------------------------------------------------------------------------------\n\n. \n. * Now, look at the important differences\n. \n. lincom male\n\n ( 1)  male = 0\n\n------------------------------------------------------------------------------\n          bp |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n         (1) |       15.3    8.97806     1.70   0.097    -2.908349    33.50835\n------------------------------------------------------------------------------\n\n. lincom male + malesmoke\n\n ( 1)  male + malesmoke = 0\n\n------------------------------------------------------------------------------\n          bp |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n         (1) |       39.5   6.528314     6.05   0.000     26.25996    52.74004\n------------------------------------------------------------------------------\n\n\n. lincom smoke\n\n ( 1)  smoke = 0\n\n------------------------------------------------------------------------------\n          bp |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n         (1) |       27.8   7.518865     3.70   0.001     12.55103    43.04897\n------------------------------------------------------------------------------\n\n\n. lincom smoke + malesmoke\n\n ( 1)  smoke + malesmoke = 0\n\n------------------------------------------------------------------------------\n          bp |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n         (1) |         52   8.166463     6.37   0.000     35.43765    68.56235\n------------------------------------------------------------------------------\n\n. lincom malesmoke\n\n ( 1)  malesmoke = 0\n\n------------------------------------------------------------------------------\n          bp |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n         (1) |       24.2   11.10065     2.18   0.036     1.686837    46.71316\n------------------------------------------------------------------------------\n\n. \n. . tabulate male smoke, summarize(logbp) nostandard nofreq noobs\n\n                              Means of logbp\n\n           |        smoke\n      male |         0          1 |     Total\n-----------+----------------------+----------\n         0 | 4.6592534  4.9027077 | 4.7809805\n         1 | 4.7963605  5.1595116 | 4.9779361\n-----------+----------------------+----------\n     Total | 4.7278069  5.0311096 | 4.8794583\n\n. \n. di exp(4.6592534)\n105.55724\n\n. di exp(4.7963605)\n121.06898\n\n. di exp(4.9027077)\n134.65389\n\n. di exp(5.1595116)\n174.07941\n\n. \n. di exp(4.7963605) - exp(4.6592534)\n15.51174\n\n. di exp(4.7963605) / exp(4.6592534)\n1.146951\n\n. \n. di exp(5.1595116) - exp(4.9027077)\n39.425526\n\n. di exp(5.1595116) / exp(4.9027077)\n1.2927916\n\n\n. \n. ***********************************************************\n. * Linear Regression: Inference about geometric means *\n. ***********************************************************\n. \n. \n. * Get the log geometric means presented in the Table \n. *   Method 1: The adjust command                                \n. *   Method 2: Four linear combos                                   \n. *   After either method, take exp(x) to get geometric mean\n. \n. regress logbp male smoke malesmoke, robust\n\nLinear regression                                      Number of obs =      40\n                                                       F(  3,    36) =   28.00\n                                                       Prob > F      =  0.0000\n                                                       R-squared     =  0.6271\n                                                       Root MSE      =  .14898\n\n------------------------------------------------------------------------------\n             |               Robust\n       logbp |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n        male |   .1371071   .0850408     1.61   0.116    -.0353636    .3095778\n       smoke |   .2434543   .0707116     3.44   0.001     .1000444    .3868641\n   malesmoke |   .1196969   .0942253     1.27   0.212    -.0714009    .3107946\n       _cons |   4.659253   .0642883    72.47   0.000     4.528871    4.789636\n------------------------------------------------------------------------------\n\n. adjust, by(male smoke)\n\n---------------------------------------------------------------------------------------\n     Dependent variable: logbp     Command: regress\n    Variable left as is: malesmoke\n---------------------------------------------------------------------------------------\n\n----------------------------\n          |      smoke      \n     male |       0        1\n----------+-----------------\n        0 | 4.65925  4.90271\n        1 | 4.79636  5.15951\n----------------------------\n     Key:  Linear Prediction\n\n. lincom _cons\n\n ( 1)  _cons = 0\n\n------------------------------------------------------------------------------\n       logbp |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n         (1) |   4.659253   .0642883    72.47   0.000     4.528871    4.789636\n------------------------------------------------------------------------------\n\n. lincom _cons + male\n\n ( 1)  male + _cons = 0\n\n------------------------------------------------------------------------------\n       logbp |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n         (1) |    4.79636   .0556682    86.16   0.000      4.68346    4.909261\n------------------------------------------------------------------------------\n\n. lincom _cons + smoke\n\n ( 1)  smoke + _cons = 0\n\n------------------------------------------------------------------------------\n       logbp |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n         (1) |   4.902708   .0294475   166.49   0.000     4.842985     4.96243\n------------------------------------------------------------------------------\n\n. lincom _cons + male + smoke + malesmoke\n\n ( 1)  male + smoke + malesmoke + _cons = 0\n\n------------------------------------------------------------------------------\n       logbp |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n         (1) |   5.159512   .0279162   184.82   0.000     5.102895    5.216128\n------------------------------------------------------------------------------\n\n. \n. \n. * Now, look at the important ratios\n. \n. lincom male, eform\n\n ( 1)  male = 0\n\n------------------------------------------------------------------------------\n       logbp |     exp(b)   Std. Err.      t    P>|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n         (1) |   1.146951   .0975376     1.61   0.116     .9652544     1.36285\n------------------------------------------------------------------------------\n\n. lincom male + malesmoke, eform\n\n ( 1)  male + malesmoke = 0\n\n------------------------------------------------------------------------------\n       logbp |     exp(b)   Std. Err.      t    P>|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n         (1) |   1.292792   .0524572     6.33   0.000     1.190663     1.40368\n------------------------------------------------------------------------------\n\n. lincom smoke, eform\n\n ( 1)  smoke = 0\n\n------------------------------------------------------------------------------\n       logbp |     exp(b)   Std. Err.      t    P>|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n         (1) |   1.275648   .0902032     3.44   0.001      1.10522    1.472356\n------------------------------------------------------------------------------\n\n. lincom malesmoke, eform\n\n ( 1)  malesmoke = 0\n\n------------------------------------------------------------------------------\n       logbp |     exp(b)   Std. Err.      t    P>|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n         (1) |   1.127155   .1062065     1.27   0.212     .9310886    1.364509\n------------------------------------------------------------------------------\n```\n\n### Ignoring effect modification\n\n-   By design or mistake, we sometimes do not model effect modification\n\n-   Might perform\n\n    -   Unadjusted analysis: POI only\n\n    -   Adjusted analysis: POI and third variable, but no interaction term\n\n-   If effect modification exists, an unadjusted analysis will give different results according to the association between the POI and effect modifier in the sample\n\n    -   If the POI and the effect modifier are not associated\n\n        -   Unadjusted analysis tends toward an (approximate) weighted average of the stratum specific effects\n\n        -   With means, exactly a weighted average\n\n        -   With odds and hazards, an approximate weighted average (because they are non-linear functions of the mean)\n\n    -   If the POI and the effect modifier are associated in the sample\n\n        -   The \"average\" effect is confounded and thus unreliable\n\n        -   (variables can be both effect modifiers and confounders)\n\n-   If effect modification exists, an analysis adjusting only for the third variable (but no interaction) will tend toward a weight average of the stratum specific effects\n\n    -   Hence, an association in one stratum and not the other will make an adjusted analysis look like an association (provide the sample size is large enough)\n\n## General Model for Effect Modification\n\n-   Typical model for effect modification will include\n\n    -   Main effects\n\n        -   $X$, or predictors involving only $X$\n\n        -   $W$, or predictors involving only $W$\n\n    -   Interactions\n\n        -   Predictors derived from both $X$ and $W$\n\n    $$\\begin{aligned}\n     g(\\theta | X_i , W_i) & = & \\beta_0 + \\beta_1 \\times X_i + \\beta_2 \\times W_i + \\beta_3 \\times (XW)_i \\\\\n     g(\\theta | X_i , W_i) & = & \\beta_0 + \\beta_1 \\times X_i + \\beta_2 \\times W_i + \\beta_3 \\times X_i \\times W_i\\end{aligned}$$\n\n-   Interpretation of parameters more difficult\n\n    -   Can try the usual approach of making comparisons of $\\theta$ \"across groups differing by 1 unit in corresponding predictor but agreeing in other modeled predictors.\"\n\n    -   However, terms involving two scientific variables makes this approach difficult\n\n    -   Intercept\n\n        -   $\\beta_0$: corresponds to $X=0$, $W=0$\n\n        -   May lack scientific meaning\n\n    -   Slopes for main effects\n\n        -   $\\beta_1$: corresponds to 1 unit difference in $X$, holding $W$ and $(X \\times W)$ constant\n\n            -   So, a 1 unit difference in $X$ when $W=0$\n\n            -   May lack scientific meaning\n\n        -   $\\beta_2$: corresponds to 1 unit difference in $W$, holding $X$ and $(X \\times W)$ constant\n\n            -   So, a 1 unit difference in $W$ when $X=0$\n\n            -   May lack scientific meaning\n\n    -   Slope for interaction (difficult)\n\n        -   $\\beta_3$: corresponds to 1 unit difference in $(X \\times W)$, holding $X$ and $W$ constant\n\n            -   Impossible, so we need another way to interpret this slope parameter for the interaction\n\n### Interpretation of slope parameter for the interaction\n\n-   Consider fixing stratum $W_i = w$. Then, $$\\begin{aligned}\n      g(\\theta | X_i , W_i = w) & = & \\beta_0 + \\beta_1 \\times X_i + \\beta_2 \\times w + \\beta_3 \\times X_i \\times w \\\\\n     & = & (\\beta_0 + \\beta_2 \\times w) + (\\beta_1 + \\beta_3 \\times w) \\times X_i\\end{aligned}$$\n\n    -   Intercept: $(\\beta_0 + \\beta_2 \\times w)$ corresponds to $X_i = 0$\n\n    -   Slope: $(\\beta_1 + \\beta_3 \\times w)$ compares groups differing by 1 unit in $X$\n\n    -   $\\beta_3$: Difference in $X$ slope per 1 unit difference in $W$\n\n    -   Example: $Y$ is height, $X$ is age, $W$ is gender\n\n        -   $\\beta_3$: Difference in growth curves, males compared to females\n\n-   Consider fixing stratum $X_i = x$. Then, $$\\begin{aligned}\n      g(\\theta | X_i = x , W_i) & = & \\beta_0 + \\beta_1 \\times x + \\beta_2 \\times W_i + \\beta_3 \\times x \\times W_i \\\\\n     & = & (\\beta_0 + \\beta_1 \\times x) + (\\beta_2 + \\beta_3 \\times x) \\times W_i\\end{aligned}$$\n\n    -   Intercept: $(\\beta_0 + \\beta_1 \\times x)$ corresponds to $W_i = 0$\n\n    -   Slope: $(\\beta_2 + \\beta_3 \\times x)$ compares groups differing by 1 unit in $W$\n\n    -   $\\beta_3$: Difference in $W$ slope per 1 unit difference in $X$\n\n-   Note the implied symmetry\n\n    -   If $W$ modifies the association between $X$ and $Y$, then $X$ modifies the association between $W$ and $Y$\n\n    -   Statistically, there is no distinction between which variable you call your \"effect modifier\" and your \"POI\"\n\n        -   There often is a scientific distinction that should be considered\n\n-   Aside: Does confounding have to be symmetric?\n\n    -   If $W$ confounds the association between $Y$ and $X$, must it also be true that $X$ confounds the association between $W$ and $Y$?\n\n### Inference for Effect Modification\n\n$g(\\theta | X_i , W_i) = \\beta_0 + \\beta_1 \\times X_i + \\beta_2 \\times W_i + \\beta_3 \\times X_i \\times W_i$\n\n-   Inference for effect modification\n\n    -   No effect modification if $\\beta_3 = 0$\n\n    -   Hence, to test for existence of effect modification we consider $H_0: \\beta_3 = 0$\n\n        -   We can perform such inference using standard regression output for the corresponding slope parameter\n\n-   Inference for main effect slope\n\n    -   Interpretation of $\\beta_1 = 0$\n\n        -   Same intercept in strata defined by $W$\n\n        -   Generally a very uninteresting question\n\n        -   We rarely make inference on main effects slopes by themselves\n\n-   Inference about effect of $X$\n\n    -   Response parameter not associated with $X$ if $\\beta_1 = 0$ AND $\\beta_3 = 0$\n\n    -   We will need to construct special tests that both parameters are simultaneously $0$\n\n        -   $H_0: \\beta_1 = 0, \\beta_3 = 0$\n\n        -   Note that the Wald tests given in regression output only consider one slope at a time\n\n-   Testing multiple slopes in Stata\n\n    -   Stata has an easy method for performing a test that multiple parameters are simultaneously $0$\n\n        -   First, perform any regression command\n\n        -   Then, use `test var1 var2 ...`\n\n        -   Provides P value of the hypothesis test based on most recently executed regression command\n\n        -   Will work on any type of regression\n\n### Salary by sex and administration\n\n-   Question: Does sex modify the association between mean salary and administrative duties?\n\n-   With two binary variables (sex, admin), modeling the interaction using a product is the obvious choice\n\n-   $E[Salary | Male, Adm] = \\beta_0 + \\beta_1 \\times Adm_i + \\beta_2 \\times Male_i + \\beta_3 \\times Adm_i \\times Male_i$\n\n#### Stata Regression Output\n\n```         \n. gen maleadmin = male*admin\n \n. regress salary admin male maleadmin, robust\n\nLinear regression                                      Number of obs =    1597\n                                                       F(  3,  1593) =  125.26\n                                                       Prob > F      =  0.0000\n                                                       R-squared     =  0.1615\n                                                       Root MSE      =  1866.9\n\n------------------------------------------------------------------------------\n             |               Robust\n      salary |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n       admin |   1489.471    292.628     5.09   0.000     915.4946    2063.448\n        male |   1226.234   95.37051    12.86   0.000      1039.17    1413.299\n   maleadmin |   461.9072   341.6782     1.35   0.177    -208.2789    1132.093\n       _cons |   5280.373   72.61871    72.71   0.000     5137.934    5422.811\n------------------------------------------------------------------------------\n```\n\n#### Descriptive statistics\n\n-   Note that with two binary variables, the regression parameters agree exactly with the corresponding group means\n\n```         \n------------------------------\n          |        male       \n    admin |        0         1\n----------+-------------------\n        0 | 5280.373  6506.607\n        1 | 6769.844  8457.985\n------------------------------\n```\n\n#### Inference about effect modification\n\n-   Does sex modify the association between mean salary and administrative duties?\n\n-   Model estimates that the administrative supplement is \\$462 per month more for men than women\n\n    -   95% confident that the true difference is between \\$1132 more and \\$208 less\n\n    -   Not statistically significant ($p = 0.177$)\n\n```         \n------------------------------------------------------------------------------\n             |               Robust\n      salary |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n       admin |   1489.471    292.628     5.09   0.000     915.4946    2063.448\n        male |   1226.234   95.37051    12.86   0.000      1039.17    1413.299\n   maleadmin |   461.9072   341.6782     1.35   0.177    -208.2789    1132.093\n       _cons |   5280.373   72.61871    72.71   0.000     5137.934    5422.811\n------------------------------------------------------------------------------\n```\n\n#### Inference about sex association\n\n-   Is sex associated with mean salary?\n\n-   Need to test that the slope parameter for male and maleadmin are simultaneously $0$\n\n    -   $H_0: \\beta_2 = 0, \\beta_3 = 0$\n\n    -   $H_1:$ At least one not equal\n\n```         \n. test male maleadmin\n\n ( 1)  male = 0\n ( 2)  maleadmin = 0\n\n       F(  2,  1593) =   95.90\n            Prob > F =    0.0000\n```\n\n#### Inference about admin association\n\n-   Are administrative duties associated with mean salary?\n\n-   Need to test that the slope parameter for admin and maleadmin are simultaneously $0$\n\n    -   $H_0: \\beta_1 = 0, \\beta_3 = 0$\n\n    -   $H_1:$ At least one not equal\n\n```         \n. test admin maleadmin\n\n ( 1)  admin = 0\n ( 2)  maleadmin = 0\n\n       F(  2,  1593) =   74.15\n            Prob > F =    0.0000\n```\n\n#### Inference about admin association using R\n\n- See the effect modification lab.  We will go over this in detail in class.\n\n\n## Continuous Effect Modification\n\n-   Modeling interactions with continuous predictors is conceptually more complicated\n\n-   Is a multiplicative model at all a reasonable model for the data?\n\n-   Nonetheless, this is the most common way we detect interactions\n\n    -   Be cautious against using the model for prediction of means or individual observations\n\n### Example: Normal ranges for SEP\n\n-   We want to find normal ranges for somatosensory evoked potential (SEP)\n\n    -   p60: Average time (in milliseconds) to detection of the second positive SEP following stimulation of the posterior right and left tibial nerve\n\n-   As a first step, we want to consider important predictors of nerve conduction times\n\n    -   If any variables such as sex, age, height, race, etc. are important predictors of nerve conduction times, then it would make most sense to obtain normal ranges within such groups\n\n    -   Scientifically, we might expect that height, age, and sex are related to the nerve conduction time\n\n        -   Nerve length should matter, and height is a surrogate for nerve length\n\n        -   Age might affect nerve conduction times (people slow down with age)\n\n        -   Sex: Males have worse nervous systems\n\n-   Prior to looking at the data, we can also consider the possibility that interactions between these variables might be important\n\n    -   Height and age interaction? Do we expect\\...\n\n        -   Difference in conduction times comparing 6 foot tall and 5 foot tall 20 year old, to be the same as \\...\n\n        -   Difference in conduction times comparing 6 foot tall and 5 foot tall 50 year old, to be the same as \\...\n\n        -   Difference in conduction times comparing 6 foot tall and 5 foot tall 80 year old?\n\n    -   We might suspect such an interaction due to the fact that height may not be as good a surrogate for nerve length in older people\n\n        -   With age, some people tend to shrink due to osteoporosis and compression of intervertebral discs. It is not clear that nerve length would be altered in such a process.\n\n    -   Thus, in young people, differences in height probably are a better measure of nerve length than in old people\n\n        -   Tall old people probably have been tall always\n\n        -   Short old people will include some who were taller when they were young\n\n-   We can also consider the possibility of three way interactions between height, age, and sex\n\n    -   Osteoporosis affects women far more than men\n\n    -   We might expect the height-age interaction to be greatest in women and not so important in men\n\n    -   A two-way interaction between height and age that is different between men and women defines a three way interaction between height, age, and sex\n\n### SEP regression model\n\n-   To define a regression model with interactions, we must create variables to model the three way interaction term\n\n-   Furthermore, it is a **very good** idea to include all main effects and lower order interactions in the model too\n\n    -   Main effects: The individual variables which contribute to the interaction\n\n    -   Lower order terms: All interactions that involve some combination of the variables which contribute to the interaction\n\n-   Most often, we lack sufficient information to be able to guess what the true form of an interaction might be\n\n    -   The most popular approach is thus to consider multiplicative interactions\n\n    -   Create a new variable by merely multiplying the two (or more) interacting predictors\n\n-   For the problem, we will create the variables\n\n    -   HA = Height \\* Age\n\n    -   HM = Height \\* Male\n\n    -   AM = Age \\* Male\n\n    -   HAM = Height \\* Age \\* Male\n\n-   Interpretation: In the presence of higher order terms (powers, interactions) interpretation of parameters is not easy\n\n    -   We can no longer use \"the change associated with a 1-unit difference in predictor holding other variables constant\"\n\n    -   It is generally impossible to hold other variables constant when changing a covariate involved in an interaction\n\n    -   When it is not impossible, it is often uninteresting scientifically\n\n$E[p60 | Ht, Age, Male] = \\beta_0 + \\beta_H Ht + \\beta_A Age + \\beta_M Male + \\beta_{HA} HA + \\beta_{HM} HM + \\beta_{AM} AM + \\beta_{HAM} HAM$\n\n-   p60 - Height relationship for Age = a:\n\n    | Sex | Intercept                                        | Slope                                                   |\n    |:------------------|:-------------------------|:---------------------------|\n    | F   | $(\\beta_0 + \\beta_A a)$                          | $(\\beta_H + \\beta_{HA} a)$                              |\n    | M   | $(\\beta_0 + \\beta_M + (\\beta_A + \\beta_{AM}) a)$ | $(\\beta_H + \\beta_{HM} + (\\beta_{HA} + \\beta_{HAM}) a)$ |\n\n-   From the above, we see the importance of including the main effects and lower order terms\n\n    -   E.g., leaving out the height-sex interaction is tantamount that claiming the p60 - height relationship among newborns is the same for the two sexes\n\n        -   It might be true, but the chance that our lines would predict the truth is very slight-- we are trying to approximate relationships in other age ranges\n\n### Regression Output\n\n```         \n. regress p60 height age male ha hm am ham\n\n------------------------------------------------------------------------------\n         p60 |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n      height |   1.380275    .362647     3.81   0.000     .6659271    2.094622\n         age |   1.129423   .4249166     2.66   0.008     .2924161     1.96643\n        male |   74.95773   32.30708     2.32   0.021     11.31875    138.5967\n          ha |  -.0149985   .0066386    -2.26   0.025    -.0280754   -.0019217\n          hm |  -1.127006   .4825427    -2.34   0.020    -2.077526   -.1764858\n          am |  -1.162866   .5817348    -2.00   0.047    -2.308776   -.0169558\n         ham |   .0175005   .0087708     2.00   0.047     .0002236    .0347773\n       _cons |  -36.44286   23.48684    -1.55   0.122    -82.70758     9.82187\n------------------------------------------------------------------------------\n```\n\n-   If we restrict analysis to just females\n\n    -   Point estimates are the same as in the saturated model\n\n    -   Inference (CIs, p-values) can differ due to the estimate of the residual standard error\n\n    -   Note that restricting by age or height would give different estimates because we are still borrowing information across groups\n\n    <!-- -->\n\n    ```         \n    . regress p60 height age ha if male==0\n\n    ------------------------------------------------------------------------------\n             p60 |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]\n    -------------+----------------------------------------------------------------\n          height |   1.380275   .3614558     3.82   0.000     .6653291     2.09522\n             age |   1.129423   .4235208     2.67   0.009     .2917155    1.967131\n              ha |  -.0149985   .0066168    -2.27   0.025    -.0280863   -.0019108\n           _cons |  -36.44286   23.40969    -1.56   0.122    -82.74631    9.860598\n    ------------------------------------------------------------------------------\n    ```\n\n-   Interpreting all of the estimates can be difficult\n\n    -   Can graph predicted values, which should be multiple linear for this model\n\n    -   Note that age is a continuous variable, so to understand how age modified the association between p60 and height, we will fix age at some values\n\n        -   Using general notation, let $Age = a$\n\n-   Estimated association between p60 and height for Females\n\n    | Age | Intercept                                | Slope                                       |\n    |:------------------|:--------------------------|:--------------------------|\n    | a   | $\\hat{\\beta}_0 + \\hat{\\beta}_A \\times a$ | $\\hat{\\beta}_H + \\hat{\\beta}_{HA} \\times a$ |\n    | a   | $-36.44 + 1.129 \\times a$                | $1.38 - 0.01499 \\times a$                   |\n\n-   Estimated association between p60 and height in Males\n\n    | Age | Intercept                                                              | Slope                                                                         |\n    |:------------------|:-------------------------|:---------------------------|\n    | a   | $\\hat{\\beta}_0 + \\hat{\\beta}_M + (\\hat{\\beta}_A + \\hat{\\beta}_{AM}) a$ | $\\hat{\\beta}_H + \\hat{\\beta}_{HM} + (\\hat{\\beta}_{HA} + \\hat{\\beta}_{HAM}) a$ |\n    | a   | $-36.44 + 74.96 + (1.13 - 1.16) a$                                     | $1.38 - 1.13 + (-0.015 + 0.0175) a$                                           |\n    | a   | $38.51 - 0.033 \\times a$                                               | $0.25 + 0.0025 \\times a$                                                      |\n\n-   Which corresponds to a regression including just males\n\n    ```         \n    . regress p60 height age ha if male==1\n\n    ------------------------------------------------------------------------------\n             p60 |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]\n    -------------+----------------------------------------------------------------\n          height |   .2532689   .3196022     0.79   0.430    -.3801724    .8867101\n             age |  -.0334425   .3989042    -0.08   0.933    -.8240577    .7571727\n              ha |   .0025019   .0057549     0.43   0.665    -.0089041    .0139079\n           _cons |   38.51487   22.27228     1.73   0.087    -5.628068    82.65781\n    ------------------------------------------------------------------------------\n    ```\n\n#### R Code to generate plot\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rms)\nd2 <- stata.get(\"http://biostat.app.vumc.org/wiki/pub/Main/CourseBios312/sep.dta\")\nd2$p60 <- (d2$p60R + d2$p60L) /2\nm.female <- lm(p60 ~ height + age + height:age, data=d2, subset=sex==0)\nm.male <- lm(p60 ~ height + age + height:age, data=d2, subset=sex==1)\nd2$agecat <- (d2$age < 35) + (d2$age < 60) + 1\n\npar(mfrow=c(2,1))\nwith(d2, plot(height[sex==0], p60[sex==0], pch=agecat[sex==0], col=agecat[sex==0], xlim=c(53,80),\n   ylim=c(50,80), ylab=\"Avg P60\", xlab=\"Height\", main=\"Females\"))\nlines(53:83, predict(m.female, newdata=data.frame(age=30, height=53:83)), col=3)\nlines(53:83, predict(m.female, newdata=data.frame(age=50, height=53:83)), col=2)\nlines(53:83, predict(m.female, newdata=data.frame(age=70, height=53:83)), col=1)\nlegend(\"topleft\", c(\"30\",\"50\",\"70\"), lty=1, col=c(3,2,1), bty=\"n\", title=\"Age\")\n\nwith(d2, plot(height[sex==1], p60[sex==1], pch=agecat[sex==1], col=agecat[sex==1], xlim=c(53,80),\n   ylim=c(50,80), ylab=\"Avg P60\", xlab=\"Height\", main=\"Males\"))\nlines(53:83, predict(m.male, newdata=data.frame(age=30, height=53:83)), col=3)\nlines(53:83, predict(m.male, newdata=data.frame(age=50, height=53:83)), col=2)\nlines(53:83, predict(m.male, newdata=data.frame(age=70, height=53:83)), col=1)\nlegend(\"topleft\", c(\"<35\",\"35-60\",\"60+\"), pch=c(3,2,1), col=c(3,2,1), bty=\"n\",title=\"Age\")\n```\n\n::: {.cell-output-display}\n![](Lec10.Modeling.Effect.Modification_files/figure-html/unnamed-chunk-36-1.png){width=672}\n:::\n:::\n\n\n\n### Influence diagnostics\n\n-   From the output, we find a statistically significant three way interaction (p = 0.047)\n\n-   This would argue for making prediction based on a model that include the 3-way interaction\n\n-   However, interactions might be significant only because of a single outlier\n\n    -   If that were the case, I might choose not to include the interaction\n\n    -   But, I would include the influential data point\n\n    -   We will look at the results of a \"diagnosis of influential observations now, and cover in more detail later\n\n-   In particular, I am interested in ensuring that the evidence for an interaction is not based solely on a single person's observation\n\n    -   Hence, I consider 250 different regression in which I leave out each subject in turn\n\n    -   I plot the slope estimates and p-values for each variable as a function of which case I left out\n\n    -   For comparison, case 0 corresponds to using the full dataset\n\n\n-   Changes in coefficients ($\\beta$s)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate changes in p-values and coefficients\n# Create some variables to store the results\nid <- 1:length(d2$p60)\ncase <- 0:length(d2$p60)\n# p is number of predictors\np <- 8\ncoeffs <- matrix(NA, nrow=length(case), ncol=p)\npvals <- matrix(NA, nrow=length(case), ncol=p)\n\n# Run the regression model with 1 observation deleted, save the coeffs and p-vals\n#  Note that case=0 corresponds to the full model (no observations deleted)\nfor (i in case) {\n  m <- lm(p60 ~ height*age*sex, data=d2, subset=(id!=i))\n  coeffs[i+1,] <- coef(m)\n  pvals[i+1,] <- summary(m)$coeff[,4]\n}\n\n# Plot the regression coefficients by which subject was deleted; highlight the unusual subject\npar(mfrow=c(2,4))\nfor(i in 1:8) {\nplot(case, coeffs[,i], ylab=(names(coef(m))[i]))\npoints(x=140, y=coeffs[141,i], pch=19, col=\"Red\")\n}\n```\n\n::: {.cell-output-display}\n![](Lec10.Modeling.Effect.Modification_files/figure-html/unnamed-chunk-38-1.png){width=672}\n:::\n:::\n\n\n\n-   Changes in p-values for $\\beta$s\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot the p-values for the regression coefficients by which subject was deleted\npar(mfrow=c(2,4))\nfor(i in 1:8) {\nplot(case, pvals[,i], ylab=(names(coef(m))[i]))\npoints(x=140, y=coeffs[141,i], pch=19, col=\"Red\")\n}\n```\n\n::: {.cell-output-display}\n![](Lec10.Modeling.Effect.Modification_files/figure-html/unnamed-chunk-40-1.png){width=672}\n:::\n:::\n\n\n\n-   Contrary to my fear, the only influential observation actually lessened the evidence of an interaction\n\n    -   When observation 140 is removed from the data, the evidence of an interaction is a larger estimate and lower p-value\n\n    -   We can examine the scatterplot to see why subject 140 might be so influential\n\n    -   Subject 140 is a 43 year old, 57 inch female with an average p60 of 66.6\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Scatter plot with subject 140 highlighted\nwith(d2, plot(height, p60, xlab=\"Height\", ylab=\"Avg p60\", main=\"Subject 140: 43 year old female\"))\nwith(d2, points(height[140], p60[140], pch=19, col=\"Red\"))\nlines(53:83, predict(m.female, newdata=data.frame(age=30, height=53:83)), col=1)\nlines(53:83, predict(m.female, newdata=data.frame(age=50, height=53:83)), col=2)\nlines(53:83, predict(m.male, newdata=data.frame(age=30, height=53:83)), col=3)\nlines(53:83, predict(m.male, newdata=data.frame(age=50, height=53:83)), col=4)\nlegend(\"topleft\", c(\"30 Female\",\"50 Female\",\"30 Male\", \"50 Male\"),\n  lty=1, col=1:4, bty=\"n\", title=\"Age, Gender\")\n```\n\n::: {.cell-output-display}\n![](Lec10.Modeling.Effect.Modification_files/figure-html/unnamed-chunk-42-1.png){width=672}\n:::\n:::\n\n\n\n-   So, what do I do with observation 140?\n\n    -   From the influence diagnostics, I am still comfortable that the data suggest a 3-way interaction\n\n    -   Personally, I do not remove observation 140 when making prediction intervals\n\n        -   I do not know why observation 140 is unusual\n\n        -   It is possible that people like 140 are actually more prevalent in the population than my sample would suggest\n\n        -   My best guess is observation 140 represents only $0.4\\%$ of the population, but would still leave her in the analysis\n\n    -   Removing subject 140 could bias parameter estimates, so I would rarely remove observations based on diagnostics\n\n",
    "supporting": [
      "Lec10.Modeling.Effect.Modification_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}