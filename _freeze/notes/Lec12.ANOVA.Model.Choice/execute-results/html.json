{
  "hash": "26fa095230703b3fcf6ae46acb2baae5",
  "result": {
    "markdown": "---\ntitle: \"ANOVA and Model Choice\"\nsubtitle: \"Lecture 12\"\nname: notes/Lec12.ANOVA.Model.Choice.qmd\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nsource(file.path('..', 'pander_registry.R'))\nlibrary(rms)\nlibrary(ggplot2)\nlibrary(splines)\nlibrary(lspline)\nlibrary(lmtest)\nlibrary(sandwich)\nlibrary(car)\nlibrary(kableExtra)\nlibrary(plotrix)\n```\n:::\n\n\n## Overview\n\n-   There are many different ways we can model predictors\n\n-   Consider what alternative models will make scientific sense\n\n-   What is the impact of letting the data drive the selection of a model\n\n-   I am going to discuss in terms of a clinical trial where we have replicates at dose levels\n\n    -   We can find dose-specific means and compare modeling approaches\n\n## ANOVA versus Linear Continuous Models\n\n-   Compare power of linear continuous models versus ANOVA as a function\n\n    -   of trend in means AND\n\n    -   standard errors withing groups\n\n-   ANOVA (dummy variables)\n\n    -   Uses indicator variables for every dose (group) level\n\n        -   Again, I am thinking about \"dose\" in a general sense that could include covariates like age, cholesterol, blood pressure, etc.\n\n        -   Traditionally, dose would just be dose of some treatment\n\n    -   Fits group means exactly (saturated model)\n\n        -   One way ANOVA: One categorical predictor\n\n        -   Two way ANOVA: Two categorical predictors\n\n            -   Fit with the interactions to get group means exactly\n\n    -   Saturated models do not mix random error with systematic error\n\n        -   Systematic error: Error due to differences from sample means from predicted means\n\n        -   Random error: Error that cannot be explained after controlling for dose\n\n    -   ANOVA ignores the ordering of the groups, so it gains no power from trends\n\n        -   e.g. does not assume that the difference between dose=15 and dose=30 group is similar to the difference between the dose=30 and dose=45 groups\n\n        -   In fact, the same level of significance is gained no matter what permutation of dose groups is used\n\n-   Linear continuous models\n\n    -   Borrows information across groups\n\n        -   Accurate and efficient if the model is correct\n\n    -   If model is incorrect, mixes random and systematic error\n\n        -   Will have some systematic error because the means are not predicted exactly\n\n    -   Can gain power from ordering of groups in order to detect a trend\n\n        -   But, no matter how low the standard error is, if there is no trend in the mean, there is no statistical significance\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 10\ndose.levels <- c(0,20,40,60,80)\ndose <- rep(dose.levels, n)\nset.seed(37)\n\ny1 <- 300 + 5*dose + rnorm(5*n, 0, 200)\ny2 <- 300 + 200*(dose==20) + 100*(dose==40) - 100*(dose==60) + 0*(dose==80) + rnorm(5*n, 0, 200)\n\nm1 <- lm(y1 ~ dose)\nm2 <- lm(y1 ~ factor(dose))\n\n#m3 <- lm(y2 ~ dose)\nm4 <- lm(y2 ~ factor(dose))\n\np1 <- predict(m1, newdata=data.frame(dose=dose.levels), se.fit=TRUE)\np2 <- predict(m2, newdata=data.frame(dose=dose.levels), se.fit=TRUE)\n\n#p3 <- predict(m3, newdata=data.frame(dose=dose.levels), se.fit=TRUE)\np4 <- predict(m4, newdata=data.frame(dose=dose.levels), se.fit=TRUE)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow=c(2,2))\nplotCI(x=dose.levels, y=p1$fit, p1$se.fit, ylab=\"Response\", xlab=\"Dose\", ylim=c(0,1000), main=\"Linear: High power; ANOVA: High Power\")\nplotCI(x=dose.levels, y=p1$fit, 3*p1$se.fit, ylab=\"Response\", xlab=\"Dose\", ylim=c(0,1000), main=\"Linear: Mod power; ANOVA: Low Power\")\n\nplotCI(x=dose.levels[c(1,5,4,2,3)], y=p1$fit, p1$se.fit, ylab=\"Response\", xlab=\"Dose\", ylim=c(0,1000), main=\"Linear: No power; ANOVA: High Power\")\nplotCI(x=dose.levels[c(1,5,4,2,3)], y=p1$fit, 3*p1$se.fit, ylab=\"Response\", xlab=\"Dose\", ylim=c(0,1000), main=\"Linear: No power; ANOVA: Low Power\")\n```\n\n::: {.cell-output-display}\n![Comparsion of power using ANOVA versus linear dose for detecting a dose effect for various dose-response relationships](Lec12.ANOVA.Model.Choice_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n-   Other options for modeling continuous predictors\n\n    -   Combinations of linear trends and indicator variables\n\n    -   Splines\n\n    -   Polynomials\n\n    -   etc.\n\n## Choice of Transformation\n\n-   The exact form used to model predictors should be based on scientific (first) and statistical (second) criteria\n\n-   Scientific issues\n\n    -   The form used to model predictors must address the specific scientific question\n\n        -   Should be the next logical step in the process of investigating the overall goal\n\n        -   First, establish some sort of an association\n\n        -   Second, detect a first order trend\n\n        -   Third, detecting specific forms of non-linearities\n\n            -   Threshold effects?\n\n            -   U- or S-shaped trends?\n\n        -   Finally, more complex models\n\n    -   When the scientific question relates to prediction, it is imperative that the regression model accurately reflects the true relationship between predictors and the summary measure of response\n\n        -   Failure to have the correct model will guarantee that some groups may not have the correct predicted response\n\n    -   When the scientific question relates to detection of associations, the importance of having the true model depends on the statistical role of the predictor\n\n        -   With the predictor of interest, the most important issues is to protect the validity of the statistical inference\n\n            -   Data driven decision will inflate the type I error rate\n\n        -   With precision variables, it is not as crucial that the true relationship be modeled\n\n            -   An approximate model will provide most of the precision gains\n\n        -   With confounders, failure to accurately model the relationship between the confounder and the response may lead to residual confounding\n\n            -   Sometimes we will use very flexible models for continuous confounders as there is little cost to doing so and the potential for imporant gain\n\n    -   As the goal of any analysis is to communicate findings to the greater scientific community, it is also important that modeling of predictors is easy to understand\n\n        -   This is an issue that matters most for your predictor of interest\n\n        -   We are generally not worried about making inference about precision variables or confounders\n\n-   Statistical issues\n\n    -   The greatest statistical precision will be gained when the model reflects the true relationship between the predictor and the response\n\n        -   Accurate modeling of the relationship will avoid introducing systematic error in the estimates of the standard errors\n\n        -   Parsimony: Using the fewest parameters to model the relationship will allow greater precision\n\n        -   Precision is a trade-off between parsimony and increased accuracy from including more parameters\n\n    -   We should select the form of modeling the predictor before looking at the data\n\n        -   Data drive selection of transformations will tend to lead to inaccurate (anti-conservative) statistical inference\n\n        -   Overfitting of the data leads to spuriously low estimates of the within group variability\n\n            -   Thus standard errors estimates are too low\n\n            -   Type-I errors are inflated\n\n            -   Confidence interval are too narrow (inaccurate coverage probabilities)\n\n        -   Data-driven model selection will also lead to coefficient estimates that are biased away from the null (leading you to overstate your scientific effects)\n\n## Example: Beta Carotene Supplements\n\n### Overview\n\n-   Before doing large scale clinical trials, it is important to understand the pharmacokinetics of a drug\n\n-   Phase II prevention trials often administer a drug in various doses to volunteers, and pertinent plasma levels are then measured at regular intervals\n\n-   Of particular interest is how dose level affects the build up of drug in the plasma over time, as well as how the dose level might affect other blood chemistry\n\n-   Forty-six (46) volunteers were randomly assigned to receive one of five doses of beta-carotene (0, 15, 30, 45, or 60 mg/day) for 9 months in a double blind fashion\n\n-   The specific aim was to determine how different dose levels affected the serum beta-carotene levels after 9 months\n\n-   Other measured variables available in this data set include subject age, sex, weight, body mass index, percent body fat, and serum cholesterol level at baseline\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncarot <- stata.get(\"data/carot.dta\")\nres1 <- aggregate(carot3 ~ dose, data = carot,\nFUN = function(x) c(n = length(x), mean = mean(x), sd = sd(x),\nmin = min(x), q25 = quantile(x, 0.25),\nmedian = median(x), q75 = quantile(x, 0.75),\nmax = max(x)))\n\nprint(res1, digits=2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  dose carot3.n carot3.mean carot3.sd carot3.min carot3.q25.25% carot3.median\n1    0        7         186        88         84            133           149\n2   15        8        1254       570        577            723          1250\n3   30        9        1505       479        849           1157          1498\n4   45        7        1749       579        950           1333          1848\n5   60        9        1878       430       1233           1725          1865\n  carot3.q75.75% carot3.max\n1            241        323\n2           1682       2019\n3           1840       2248\n4           2234       2310\n5           1918       2855\n```\n:::\n:::\n\n\n-   In this randomized trial, we can consider several potential response variables\n\n    -   Plasma level at the end of treatment\n\n    -   Change in plasma level over the treatment period\n\n    -   Either of the above adjusted for baseline plasma (ANCOVA model)\n\n-   Accounting for baseline\n\n    -   Dose group $i$, subject $j$, time $t$\n\n    -   $Y_{ijt} \\sim (\\mu_{it}, \\sigma^2)$; $\\textrm{corr}(Y_{ij0}, Y_{ij9}) = \\rho$ $$\\begin{aligned}\n         \\overline{Y}_{i\\cdot9} & \\sim & \\left(\\mu_{i9}, \\sigma^2/n \\right) \\\\\n         \\overline{Y}_{i\\cdot9} - \\overline{Y}_{i\\cdot0} & \\sim & \\left(\\mu_{i9} - \\mu_{i0}, 2\\sigma^2(1-\\rho)/n \\right) \\\\\n         \\overline{Y}_{i\\cdot9} - \\rho \\overline{Y}_{i\\cdot0} & \\sim & \\left(\\mu_{i9} - \\rho \\mu_{i0}, \\sigma^2(1-\\rho^2)/n \\right)\\end{aligned}$$\n\n    -   Compared variances of the above three equations\n\n        -   When are the variances equal, smaller, larger\n\n        -   Which is always smallest\n\n-   By randomization, there will be equal means at baseline\n\n    -   $\\mu_{T,0} = \\mu_{P,0}$ where $T$ is any of the treatment doses and $P$ is placebo\n\n-   Contrast across dose groups $$\\begin{aligned}\n     \\overline{Y}_{T,\\cdot9} - \\overline{Y}_{P,\\cdot9} & \\sim & \\left(\\mu_{T,9} - \\mu_{P,9}, 2\\sigma^2/n \\right) \\\\\n     \\left(\\overline{Y}_{T,\\cdot9} - \\overline{Y}_{T, \\cdot0}\\right) - \\left(\\overline{Y}_{P,\\cdot9} - \\overline{Y}_{P, \\cdot0}\\right) & \\sim & \\left(\\mu_{T,9} - \\mu_{P,9}, 4\\sigma^2(1-\\rho)/n \\right) \\\\\n     \\left(\\overline{Y}_{T,\\cdot9} - \\rho \\overline{Y}_{T,\\cdot0}\\right) - \\left(\\overline{Y}_{P,\\cdot9} - \\rho \\overline{Y}_{P,\\cdot0}\\right) & \\sim & \\left(\\mu_{T,9} - \\mu_{P,9}, 2\\sigma^2(1-\\rho^2)/n \\right)\\end{aligned}$$\n\n-   Simple linear regression\n\n    -   Regress $Y$ on $X$\n\n        -   $Y_i \\sim \\left(\\mu_Y, \\sigma^2_Y\\right)$; $X_i \\sim \\left(\\mu_X, \\sigma^2_X\\right)$\n\n        -   $\\textrm{corr}\\left(Y_i, X_i \\right) = \\rho$\n\n    -   Regression model: $E[Y_i | X_i] = \\beta_0 + \\beta_1 X_i$\n\n        -   $\\beta_0 = \\mu_Y - \\beta_1 \\mu_x$\n\n        -   $\\beta_1 = \\rho \\frac{\\sigma_Y}{\\sigma_X}$\n\n-   Analysis of Covariance\n\n    -   Dose group $i$, subject $j$, time $t$\n\n    -   $Y_{ijt} \\sim (\\mu_{it}, \\sigma^2)$; $\\textrm{corr}(Y_{ij0}, Y_{ij9}) = \\rho$\n\n    -   Regression model: $E[Y_{ij9} | Y_{i_j0}] = \\beta_0 + \\beta_1 Y_{ij0}$\n\n        -   $\\beta_1 = \\rho$\n\n### Methods for modeling dose response\n\n-   In a randomized clinical trial, we will tend to have the greatest precision if we adjust for baseline as a predictor in a linear regression model\n\n-   A wide variety of models may be considered for examining the relationship between dose and plasma levels\n\n    -   Dummy variables where we model each dose level independently, without borrowing information across groups (ANOVA)\n\n    -   Linear continuous predictors (transformed or untransformed)\n\n    -   Dichotomization (at any of several thresholds)\n\n    -   Polynomials, splines, other flexible methods\n\n    -   Combinations of the above\n\n    -   Even more complex models\n\n-   I will compare possible models\n\n    -   Graphically: Show data and fitted values without adjustment for baseline\n\n    -   Numerically: Show regression estimates and tests after adjustment for baseline\n\n    -   Note that this is an academic exercise and not something you would do in practice to come up with the \"best\" model\n\n-   Predicted values\n\n    -   After computing a regression command, Stata will provide predicted values for each case\n\n        -   Mathematically, this is just the intercept plus the regression parameters multiplied by the covariates for each case\n\n        -   Stata command: `predict varname`\n\n### ANOVA analysis\n\n-   Fits each group independently\n\n-   Does not use the ordering of the dose groups when looking for an effect\n\n    -   Completely ignores the magnitude and ordering of the $x$-axis\n\n-   A priori, we might expect this is not the most efficient method if the alternative hypothesis is true\n\n    -   We expect larger plasma levels with increasing dose\n\n    -   We will thus have less power to detect a first-order trend\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncarot$dose.factor <- factor(carot$dose)\nm.factor <- lm(carot3 ~ dose.factor + carot0, data=carot)\ncoeftest(m.factor, vcov=sandwich)\n```\n\n::: {.cell-output-display}\n-------------------------------------------------------------------\n      &nbsp;         Estimate    Std. Error   t value    Pr(>|t|)  \n------------------- ----------- ------------ --------- ------------\n  **(Intercept)**    -361.4516   154.46722    -2.3400   0.02529265 \n\n **dose.factor15**   1224.1896   196.89130    6.2176    0.00000045 \n\n **dose.factor30**   1439.8373   143.63572    10.0242   0.00000000 \n\n **dose.factor45**   1678.9839   154.10485    10.8951   0.00000000 \n\n **dose.factor60**   1791.0090   141.00926    12.7014   0.00000000 \n\n    **carot0**        1.9028      0.49509     3.8433    0.00050617 \n-------------------------------------------------------------------\n\nTable: t test of coefficients\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(jitter(carot$dose, factor=.2), carot$carot3, ylab=\"Plasma Beta Carotene\", xlab=\"Dose of Supplementation\", main=\"Model: Dummy Variables (ANOVA)\")\nm1 <- lm(carot3~factor(dose), data=carot)\npoints(seq(0,60, by=15), predict(m1, newdata=data.frame(dose=seq(0,60, by=15))), pch=\"O\", cex=2)\npoints(seq(0,60, by=15), predict(m1, newdata=data.frame(dose=seq(0,60, by=15))), pch=\"X\", cex=2)\nlines(seq(0,60, by=15), predict(m1, newdata=data.frame(dose=seq(0,60, by=15))))\nlegend(\"topleft\", c(\"Fitted\", \"Group Means\"), pch=c(\"X\",\"O\"), lty=c(NA,1), bty=\"n\")\n```\n\n::: {.cell-output-display}\n![](Lec12.ANOVA.Model.Choice_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n-   Testing for the dose effect\n\n    -   We must use the `testparm` command (or `test`) because the model includes the baseline measurement\n\n    -   `testparm` is similar to `test`, but allows testing multiple parameters using wildcards\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinearHypothesis(m.factor, c(\"dose.factor15\",\n                       \"dose.factor30\",\n                       \"dose.factor45\",\n                       \"dose.factor60\"),\n                 vcov=sandwich(m.factor))\n```\n\n::: {.cell-output-display}\n-------------------------------\n Res.Df   Df     F      Pr(>F) \n-------- ---- -------- --------\n   38                          \n\n   34     4    69.959     0    \n-------------------------------\n\nTable: Linear hypothesis test\n:::\n:::\n\n\n-   We would have had the same fitted values (and thus inference) if we had decided to drop a different dose group\n\n    -   Example: Making my own dummy variables for dose, with dose at 60 being the reference group\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncarot$dose.new <- relevel(carot$dose.factor, ref=\"60\")\n\nm2.factor <- lm(carot3 ~ dose.new + carot0, data=carot)\ncoeftest(m2.factor, vcov=sandwich)\n```\n\n::: {.cell-output-display}\n--------------------------------------------------------------------\n     &nbsp;         Estimate    Std. Error    t value     Pr(>|t|)  \n----------------- ------------ ------------ ----------- ------------\n **(Intercept)**   1429.5573    163.01893     8.76927    3.0000e-10 \n\n  **dose.new0**    -1791.0090   141.00926    -12.70136   0.0000e+00 \n\n **dose.new15**    -566.8194    221.47493    -2.55929    1.5108e-02 \n\n **dose.new30**    -351.1717    173.86127    -2.01984    5.1337e-02 \n\n **dose.new45**    -112.0251    188.55038    -0.59414    5.5635e-01 \n\n   **carot0**        1.9028      0.49509      3.84332    5.0617e-04 \n--------------------------------------------------------------------\n\nTable: t test of coefficients\n:::\n\n```{.r .cell-code}\nlinearHypothesis(m2.factor, c(\"dose.new0\",\n                       \"dose.new15\",\n                       \"dose.new30\",\n                       \"dose.new45\"),\n                 vcov=sandwich(m2.factor))\n```\n\n::: {.cell-output-display}\n-------------------------------\n Res.Df   Df     F      Pr(>F) \n-------- ---- -------- --------\n   38                          \n\n   34     4    69.959     0    \n-------------------------------\n\nTable: Linear hypothesis test\n:::\n:::\n\n\n-   Note that the parameter estimates all will lead to the same fitted values\n\n    -   e.g. Intercept in above model (1430) equals the intercept + dose60 coefficient (-361 + 1791) in previous model\n\n-   Overall F statistics, R-squared, Root MSE all the same\n\n-   Partial t-tests tend to differ as we are making comparisons to different reference groups\n\n-   Could also fit the same model with no intercept\n\n    -   Would then have to include all five dose groups\n\n    -   We can get Stata to include fit all five dose groups and no intercept using the `noconstant` option\n\n    -   In R, fit a model without an intercept by adding a $-1$ in the model equation (e.g. $y \\sim -1 + x$)\n\n    -   Not including the intercept changes the overall F statistic and the R-squared measures\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm3.factor <- lm(carot3 ~ -1 + dose.factor + carot0, data=carot)\ncoeftest(m3.factor, vcov=sandwich)\n```\n\n::: {.cell-output-display}\n-------------------------------------------------------------------\n      &nbsp;         Estimate    Std. Error   t value    Pr(>|t|)  \n------------------- ----------- ------------ --------- ------------\n **dose.factor0**    -361.4516   154.46722    -2.3400   2.5293e-02 \n\n **dose.factor15**   862.7379    223.06284    3.8677    4.7235e-04 \n\n **dose.factor30**   1078.3857   165.01387    6.5351    1.7530e-07 \n\n **dose.factor45**   1317.5322   205.85752    6.4002    2.6150e-07 \n\n **dose.factor60**   1429.5573   163.01893    8.7693    3.0000e-10 \n\n    **carot0**        1.9028      0.49509     3.8433    5.0617e-04 \n-------------------------------------------------------------------\n\nTable: t test of coefficients\n:::\n:::\n\n\n-   Correspondence of the no-intercept model compared to previous models\n\n    -   Some textbooks refer to this as a \"cell means\" coding system\n\n        -   If we didn't have baseline beta carotene in the model, the dose parameters would correspond directly to the means in each dose group\n\n        -   With baseline beta carotene in the model, the dose parameters are the means when carot0 is 0\n\n    -   In terms of model fit, the model is the same as before\n\n        -   No intercept means each dose group is compared to a mean of 0\n\n    -   Fitted values will be the same\n\n    -   Test of dose effect will need to test equality of all five dose covariates\n\n        -   This is *not* a test that these 5 parameters are 0\n\n        -   $H_0: dose0 = dose15 = dose30 = dose45 = dose60$\n\n        -   $H_1:$ at least one of the above is not equal\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinearHypothesis(m3.factor, c(\"dose.factor0=dose.factor15\",\n                              \"dose.factor0=dose.factor30\",\n                              \"dose.factor0=dose.factor45\",\n                              \"dose.factor0=dose.factor60\"),\n                 vcov=sandwich(m3.factor))\n```\n\n::: {.cell-output-display}\n-------------------------------\n Res.Df   Df     F      Pr(>F) \n-------- ---- -------- --------\n   38                          \n\n   34     4    69.959     0    \n-------------------------------\n\nTable: Linear hypothesis test\n:::\n:::\n\n\n### Binary dose: Placebo versus Active\n\n-   Dichotomize into dose 0 versus dose $>$ 0\n\n    -   Will be an accurate model if all (or virtually all) of the effect is attained at the lowest dose level\n\n    -   Often used when little is know about a treatment, or when dose is difficult to quantify\n\n        -   e.g. Smoking\n\n        -   We are relatively certain of a smoking effect, so our major scientific interest is likely related to the dose-response relationship above the lowest dose\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncarot$trt <- (carot$dose>0)+0\n\nm.trt <- lm(carot3 ~ trt + carot0, data=carot)\ncoeftest(m.trt, vcov=sandwich(m.trt))\n```\n\n::: {.cell-output-display}\n----------------------------------------------------------------\n     &nbsp;        Estimate    Std. Error   t value   Pr(>|t|)  \n----------------- ----------- ------------ --------- -----------\n **(Intercept)**   -406.6816   207.08860    -1.9638   0.0570999 \n\n     **trt**       1544.2205   115.44469    13.3763   0.0000000 \n\n   **carot0**       2.0599      0.68203     3.0203    0.0045596 \n----------------------------------------------------------------\n\nTable: t test of coefficients\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nm2 <- lm(carot3 ~ dose>0, data=carot)\nplot(jitter(carot$dose, factor=.2), carot$carot3, ylab=\"Plasma Beta Carotene\", xlab=\"Dose of Supplementation\", main=\"Model: Dichotomous Dose\")\nlines(seq(0,60, by=15), predict(m1, newdata=data.frame(dose=seq(0,60, by=15))), pch=\"O\", cex=2)\npoints(seq(0,60, by=15), predict(m1, newdata=data.frame(dose=seq(0,60, by=15))), pch=\"O\", cex=2)\npoints(seq(0,60, by=15), predict(m2, newdata=data.frame(dose=seq(0,60, by=15))), pch=\"X\", cex=2)\nlegend(\"topleft\", c(\"Fitted\", \"Group Means\"), pch=c(\"X\",\"O\"), lty=c(NA,1), bty=\"n\")\n```\n\n::: {.cell-output-display}\n![](Lec12.ANOVA.Model.Choice_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n### Linear, continuous dose\n\n-   Estimates the best fitting straight line to response\n\n    -   Accurate if the response is linear\n\n-   Often used when little is know about the treatment and a general trend is expected\n\n    -   In this particular application, we are relatively certain of an effect, so our major interest is in modeling the dose response relationship above 0.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm.cont <- lm(carot3 ~ dose + carot0, data=carot)\ncoeftest(m.cont, vcov=sandwich(m.cont))\n```\n\n::: {.cell-output-display}\n----------------------------------------------------------------\n     &nbsp;        Estimate   Std. Error   t value    Pr(>|t|)  \n----------------- ---------- ------------ --------- ------------\n **(Intercept)**   245.0003   214.29566    1.1433    2.6027e-01 \n\n    **dose**       25.4628     3.49367     7.2883    1.1700e-08 \n\n   **carot0**       1.3341     0.63189     2.1113    4.1565e-02 \n----------------------------------------------------------------\n\nTable: t test of coefficients\n:::\n:::\n\n\n-   To test the treatment effect, could either use the `test` command for dose or use the output directly as we are only testing one parameter\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm3 <- lm(carot3 ~ dose, data=carot)\nplot(jitter(carot$dose, factor=.2), carot$carot3, ylab=\"Plasma Beta Carotene\", xlab=\"Dose of Supplementation\", main=\"Model: Linear Dose\")\nlines(seq(0,60, by=15), predict(m1, newdata=data.frame(dose=seq(0,60, by=15))), pch=\"O\", cex=2)\npoints(seq(0,60, by=15), predict(m1, newdata=data.frame(dose=seq(0,60, by=15))), pch=\"O\", cex=2)\npoints(seq(0,60, by=15), predict(m3, newdata=data.frame(dose=seq(0,60, by=15))), pch=\"X\", cex=2)\nlegend(\"topleft\", c(\"Fitted\", \"Group Means\"), pch=c(\"X\",\"O\"), lty=c(NA,1), bty=\"n\")\n```\n\n::: {.cell-output-display}\n![](Lec12.ANOVA.Model.Choice_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n### Polynomial models of dose\n\n-   Fit terms involving dose, dose squared\n\n    -   Often used to fit U-shaped trends\n\n    -   In general, a quadratic is a pretty strong assumption in that it assumes constant curvature over dose\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm.poly <- lm(carot3 ~ poly(dose,2) + carot0, data=carot)\ncoeftest(m.poly, vcov=sandwich(m.poly))\n```\n\n::: {.cell-output-display}\n---------------------------------------------------------------------\n       &nbsp;          Estimate    Std. Error   t value    Pr(>|t|)  \n-------------------- ------------ ------------ --------- ------------\n  **(Intercept)**      935.2470    157.31460    5.9451    8.2290e-07 \n\n **poly(dose, 2)1**   3652.1148    377.96287    9.6626    0.0000e+00 \n\n **poly(dose, 2)2**   -1716.6346   351.59782    -4.8824   2.1511e-05 \n\n     **carot0**         1.7281      0.53494     3.2304    2.6414e-03 \n---------------------------------------------------------------------\n\nTable: t test of coefficients\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nm4 <- lm(carot3 ~ poly(dose, 2), data=carot)\nplot(jitter(carot$dose, factor=.2), carot$carot3, ylab=\"Plasma Beta Carotene\", xlab=\"Dose of Supplementation\", main=\"Model: Quadratic Dose\")\nlines(seq(0,60, by=15), predict(m1, newdata=data.frame(dose=seq(0,60, by=15))), pch=\"O\", cex=2)\npoints(seq(0,60, by=15), predict(m1, newdata=data.frame(dose=seq(0,60, by=15))), pch=\"O\", cex=2)\npoints(seq(0,60, by=15), predict(m4, newdata=data.frame(dose=seq(0,60, by=15))), pch=\"X\", cex=2)\nlegend(\"topleft\", c(\"Fitted\", \"Group Means\"), pch=c(\"X\",\"O\"), lty=c(NA,1), bty=\"n\")\n```\n\n::: {.cell-output-display}\n![](Lec12.ANOVA.Model.Choice_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n-   The partial t-test for dosesqr can be interpreted as a test for linear dose response\n\n    -   It is highly significant, suggestion departure from linearity\n\n-   To test the treatment effect, we need to test the two dose covariates\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinearHypothesis(m.poly, c(\"poly(dose, 2)1\",\n                           \"poly(dose, 2)2\"),\n                 vcov=sandwich(m.poly))\n```\n\n::: {.cell-output-display}\n-------------------------------\n Res.Df   Df     F      Pr(>F) \n-------- ---- -------- --------\n   38                          \n\n   36     2    93.951     0    \n-------------------------------\n\nTable: Linear hypothesis test\n:::\n:::\n\n\n### Highest order polynomial models versus ANOVA\n\n-   With 5 discrete dose levels, a 4th degree polynomial will fit the means exactly\n\n-   Thus, the model will have the same fit as the ANOVA model using dummy variables for each levels of dose\n\n    -   Higher order polynomials are borrowing less information across dose groups\n\n    -   Highest order polynomial borrows no information across dose groups\n\n<!-- -->\n\n```stata\n. gen dosecub = dose^3\n. gen dosequad = dose^4\n. regress carot3 dose dosesqr dosecub dosequad carot0, robust\n\nLinear regression                                      Number of obs =      40\n                                                       F(  5,    34) =   47.68\n                                                       Prob > F      =  0.0000\n                                                       R-squared     =  0.7184\n                                                       Root MSE      =  417.46\n\n------------------------------------------------------------------------------\n             |               Robust\n      carot3 |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n        dose |    157.876   61.94333     2.55   0.015     31.99197    283.7599\n     dosesqr |  -6.943752   5.066692    -1.37   0.180    -17.24051    3.353004\n     dosecub |   .1385695   .1313523     1.05   0.299    -.1283706    .4055096\n    dosequad |  -.0009734   .0010718    -0.91   0.370    -.0031515    .0012047\n      carot0 |   1.902792   .5370015     3.54   0.001     .8114738     2.99411\n       _cons |  -361.4516   167.5432    -2.16   0.038    -701.9404   -20.96284\n------------------------------------------------------------------------------\n\n\n. xi: regress carot3 i.dose carot0, robust\ni.dose            _Idose_0-60         (naturally coded; _Idose_0 omitted)\n\nLinear regression                                      Number of obs =      40\n                                                       F(  5,    34) =   47.68\n                                                       Prob > F      =  0.0000\n                                                       R-squared     =  0.7184\n                                                       Root MSE      =  417.46\n\n------------------------------------------------------------------------------\n             |               Robust\n      carot3 |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n   _Idose_15 |    1224.19   213.5586     5.73   0.000     790.1863    1658.193\n   _Idose_30 |   1439.837   155.7948     9.24   0.000     1123.224     1756.45\n   _Idose_45 |   1678.984   167.1502    10.04   0.000     1339.294    2018.674\n   _Idose_60 |   1791.009    152.946    11.71   0.000     1480.185    2101.833\n      carot0 |   1.902792   .5370015     3.54   0.001     .8114738     2.99411\n       _cons |  -361.4516   167.5432    -2.16   0.038    -701.9404   -20.96284\n------------------------------------------------------------------------------\n```\n\n### Threshold at 0 and Linear Term\n\n-   Threshold at 0 and linear dose\n\n-   To fit, use a dummy variable for dose0 plus dose (continuous)\n\n    -   Fits dose 0 by its group mean\n\n    -   Fits dose $>$ 0 by a line (an intercept and slope)\n\n    -   Allows us to address two scientific questions\n\n        -   Is there any effect of dose? (test both slopes)\n\n        -   Is there any additional benefit beyond the lowest dose? (test linear term's slope)\n\n<!-- -->\n\n```stata\n. regress carot3 trt dose carot0, robust\n\nLinear regression                                      Number of obs =      40\n                                                       F(  3,    36) =   81.26\n                                                       Prob > F      =  0.0000\n                                                       R-squared     =  0.7170\n                                                       Root MSE      =  406.69\n\n------------------------------------------------------------------------------\n             |               Robust\n      carot3 |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n         trt |   1050.836    223.418     4.70   0.000     597.7237    1503.949\n        dose |   12.81144   4.794314     2.67   0.011     3.088122    22.53476\n      carot0 |   1.904584   .5164903     3.69   0.001     .8570932    2.952075\n       _cons |  -361.9675   161.4254    -2.24   0.031    -689.3534   -34.58168\n------------------------------------------------------------------------------\n```\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm5 <- lm(carot3 ~ (dose >0) + dose, data=carot)\nplot(jitter(carot$dose, factor=.2), carot$carot3, ylab=\"Plasma Beta Carotene\", xlab=\"Dose of Supplementation\", main=\"Model: Threshold and Linear Dose\")\nlines(seq(0,60, by=15), predict(m1, newdata=data.frame(dose=seq(0,60, by=15))), pch=\"O\", cex=2)\npoints(seq(0,60, by=15), predict(m1, newdata=data.frame(dose=seq(0,60, by=15))), pch=\"O\", cex=2)\npoints(seq(0,60, by=15), predict(m5, newdata=data.frame(dose=seq(0,60, by=15))), pch=\"X\", cex=2)\nlegend(\"topleft\", c(\"Fitted\", \"Group Means\"), pch=c(\"X\",\"O\"), lty=c(NA,1), bty=\"n\")\n```\n\n::: {.cell-output-display}\n![](Lec12.ANOVA.Model.Choice_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n-   Testing the effect of treatment\n\n    -   Two variables model dose, so we need to test both\n\n    -   If response increases from dose 0 to lowest dose OR\n\n    -   ... response increases as dose increase, THEN\n\n    -   ... we will declare an effect of treatment\n\n-   The partial t-test for the `trt` term can be used to test for linear dose response\n\n    -   Here, it is highly significantly different from 0, indicating that just a linear model is not adequate\n\n-   The partial t-test for the `dose` term can be interpreted as a test for any added effect above the lowest dose\n\n    -   It is significantly different from 0 ($p = 0.011$)\n\n    -   There is a multiple comparison issue here, but many people are comfortable doing this 'step down' test after they have already tested from any treatment effect\n\n<!-- -->\n\n```         \n. test trt dose\n\n ( 1)  trt = 0\n ( 2)  dose = 0\n\n       F(  2,    36) =  121.88\n            Prob > F =    0.0000\n```\n\n## Data driven model selection\n\n-   Suppose we look at a scatterplot before deciding which model we fit and choose a model that can fit the data well\n\n    -   If the data looks like a straight line, choose the model linear in dose\n\n    -   If the data looks like a U, choose a quadratic\n\n    -   If the data is a complicated pattern of differences among groups, we might choose dummy variables or splines\n\n    -   etc.\n\n-   This approach would tend to mimic the behavior of fitting several different models and choosing the model with the lowest $p$-value\n\n    -   When our eye sees some trend in the data, we would be most likely to pick the model giving the lowest $p$-value\n\n### Simulation\n\n-   Using the 46 subjects in this dataset, I can randomly permute the dose they received\n\n    -   Effectively, randomize subjects to a different dose\n\n    -   But, keep their 9-month and baseline beta carotene levels the same (not permuted)\n\n        -   Should remove any association between dose and beta carotene\n\n-   Next, fit each of the five models (linear, quadratic, ANOVA, dichotomized, and dichotomized plus linear)\n\n-   Repeat the process 1000 times (representing 1000 studies)\n\n    -   Calculate how often each model rejects the null hypothesis of a dose effect\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(80)\n\nreps <- 1000\nn <- length(carot$carot3)\np.vals <- matrix(NA, nrow=reps, ncol=5)\n\ny <- carot$carot3\ny0 <- carot$carot0\nm.restrict <- lm(y ~ y0)\n\nfor(i in 1:reps) {\n perm <- sample(1:n)\n\n#Permute the dose, but not the outcome and baseline\n d <- carot$dose[perm]\n\n m.anova <- lm(y ~ y0 + factor(d))\n m.linear <- lm(y ~ y0 + d)\n m.quad <- lm(y ~ y0 + poly(d,2))\n m.dichot <- lm(y ~ y0 + (d>0))\n m.dilin <- lm(y ~ y0 + d + (d>0))\n\n p.vals[i,1:5] <- c(anova(m.anova)[2,\"Pr(>F)\"], anova(m.linear)[2,\"Pr(>F)\"],  anova(m.quad)[2,\"Pr(>F)\"],  anova(m.dichot)[2,\"Pr(>F)\"], anova(m.restrict, m.dilin)[2,\"Pr(>F)\"])\n}\n\n# Proportion significant by model\napply(p.vals<0.05, 2, mean)\n\n1.96*sqrt(.05*(1-.05)/reps)\n\n\ntable(apply(p.vals<.05,1, sum)) / reps\n# Propotion where at least 1 was significant\np.hat <- sum(apply(p.vals<.05,1, sum) >= 1) / reps\n\np.hat\np.hat + c(-1.96,1.96)*sqrt(p.hat*(1-p.hat)/reps)\n```\n:::\n\n\n-   Individual Model Results\n\n    -   Empirical type I error for each method of analysis individually\n\n    |                 |                   |\n    |:----------------|------------------:|\n    | Model           | Emp. Type-I error |\n    | ANOVA           |             0.049 |\n    | Linear          |             0.046 |\n    | Quadratic       |             0.046 |\n    | Dichotomized    |             0.050 |\n    | Dichot + linear |             0.041 |\n\n-   Multiple comparison issues\n\n    -   With 5 hypothesis tests at a nominal 0.05 level, experiment-wise error rate is at most $0.25$ ($0.05 \\times 5$)\n\n    -   Worst-case assumes that all tests are mutually exclusive\n\n        -   e.g. If the linear dose-response model is significant, no other model is more likely to be significant\n\n        -   In fact, the tests will be correlated\n\n    -   How many of the 1000 simulated trials had at least on model with a $p$-value $< 0.05$?\n\n        -   From the simulation, I found this to be $122$ or $12.2\\%$\n\n        -   Note that there is error in this estimate (due to the simulation randomness)\n\n            -   95% CI: \\[$10.2\\%, 14.2\\%$\\]\n\n-   General statistical issues\n\n    -   The true type 1 error rate for such data driven analyses will depend on several factors\n\n        -   The number of tests performed\n\n        -   The models considered\n\n            -   Similar models will tend to reject the null hypotheses on the same dataset\n\n        -   The distribution of the data\n\n            -   In particular, heavy tailed distributions decreases the concordance between the tests\n\n-   When you have multiple models you are considering, the conclusions are less strong\n\n    -   The p-values (or other metrics) can still be useful in ordering the associations\n\n    -   Among all of the models considered, it appears as if SNP X is the most strongly associated with CVD\n\n        -   Would be useful to put a CI around this ranking as well\n\n### Post hoc adjustments for multiple comparisons\n\n-   In frequentist reasoning, we try to ensure that our error rate is held at some level $\\alpha$\n\n    -   When only considering one decision, this is relatively easy\n\n    -   When making multiple decisions, we must consider the experiment-wise error rate\n\n-   In the worst case scenario, an error rate of $\\alpha$ on each decision could lead to an experiment-wise error rate that is as high as $k \\times \\alpha$\n\n    -   Such would be the case if all of our errors were mutually exclusive\n\n-   If all error were independent of each other, then the experiment-wise error rate is\n\n    -   $1 - (1 - \\alpha)^k$\n\n-   Experiment-wise error rates ($\\alpha = 0.05$ at each decision)\n\n    |             |            |             |\n    |:-----------:|:----------:|:-----------:|\n    |  Number of  | Worst Case | Independent |\n    | Comparisons |  Scenario  |   Errors    |\n    |      1      |   0.0500   |   0.0500    |\n    |      2      |   0.1000   |   0.0975    |\n    |      3      |   0.1500   |   0.1426    |\n    |      5      |   0.2500   |   0.2262    |\n    |     10      |   0.5000   |   0.4013    |\n    |     20      |   1.0000   |   0.6415    |\n    |     50      |   1.0000   |   0.9231    |\n\n-   When making multiple comparison which all tend address the same scientific question, we may adjust our level of significance to protect the experiment-wise error rate\n\n    -   The problem with this approach is does not adjust for any bias in parameter estimates\n\n-   Bonferroni Correction\n\n    -   Assumes the worst case scenario\n\n    -   When making $k$ comparisons, either\n\n        -   Tests individual $p$-values against $\\frac{\\alpha}{k}$\n\n        -   Multiply $p$-values by $k$ and compare to $\\alpha$ (keeping the $p$-values $< 1$)\n\n-   Bonferroni is easy and it can be applied in all settings\n\n    -   Extremely conservative when the statistics from various tests are positively correlated\n\n-   Many other varieties of adjusting after performing multiple comparisons\n\n    -   Tukey, Scheffe, etc.\n\n    -   None are great\n\n    -   Did they really adjust for all of the comparisons they made? Probably not.\n\n    -   My strong preference is to avoid multiple comparisons in the first place\n\n        -   If there was some model fitting involved to get to the final model, acknowledge that fact in the paper\n\n        -   Understand the science\n\n        -   Avoid data-driven approaches when you care about correct statistical inference (CIs and p-values)\n",
    "supporting": [
      "Lec12.ANOVA.Model.Choice_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}