[
  {
    "objectID": "notes/Lec12.ANOVA.Model.Choice.html",
    "href": "notes/Lec12.ANOVA.Model.Choice.html",
    "title": "ANOVA and Model Choice",
    "section": "",
    "text": "Code\nlibrary(rms)\nlibrary(ggplot2)\nlibrary(splines)\nlibrary(lspline)\nlibrary(lmtest)\nlibrary(sandwich)\nlibrary(car)\nlibrary(kableExtra)\nlibrary(plotrix)"
  },
  {
    "objectID": "notes/Lec12.ANOVA.Model.Choice.html#overview",
    "href": "notes/Lec12.ANOVA.Model.Choice.html#overview",
    "title": "ANOVA and Model Choice",
    "section": "1 Overview",
    "text": "1 Overview\n\nThere are many different ways we can model predictors\nConsider what alternative models will make scientific sense\nWhat is the impact of letting the data drive the selection of a model\nI am going to discuss in terms of a clinical trial where we have replicates at dose levels\n\nWe can find dose-specific means and compare modeling approaches"
  },
  {
    "objectID": "notes/Lec12.ANOVA.Model.Choice.html#anova-versus-linear-continuous-models",
    "href": "notes/Lec12.ANOVA.Model.Choice.html#anova-versus-linear-continuous-models",
    "title": "ANOVA and Model Choice",
    "section": "2 ANOVA versus Linear Continuous Models",
    "text": "2 ANOVA versus Linear Continuous Models\n\nCompare power of linear continuous models versus ANOVA as a function\n\nof trend in means AND\nstandard errors withing groups\n\nANOVA (dummy variables)\n\nUses indicator variables for every dose (group) level\n\nAgain, I am thinking about “dose” in a general sense that could including covariates like age, cholesterol, blood pressure, etc.\nTraditionally, dose would just be dose of some treatment\n\nFits group means exactly (saturated model)\n\nOne way ANOVA: One categorical predictor\nTwo way ANOVA: Two categorical predictors\n\nFit with the interactions to get group means exactly\n\n\nSaturated models do not mix random error with systematic error\n\nSystematic error: Error due to differences from sample means from predicted means\nRandom error: Error that cannot be explained after controlling for dose\n\nANOVA ignores the ordering of the groups, so it gains no power from trends\n\ne.g. does not assume that the difference between dose=15 and dose=30 group is similar to the difference between the dose=30 and dose=45 groups\nIn fact, the same level of significance is gained no matter what permutation of dose groups is used\n\n\nLinear continuous models\n\nBorrows information across groups\n\nAccurate and efficient if the model is correct\n\nIf model is incorrect, mixes random and systematic error\n\nWill have some systematic error because the means are not predicted exactly\n\nCan gain power from ordering of groups in order to detect a trend\n\nBut, no matter how low the standard error is, if there is no trend in the mean, there is no statistical significance\n\n\n\n\n\nCode\nn &lt;- 10\ndose.levels &lt;- c(0,20,40,60,80)\ndose &lt;- rep(dose.levels, n)\nset.seed(37)\n\ny1 &lt;- 300 + 5*dose + rnorm(5*n, 0, 200)\ny2 &lt;- 300 + 200*(dose==20) + 100*(dose==40) - 100*(dose==60) + 0*(dose==80) + rnorm(5*n, 0, 200)\n\nm1 &lt;- lm(y1 ~ dose)\nm2 &lt;- lm(y1 ~ factor(dose))\n\n#m3 &lt;- lm(y2 ~ dose)\nm4 &lt;- lm(y2 ~ factor(dose))\n\np1 &lt;- predict(m1, newdata=data.frame(dose=dose.levels), se.fit=TRUE)\np2 &lt;- predict(m2, newdata=data.frame(dose=dose.levels), se.fit=TRUE)\n\n#p3 &lt;- predict(m3, newdata=data.frame(dose=dose.levels), se.fit=TRUE)\np4 &lt;- predict(m4, newdata=data.frame(dose=dose.levels), se.fit=TRUE)\n\n\n\n\nCode\npar(mfrow=c(2,2))\nplotCI(x=dose.levels, y=p1$fit, p1$se.fit, ylab=\"Response\", xlab=\"Dose\", ylim=c(0,1000), main=\"Linear: High power; ANOVA: High Power\")\nplotCI(x=dose.levels, y=p1$fit, 3*p1$se.fit, ylab=\"Response\", xlab=\"Dose\", ylim=c(0,1000), main=\"Linear: Mod power; ANOVA: Low Power\")\n\nplotCI(x=dose.levels[c(1,5,4,2,3)], y=p1$fit, p1$se.fit, ylab=\"Response\", xlab=\"Dose\", ylim=c(0,1000), main=\"Linear: No power; ANOVA: High Power\")\nplotCI(x=dose.levels[c(1,5,4,2,3)], y=p1$fit, 3*p1$se.fit, ylab=\"Response\", xlab=\"Dose\", ylim=c(0,1000), main=\"Linear: No power; ANOVA: Low Power\")\n\n\n\n\n\nComparsion of power using ANOVA versus linear dose for detecting a dose effect for various dose-response relationships\n\n\n\n\n\nOther options for modeling continuous predictors\n\nCombinations of linear trends and indicator variables\nSplines\nPolynomials\netc."
  },
  {
    "objectID": "notes/Lec12.ANOVA.Model.Choice.html#choice-of-transformation",
    "href": "notes/Lec12.ANOVA.Model.Choice.html#choice-of-transformation",
    "title": "ANOVA and Model Choice",
    "section": "3 Choice of Transformation",
    "text": "3 Choice of Transformation\n\nThe exact form used to model predictors should be based on scientific (first) and statistical (second) criteria\nScientific issues\n\nThe form used to model predictors must address the specific scientific question\n\nShould be the next logical step in the process of investigating the overall goal\nFirst, establish some sort of an association\nSecond, detect a first order trend\nThird, detecting specific forms of non-linearities\n\nThreshold effects?\nU- or S-shaped trends?\n\nFinally, more complex models\n\nWhen the scientific question relates to prediction, it is imperative that the regression model accurately reflects the true relationship between predictors and the summary measure of response\n\nFailure to have the correct model will guarantee that some groups may not have the correct predicted response\n\nWhen the scientific question relates to detection of associations, the importance of having the true model depends on the statistical role of the predictor\n\nWith the predictor of interest, the most important issues is to protect the validity of the statistical inference\n\nData driven decision will inflate the type I error rate\n\nWith precision variables, it is not as crucial that the true relationship be modeled\n\nAn approximate model will provide most of the precision gains\n\nWith confounders, failure to accurately model the relationship between the confounder and the response may lead to residual confounding\n\nSometimes we will use very flexible models for continuous confounders as there is little cost to doing so and the potential for imporant gain\n\n\nAs the goal of any analysis is to communicate findings to the greater scientific community, it is also important that modeling of predictors is easy to understand\n\nThis is an issue that matters most for your predictor of interest\nWe are generally not worried about making inference about precision variables or confounders\n\n\nStatistical issues\n\nThe greatest statistical precision will be gained when the model reflects the true relationship between the predictor and the response\n\nAccurate modeling of the relationship will avoid introducing systematic error in the estimates of the standard errors\nParsimony: Using the fewest parameters to model the relationship will allow greater precision\nPrecision is a trade-off between parsimony and increased accuracy from including more parameters\n\nWe should select the form of modeling the predictor before looking at the data\n\nData drive selection of transformations will tend to lead to inaccurate (anti-conservative) statistical inference\nOverfitting of the data leads to spuriously low estimates of the within group variability\n\nThus standard errors estimates are too low\nType-I errors are inflated\nConfidence interval are too narrow (inaccurate coverage probabilities)\n\nData-driven model selection will also lead to coefficient estimates that are biased away from the null (leading you to overstate your scientific effects)"
  },
  {
    "objectID": "notes/Lec12.ANOVA.Model.Choice.html#example-beta-carotene-supplements",
    "href": "notes/Lec12.ANOVA.Model.Choice.html#example-beta-carotene-supplements",
    "title": "ANOVA and Model Choice",
    "section": "4 Example: Beta Carotene Supplements",
    "text": "4 Example: Beta Carotene Supplements\n\n4.1 Overview\n\nBefore doing large scale clinical trials, it is important to understand the pharmacokinetics of a drug\nPhase II prevention trials often administer a drug in various doses to volunteers, and pertinent plasma levels are then measured at regular intervals\nOf particular interest is how dose level affects the build up of drug in the plasma over time, as well as how the dose level might affect other blood chemistry\nForty-six (46) volunteers were randomly assigned to receive one of five doses of beta-carotene (0, 15, 30, 45, or 60 mg/day) for 9 months in a double blind fashion\nThe specific aim was to determine how different dose levels affected the serum beta-carotene levels after 9 months\nOther measured variables available in this data set include subject age, sex, weight, body mass index, percent body fat, and serum cholesterol level at baseline\n\n\n\nCode\ncarot &lt;- stata.get(\"data/carot.dta\")\nres1 &lt;- aggregate(carot3 ~ dose, data = carot,\nFUN = function(x) c(n = length(x), mean = mean(x), sd = sd(x),\nmin = min(x), q25 = quantile(x, 0.25),\nmedian = median(x), q75 = quantile(x, 0.75),\nmax = max(x)))\n\nprint(res1, digits=2)\n\n\n  dose carot3.n carot3.mean carot3.sd carot3.min carot3.q25.25% carot3.median\n1    0        7         186        88         84            133           149\n2   15        8        1254       570        577            723          1250\n3   30        9        1505       479        849           1157          1498\n4   45        7        1749       579        950           1333          1848\n5   60        9        1878       430       1233           1725          1865\n  carot3.q75.75% carot3.max\n1            241        323\n2           1682       2019\n3           1840       2248\n4           2234       2310\n5           1918       2855\n\n\n\nIn this randomized trial, we can consider several potential response variables\n\nPlasma level at the end of treatment\nChange in plasma level over the treatment period\nEither of the above adjusted for baseline plasma (ANCOVA model)\n\nAccounting for baseline\n\nDose group \\(i\\), subject \\(j\\), time \\(t\\)\n\\(Y_{ijt} \\sim (\\mu_{it}, \\sigma^2)\\); \\(\\textrm{corr}(Y_{ij0}, Y_{ij9}) = \\rho\\) \\[\\begin{aligned}\n\\overline{Y}_{i\\cdot9} & \\sim & \\left(\\mu_{i9}, \\sigma^2/n \\right) \\\\\n\\overline{Y}_{i\\cdot9} - \\overline{Y}_{i\\cdot0} & \\sim & \\left(\\mu_{i9} - \\mu_{i0}, 2\\sigma^2(1-\\rho)/n \\right) \\\\\n\\overline{Y}_{i\\cdot9} - \\rho \\overline{Y}_{i\\cdot0} & \\sim & \\left(\\mu_{i9} - \\rho \\mu_{i0}, \\sigma^2(1-\\rho^2)/n \\right)\\end{aligned}\\]\nCompared variances of the above three equations\n\nWhen are the variances equal, smaller, larger\nWhich is always smallest\n\n\nBy randomization, there will be equal means at baseline\n\n\\(\\mu_{T,0} = \\mu_{P,0}\\) where \\(T\\) is any of the treatment doses and \\(P\\) is placebo\n\nContrast across dose groups \\[\\begin{aligned}\n\\overline{Y}_{T,\\cdot9} - \\overline{Y}_{P,\\cdot9} & \\sim & \\left(\\mu_{T,9} - \\mu_{P,9}, 2\\sigma^2/n \\right) \\\\\n\\left(\\overline{Y}_{T,\\cdot9} - \\overline{Y}_{T, \\cdot0}\\right) - \\left(\\overline{Y}_{P,\\cdot9} - \\overline{Y}_{P, \\cdot0}\\right) & \\sim & \\left(\\mu_{T,9} - \\mu_{P,9}, 4\\sigma^2(1-\\rho)/n \\right) \\\\\n\\left(\\overline{Y}_{T,\\cdot9} - \\rho \\overline{Y}_{T,\\cdot0}\\right) - \\left(\\overline{Y}_{P,\\cdot9} - \\rho \\overline{Y}_{P,\\cdot0}\\right) & \\sim & \\left(\\mu_{T,9} - \\mu_{P,9}, 2\\sigma^2(1-\\rho^2)/n \\right)\\end{aligned}\\]\nSimple linear regression\n\nRegress \\(Y\\) on \\(X\\)\n\n\\(Y_i \\sim \\left(\\mu_Y, \\sigma^2_Y\\right)\\); \\(X_i \\sim \\left(\\mu_X, \\sigma^2_X\\right)\\)\n\\(\\textrm{corr}\\left(Y_i, X_i \\right) = \\rho\\)\n\nRegression model: \\(E[Y_i | X_i] = \\beta_0 + \\beta_1 X_i\\)\n\n\\(\\beta_0 = \\mu_Y - \\beta_1 \\mu_x\\)\n\\(\\beta_1 = \\rho \\frac{\\sigma_Y}{\\sigma_X}\\)\n\n\nAnalysis of Covariance\n\nDose group \\(i\\), subject \\(j\\), time \\(t\\)\n\\(Y_{ijt} \\sim (\\mu_{it}, \\sigma^2)\\); \\(\\textrm{corr}(Y_{ij0}, Y_{ij9}) = \\rho\\)\nRegression model: \\(E[Y_{ij9} | Y_{i_j0}] = \\beta_0 + \\beta_1 Y_{ij0}\\)\n\n\\(\\beta_1 = \\rho\\)\n\n\n\n\n\n4.2 Methods for modeling dose response\n\nIn a randomized clinical trial, we will tend to have the greatest precision if we adjust for baseline as a predictor in a linear regression model\nA wide variety of models may be considered for examining the relationship between dose and plasma levels\n\nDummy variables where we model each dose level independently, without borrowing information across groups (ANOVA)\nLinear continuous predictors (transformed or untransformed)\nDichotomization (at any of several thresholds)\nPolynomials, splines, other flexible methods\nCombinations of the above\nEven more complex models\n\nI will compare possible models\n\nGraphically: Show data and fitted values without adjustment for baseline\nNumerically: Show regression estimates and tests after adjustment for baseline\nNote that this is an academic exercise and not something you would do in practice to come up with the “best” model\n\nPredicted values\n\nAfter computing a regression command, Stata will provide predicted values for each case\n\nMathematically, this is just the intercept plus the regression parameters multiplied by the covariates for each case\nStata command: predict varname\n\n\n\n\n\n4.3 ANOVA analysis\n\nFits each group independently\nDoes not use the ordering of the dose groups when looking for an effect\n\nCompletely ignores the magnitude and ordering of the \\(x\\)-axis\n\nA priori, we might expect this is not the most efficient method if the alternative hypothesis is true\n\nWe expect larger plasma levels with increasing dose\nWe will thus have less power to detect a first-order trend\n\n\n\n\nCode\ncarot$dose.factor &lt;- factor(carot$dose)\nm.factor &lt;- lm(carot3 ~ dose.factor + carot0, data=carot)\ncoeftest(m.factor, vcov=sandwich)\n\n\n\nt test of coefficients:\n\n                Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept)   -361.45165  154.46722 -2.3400 0.0252926 *  \ndose.factor15 1224.18959  196.89130  6.2176 4.500e-07 ***\ndose.factor30 1439.83731  143.63572 10.0242 1.096e-11 ***\ndose.factor45 1678.98389  154.10485 10.8951 1.242e-12 ***\ndose.factor60 1791.00899  141.00926 12.7014 1.838e-14 ***\ncarot0           1.90279    0.49509  3.8433 0.0005062 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nCode\nplot(jitter(carot$dose, factor=.2), carot$carot3, ylab=\"Plasma Beta Carotene\", xlab=\"Dose of Supplementation\", main=\"Model: Dummy Variables (ANOVA)\")\nm1 &lt;- lm(carot3~factor(dose), data=carot)\npoints(seq(0,60, by=15), predict(m1, newdata=data.frame(dose=seq(0,60, by=15))), pch=\"O\", cex=2)\npoints(seq(0,60, by=15), predict(m1, newdata=data.frame(dose=seq(0,60, by=15))), pch=\"X\", cex=2)\nlines(seq(0,60, by=15), predict(m1, newdata=data.frame(dose=seq(0,60, by=15))))\nlegend(\"topleft\", c(\"Fitted\", \"Group Means\"), pch=c(\"X\",\"O\"), lty=c(NA,1), bty=\"n\")\n\n\n\n\n\n\nTesting for the dose effect\n\nWe must use the testparm command (or test) because the model includes the baseline measurement\ntestparm is similar to test, but allows testing multiple parameters using wildcards\n\n\n\n\nCode\nlinearHypothesis(m.factor, c(\"dose.factor15\",\n                       \"dose.factor30\",\n                       \"dose.factor45\",\n                       \"dose.factor60\"),\n                 vcov=sandwich(m.factor))\n\n\nLinear hypothesis test\n\nHypothesis:\ndose.factor15 = 0\ndose.factor30 = 0\ndose.factor45 = 0\ndose.factor60 = 0\n\nModel 1: restricted model\nModel 2: carot3 ~ dose.factor + carot0\n\nNote: Coefficient covariance matrix supplied.\n\n  Res.Df Df      F    Pr(&gt;F)    \n1     38                        \n2     34  4 69.959 6.303e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nWe would have had the same fitted values (and thus inference) if we had decided to drop a different dose group\n\nExample: Making my own dummy variables for dose, with dose at 60 being the reference group\n\n\n\n\nCode\ncarot$dose.new &lt;- relevel(carot$dose.factor, ref=\"60\")\n\nm2.factor &lt;- lm(carot3 ~ dose.new + carot0, data=carot)\ncoeftest(m2.factor, vcov=sandwich)\n\n\n\nt test of coefficients:\n\n               Estimate  Std. Error  t value  Pr(&gt;|t|)    \n(Intercept)  1429.55735   163.01893   8.7693 3.010e-10 ***\ndose.new0   -1791.00899   141.00926 -12.7014 1.838e-14 ***\ndose.new15   -566.81940   221.47493  -2.5593 0.0151082 *  \ndose.new30   -351.17168   173.86127  -2.0198 0.0513369 .  \ndose.new45   -112.02510   188.55038  -0.5941 0.5563522    \ncarot0          1.90279     0.49509   3.8433 0.0005062 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\nlinearHypothesis(m2.factor, c(\"dose.new0\",\n                       \"dose.new15\",\n                       \"dose.new30\",\n                       \"dose.new45\"),\n                 vcov=sandwich(m2.factor))\n\n\nLinear hypothesis test\n\nHypothesis:\ndose.new0 = 0\ndose.new15 = 0\ndose.new30 = 0\ndose.new45 = 0\n\nModel 1: restricted model\nModel 2: carot3 ~ dose.new + carot0\n\nNote: Coefficient covariance matrix supplied.\n\n  Res.Df Df      F    Pr(&gt;F)    \n1     38                        \n2     34  4 69.959 6.303e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nNote that the parameter estimates all will lead to the same fitted values\n\ne.g. Intercept in above model (1430) equals the intercept + dose60 coefficient (-361 + 1791) in previous model\n\nOverall F statistics, R-squared, Root MSE all the same\nPartial t-tests tend to differ as we are making comparisons to different reference groups\nCould also fit the same model with no intercept\n\nWould then have to include all five dose groups\nWe can get Stata to include fit all five dose groups and no intercept using the noconstant option\nIn R, fit a model without an intercept by adding a \\(-1\\) in the model equation (e.g. \\(y \\sim -1 + x\\))\nNot including the intercept changes the overall F statistic and the R-squared measures\n\n\n\n\nCode\nm3.factor &lt;- lm(carot3 ~ -1 + dose.factor + carot0, data=carot)\ncoeftest(m3.factor, vcov=sandwich)\n\n\n\nt test of coefficients:\n\n                Estimate Std. Error t value  Pr(&gt;|t|)    \ndose.factor0  -361.45165  154.46722 -2.3400 0.0252926 *  \ndose.factor15  862.73795  223.06284  3.8677 0.0004724 ***\ndose.factor30 1078.38567  165.01387  6.5351 1.753e-07 ***\ndose.factor45 1317.53225  205.85752  6.4002 2.615e-07 ***\ndose.factor60 1429.55735  163.01893  8.7693 3.010e-10 ***\ncarot0           1.90279    0.49509  3.8433 0.0005062 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nCorrespondence of the no-intercept model compared to previous models\n\nSome textbooks refer to this as a “cell means” coding system\n\nIf we didn’t have baseline beta carotene in the model, the dose parameters would correspond directly to the means in each dose group\nWith baseline beta carotene in the model, the dose parameters are the means when carot0 is 0\n\nIn terms of model fit, the model is the same as before\n\nNo intercept means each dose group is compared to a mean of 0\n\nFitted values will be the same\nTest of dose effect will need to test equality of all five dose covariates\n\nThis is not a test that these 5 parameters are 0\n\\(H_0: dose0 = dose15 = dose30 = dose45 = dose60\\)\n\\(H_1:\\) at least one of the above is not equal\n\n\n\n\n\nCode\nlinearHypothesis(m3.factor, c(\"dose.factor0=dose.factor15\",\n                              \"dose.factor0=dose.factor30\",\n                              \"dose.factor0=dose.factor45\",\n                              \"dose.factor0=dose.factor60\"),\n                 vcov=sandwich(m3.factor))\n\n\nLinear hypothesis test\n\nHypothesis:\ndose.factor0 - dose.factor15 = 0\ndose.factor0 - dose.factor30 = 0\ndose.factor0 - dose.factor45 = 0\ndose.factor0 - dose.factor60 = 0\n\nModel 1: restricted model\nModel 2: carot3 ~ -1 + dose.factor + carot0\n\nNote: Coefficient covariance matrix supplied.\n\n  Res.Df Df      F    Pr(&gt;F)    \n1     38                        \n2     34  4 69.959 6.303e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n4.4 Binary dose: Placebo versus Active\n\nDichotomize into dose 0 versus dose \\(&gt;\\) 0\n\nWill be an accurate model if all (or virtually all) of the effect is attained at the lowest dose level\nOften used when little is know about a treatment, or when dose is difficult to quantify\n\ne.g. Smoking\nWe are relatively certain of a smoking effect, so our major scientific interest is likely related to the dose-response relationship above the lowest dose\n\n\n\n\n\nCode\ncarot$trt &lt;- (carot$dose&gt;0)+0\n\nm.trt &lt;- lm(carot3 ~ trt + carot0, data=carot)\ncoeftest(m.trt, vcov=sandwich(m.trt))\n\n\n\nt test of coefficients:\n\n              Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept) -406.68159  207.08860 -1.9638   0.05710 .  \ntrt         1544.22049  115.44469 13.3763 9.563e-16 ***\ncarot0         2.05991    0.68203  3.0203   0.00456 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nCode\nm2 &lt;- lm(carot3 ~ dose&gt;0, data=carot)\nplot(jitter(carot$dose, factor=.2), carot$carot3, ylab=\"Plasma Beta Carotene\", xlab=\"Dose of Supplementation\", main=\"Model: Dichotomous Dose\")\nlines(seq(0,60, by=15), predict(m1, newdata=data.frame(dose=seq(0,60, by=15))), pch=\"O\", cex=2)\npoints(seq(0,60, by=15), predict(m1, newdata=data.frame(dose=seq(0,60, by=15))), pch=\"O\", cex=2)\npoints(seq(0,60, by=15), predict(m2, newdata=data.frame(dose=seq(0,60, by=15))), pch=\"X\", cex=2)\nlegend(\"topleft\", c(\"Fitted\", \"Group Means\"), pch=c(\"X\",\"O\"), lty=c(NA,1), bty=\"n\")\n\n\n\n\n\n\n\n4.5 Linear, continuous dose\n\nEstimates the best fitting straight line to response\n\nAccurate if the response is linear\n\nOften used when little is know about the treatment and a general trend is expected\n\nIn this particular application, we are relatively certain of an effect, so our major interest is in modeling the dose response relationship above 0.\n\n\n\n\nCode\nm.cont &lt;- lm(carot3 ~ dose + carot0, data=carot)\ncoeftest(m.cont, vcov=sandwich(m.cont))\n\n\n\nt test of coefficients:\n\n             Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept) 245.00028  214.29566  1.1433   0.26027    \ndose         25.46276    3.49367  7.2883 1.175e-08 ***\ncarot0        1.33409    0.63189  2.1113   0.04156 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nTo test the treatment effect, could either use the test command for dose or use the output directly as we are only testing one parameter\n\n\n\nCode\nm3 &lt;- lm(carot3 ~ dose, data=carot)\nplot(jitter(carot$dose, factor=.2), carot$carot3, ylab=\"Plasma Beta Carotene\", xlab=\"Dose of Supplementation\", main=\"Model: Linear Dose\")\nlines(seq(0,60, by=15), predict(m1, newdata=data.frame(dose=seq(0,60, by=15))), pch=\"O\", cex=2)\npoints(seq(0,60, by=15), predict(m1, newdata=data.frame(dose=seq(0,60, by=15))), pch=\"O\", cex=2)\npoints(seq(0,60, by=15), predict(m3, newdata=data.frame(dose=seq(0,60, by=15))), pch=\"X\", cex=2)\nlegend(\"topleft\", c(\"Fitted\", \"Group Means\"), pch=c(\"X\",\"O\"), lty=c(NA,1), bty=\"n\")\n\n\n\n\n\n\n\n4.6 Polynomial models of dose\n\nFit terms involving dose, dose squared\n\nOften used to fit U-shaped trends\nIn general, a quadratic is a pretty strong assumption in that it assumes constant curvature over dose\n\n\n\n\nCode\nm.poly &lt;- lm(carot3 ~ poly(dose,2) + carot0, data=carot)\ncoeftest(m.poly, vcov=sandwich(m.poly))\n\n\n\nt test of coefficients:\n\n                  Estimate  Std. Error t value  Pr(&gt;|t|)    \n(Intercept)      935.24702   157.31460  5.9451 8.229e-07 ***\npoly(dose, 2)1  3652.11483   377.96287  9.6626 1.541e-11 ***\npoly(dose, 2)2 -1716.63464   351.59782 -4.8824 2.151e-05 ***\ncarot0             1.72807     0.53494  3.2304  0.002641 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nCode\nm4 &lt;- lm(carot3 ~ poly(dose, 2), data=carot)\nplot(jitter(carot$dose, factor=.2), carot$carot3, ylab=\"Plasma Beta Carotene\", xlab=\"Dose of Supplementation\", main=\"Model: Quadratic Dose\")\nlines(seq(0,60, by=15), predict(m1, newdata=data.frame(dose=seq(0,60, by=15))), pch=\"O\", cex=2)\npoints(seq(0,60, by=15), predict(m1, newdata=data.frame(dose=seq(0,60, by=15))), pch=\"O\", cex=2)\npoints(seq(0,60, by=15), predict(m4, newdata=data.frame(dose=seq(0,60, by=15))), pch=\"X\", cex=2)\nlegend(\"topleft\", c(\"Fitted\", \"Group Means\"), pch=c(\"X\",\"O\"), lty=c(NA,1), bty=\"n\")\n\n\n\n\n\n\nThe partial t-test for dosesqr can be interpreted as a test for linear dose response\n\nIt is highly significant, suggestion departure from linearity\n\nTo test the treatment effect, we need to test the two dose covariates\n\n\n\nCode\nlinearHypothesis(m.poly, c(\"poly(dose, 2)1\",\n                           \"poly(dose, 2)2\"),\n                 vcov=sandwich(m.poly))\n\n\nLinear hypothesis test\n\nHypothesis:\npoly(dose, 2)1 = 0\npoly(dose, 2)2 = 0\n\nModel 1: restricted model\nModel 2: carot3 ~ poly(dose, 2) + carot0\n\nNote: Coefficient covariance matrix supplied.\n\n  Res.Df Df      F    Pr(&gt;F)    \n1     38                        \n2     36  2 93.951 5.157e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n4.7 Highest order polynomial models versus ANOVA\n\nWith 5 discrete dose levels, a 4th degree polynomial will fit the means exactly\nThus, the model will have the same fit as the ANOVA model using dummy variables for each levels of dose\n\nHigher order polynomials are borrowing less information across dose groups\nHighest order polynomial borrows no information across dose groups\n\n\n\n. gen dosecub = dose^3\n. gen dosequad = dose^4\n. regress carot3 dose dosesqr dosecub dosequad carot0, robust\n\nLinear regression                                      Number of obs =      40\n                                                       F(  5,    34) =   47.68\n                                                       Prob &gt; F      =  0.0000\n                                                       R-squared     =  0.7184\n                                                       Root MSE      =  417.46\n\n------------------------------------------------------------------------------\n             |               Robust\n      carot3 |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n        dose |    157.876   61.94333     2.55   0.015     31.99197    283.7599\n     dosesqr |  -6.943752   5.066692    -1.37   0.180    -17.24051    3.353004\n     dosecub |   .1385695   .1313523     1.05   0.299    -.1283706    .4055096\n    dosequad |  -.0009734   .0010718    -0.91   0.370    -.0031515    .0012047\n      carot0 |   1.902792   .5370015     3.54   0.001     .8114738     2.99411\n       _cons |  -361.4516   167.5432    -2.16   0.038    -701.9404   -20.96284\n------------------------------------------------------------------------------\n\n\n. xi: regress carot3 i.dose carot0, robust\ni.dose            _Idose_0-60         (naturally coded; _Idose_0 omitted)\n\nLinear regression                                      Number of obs =      40\n                                                       F(  5,    34) =   47.68\n                                                       Prob &gt; F      =  0.0000\n                                                       R-squared     =  0.7184\n                                                       Root MSE      =  417.46\n\n------------------------------------------------------------------------------\n             |               Robust\n      carot3 |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n   _Idose_15 |    1224.19   213.5586     5.73   0.000     790.1863    1658.193\n   _Idose_30 |   1439.837   155.7948     9.24   0.000     1123.224     1756.45\n   _Idose_45 |   1678.984   167.1502    10.04   0.000     1339.294    2018.674\n   _Idose_60 |   1791.009    152.946    11.71   0.000     1480.185    2101.833\n      carot0 |   1.902792   .5370015     3.54   0.001     .8114738     2.99411\n       _cons |  -361.4516   167.5432    -2.16   0.038    -701.9404   -20.96284\n------------------------------------------------------------------------------\n\n\n4.8 Threshold at 0 and Linear Term\n\nThreshold at 0 and linear dose\nTo fit, use a dummy variable for dose0 plus dose (continuous)\n\nFits dose 0 by its group mean\nFits dose \\(&gt;\\) 0 by a line (an intercept and slope)\nAllows us to address two scientific questions\n\nIs there any effect of dose? (test both slopes)\nIs there any additional benefit beyond the lowest dose? (test linear term’s slope)\n\n\n\n\n. regress carot3 trt dose carot0, robust\n\nLinear regression                                      Number of obs =      40\n                                                       F(  3,    36) =   81.26\n                                                       Prob &gt; F      =  0.0000\n                                                       R-squared     =  0.7170\n                                                       Root MSE      =  406.69\n\n------------------------------------------------------------------------------\n             |               Robust\n      carot3 |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n         trt |   1050.836    223.418     4.70   0.000     597.7237    1503.949\n        dose |   12.81144   4.794314     2.67   0.011     3.088122    22.53476\n      carot0 |   1.904584   .5164903     3.69   0.001     .8570932    2.952075\n       _cons |  -361.9675   161.4254    -2.24   0.031    -689.3534   -34.58168\n------------------------------------------------------------------------------\n\n\nCode\nm5 &lt;- lm(carot3 ~ (dose &gt;0) + dose, data=carot)\nplot(jitter(carot$dose, factor=.2), carot$carot3, ylab=\"Plasma Beta Carotene\", xlab=\"Dose of Supplementation\", main=\"Model: Threshold and Linear Dose\")\nlines(seq(0,60, by=15), predict(m1, newdata=data.frame(dose=seq(0,60, by=15))), pch=\"O\", cex=2)\npoints(seq(0,60, by=15), predict(m1, newdata=data.frame(dose=seq(0,60, by=15))), pch=\"O\", cex=2)\npoints(seq(0,60, by=15), predict(m5, newdata=data.frame(dose=seq(0,60, by=15))), pch=\"X\", cex=2)\nlegend(\"topleft\", c(\"Fitted\", \"Group Means\"), pch=c(\"X\",\"O\"), lty=c(NA,1), bty=\"n\")\n\n\n\n\n\n\nTesting the effect of treatment\n\nTwo variables model dose, so we need to test both\nIf response increases from dose 0 to lowest dose OR\n... response increases as dose increase, THEN\n... we will declare an effect of treatment\n\nThe partial t-test for the trt term can be used to test for linear dose response\n\nHere, it is highly significantly different from 0, indicating that just a linear model is not adequate\n\nThe partial t-test for the dose term can be interpreted as a test for any added effect above the lowest dose\n\nIt is significantly different from 0 (\\(p = 0.011\\))\nThere is a multiple comparison issue here, but many people are comfortable doing this ‘step down’ test after they have already tested from any treatment effect\n\n\n\n. test trt dose\n\n ( 1)  trt = 0\n ( 2)  dose = 0\n\n       F(  2,    36) =  121.88\n            Prob &gt; F =    0.0000"
  },
  {
    "objectID": "notes/Lec12.ANOVA.Model.Choice.html#data-driven-model-selection",
    "href": "notes/Lec12.ANOVA.Model.Choice.html#data-driven-model-selection",
    "title": "ANOVA and Model Choice",
    "section": "5 Data driven model selection",
    "text": "5 Data driven model selection\n\nSuppose we look at a scatterplot before deciding which model we fit and choose a model that can fit the data well\n\nIf the data looks like a straight line, choose the model linear in dose\nIf the data looks like a U, choose a quadratic\nIf the data is a complicated pattern of differences among groups, we might choose dummy variables or splines\netc.\n\nThis approach would tend to mimic the behavior of fitting several different models and choosing the model with the lowest \\(p\\)-value\n\nWhen our eye sees some trend in the data, we would be most likely to pick the model giving the lowest \\(p\\)-value\n\n\n\n5.1 Simulation\n\nUsing the 46 subjects in this dataset, I can randomly permute the dose they received\n\nEffectively, randomize subjects to a different dose\nBut, keep their 9-month and baseline beta carotene levels the same (not permuted)\n\nShould remove any association between dose and beta carotene\n\n\nNext, fit each of the five models (linear, quadratic, ANOVA, dichotomized, and dichotomized plus linear)\nRepeat the process 1000 times (representing 1000 studies)\n\nCalculate how often each model rejects the null hypothesis of a dose effect\n\n\n\n\nCode\nset.seed(80)\n\nreps &lt;- 1000\nn &lt;- length(carot$carot3)\np.vals &lt;- matrix(NA, nrow=reps, ncol=5)\n\ny &lt;- carot$carot3\ny0 &lt;- carot$carot0\nm.restrict &lt;- lm(y ~ y0)\n\nfor(i in 1:reps) {\n perm &lt;- sample(1:n)\n\n#Permute the dose, but not the outcome and baseline\n d &lt;- carot$dose[perm]\n\n m.anova &lt;- lm(y ~ y0 + factor(d))\n m.linear &lt;- lm(y ~ y0 + d)\n m.quad &lt;- lm(y ~ y0 + poly(d,2))\n m.dichot &lt;- lm(y ~ y0 + (d&gt;0))\n m.dilin &lt;- lm(y ~ y0 + d + (d&gt;0))\n\n p.vals[i,1:5] &lt;- c(anova(m.anova)[2,\"Pr(&gt;F)\"], anova(m.linear)[2,\"Pr(&gt;F)\"],  anova(m.quad)[2,\"Pr(&gt;F)\"],  anova(m.dichot)[2,\"Pr(&gt;F)\"], anova(m.restrict, m.dilin)[2,\"Pr(&gt;F)\"])\n}\n\n# Proportion significant by model\napply(p.vals&lt;0.05, 2, mean)\n\n1.96*sqrt(.05*(1-.05)/reps)\n\n\ntable(apply(p.vals&lt;.05,1, sum)) / reps\n# Propotion where at least 1 was significant\np.hat &lt;- sum(apply(p.vals&lt;.05,1, sum) &gt;= 1) / reps\n\np.hat\np.hat + c(-1.96,1.96)*sqrt(p.hat*(1-p.hat)/reps)\n\n\n\nIndividual Model Results\n\nEmpirical type I error for each method of analysis individually\n\n\n\n\nModel\nEmp. Type-I error\n\n\nANOVA\n0.049\n\n\nLinear\n0.046\n\n\nQuadratic\n0.046\n\n\nDichotomized\n0.050\n\n\nDichot + linear\n0.041\n\n\n\nMultiple comparison issues\n\nWith 5 hypothesis tests at a nominal 0.05 level, experiment-wise error rate is at most \\(0.25\\) (\\(0.05 \\times 5\\))\nWorst-case assumes that all tests are mutually exclusive\n\ne.g. If the linear dose-response model is significant, no other model is more likely to be significant\nIn fact, the tests will be correlated\n\nHow many of the 1000 simulated trials had at least on model with a \\(p\\)-value \\(&lt; 0.05\\)?\n\nFrom the simulation, I found this to be \\(122\\) or \\(12.2\\%\\)\nNote that there is error in this estimate (due to the simulation randomness)\n\n95% CI: [\\(10.2\\%, 14.2\\%\\)]\n\n\n\nGeneral statistical issues\n\nThe true type 1 error rate for such data driven analyses will depend on several factors\n\nThe number of tests performed\nThe models considered\n\nSimilar models will tend to reject the null hypotheses on the same dataset\n\nThe distribution of the data\n\nIn particular, heavy tailed distributions decreases the concordance between the tests\n\n\n\nWhen you have multiple models you are considering, the conclusions are less strong\n\nThe p-values (or other metrics) can still be useful in ordering the associations\nAmong all of the models considered, it appears as if SNP X is the most strongly associated with CVD\n\nWould be useful to put a CI around this ranking as well\n\n\n\n\n\n5.2 Post hoc adjustments for multiple comparisons\n\nIn frequentist reasoning, we try to ensure that our error rate is held at some level \\(\\alpha\\)\n\nWhen only considering one decision, this is relatively easy\nWhen making multiple decisions, we must consider the experiment-wise error rate\n\nIn the worst case scenario, an error rate of \\(\\alpha\\) on each decision could lead to an experiment-wise error rate that is as high as \\(k \\times \\alpha\\)\n\nSuch would be the case if all of our errors were mutually exclusive\n\nIf all error were independent of each other, then the experiment-wise error rate is\n\n\\(1 - (1 - \\alpha)^k\\)\n\nExperiment-wise error rates (\\(\\alpha = 0.05\\) at each decision)\n\n\n\nNumber of\nWorst Case\nIndependent\n\n\nComparisons\nScenario\nErrors\n\n\n1\n0.0500\n0.0500\n\n\n2\n0.1000\n0.0975\n\n\n3\n0.1500\n0.1426\n\n\n5\n0.2500\n0.2262\n\n\n10\n0.5000\n0.4013\n\n\n20\n1.0000\n0.6415\n\n\n50\n1.0000\n0.9231\n\n\n\nWhen making multiple comparison which all tend address the same scientific question, we may adjust our level of significance to protect the experiment-wise error rate\n\nThe problem with this approach is does not adjust for any bias in parameter estimates\n\nBonferroni Correction\n\nAssumes the worst case scenario\nWhen making \\(k\\) comparisons, either\n\nTests individual \\(p\\)-values against \\(\\frac{\\alpha}{k}\\)\nMultiply \\(p\\)-values by \\(k\\) and compare to \\(\\alpha\\) (keeping the \\(p\\)-values \\(&lt; 1\\))\n\n\nBonferroni is easy and it can be applied in all settings\n\nExtremely conservative when the statistics from various tests are positively correlated\n\nMany other varieties of adjusting after performing multiple comparisons\n\nTukey, Scheffe, etc.\nNone are great\nDid they really adjust for all of the comparisons they made? Probably not.\nMy strong preference is to avoid multiple comparisons in the first place\n\nIf there was some model fitting involved to get to the final model, acknowledge that fact in the paper\nUnderstand the science\nAvoid data-driven approaches when you care about correct statistical inference (CIs and p-values)"
  },
  {
    "objectID": "notes/Lec07.MultivariableModels.html",
    "href": "notes/Lec07.MultivariableModels.html",
    "title": "Multivariable Models",
    "section": "",
    "text": "Code\nlibrary(rms)\nlibrary(ggplot2)"
  },
  {
    "objectID": "notes/Lec07.MultivariableModels.html#overview",
    "href": "notes/Lec07.MultivariableModels.html#overview",
    "title": "Multivariable Models",
    "section": "1 Overview",
    "text": "1 Overview\n\nScientific questions\n\nMost often scientific questions are translated into comparing the distribution of some response variable across groups of interest\nGroups are defined by the predictor of interest (POI)\n\nCategorical predictors of interest: Treatment or control, knockout or wild type, ethnic group\nContinuous predictors of interest: Age, BMI, cholesterol, blood pressure\n\n\nIf we only considered the response and POI, this is referred to as a simple (linear, logistic, PH, etc.) regression model\nOften we need to consider additional variables other than POI because...\n\nWe want to make comparisons in different strata\n\ne.g if we stratify by gender, we may get different answers to our scientific question in men and women\n\nGroups being compared differ in other ways\n\nConfounding: A variable that is related to both the outcome and predictor of interest\n\nLess variability in the response if we control for other variables\n\nPrecision: If we restrict to looking within certain strata, may get smaller \\(\\sigma^2\\)\n\n\nStatistics: Covariates other than the Predictor of Interest are included in the model as...\n\nEffect modifiers\nConfounders\nPrecision variables\n\nTwo main statistical methods to adjust for covariates\n\nStratified analyses\n\nCombines information about associations between response across strata\nWill not borrow information about (or even estimate) associations between response and adjustment variables\n\nAdjustment in multiple regression\n\nCan (but does not have to) borrow information about associations between response and all modeled variables\nCould conduct a stratified analysis using regression\nIn practice, when researchers say they are using regression, they are almost certainly doing so to borrow information\n\n\nExample: Is smoking associated with FEV in teenagers?\n\nStratified Analysis\n\nSeparately estimate mean FEV in 19 year olds, 18 year olds, 17 year olds, etc. by smoking status\nAverage means (using weights) to come up with overall effect of smoking on FEV\nKey: Not trying to estimate a common effect of age across strata (not borrowing information across age)\nNo estimate of the age effect in this analysis\n\nMultiple regression\n\nFit a regression model with FEV as the outcome, smoking as the POI, and age as an adjustment variable\nWill provide you an estimate of the association between FEV and age (but do you care?)\nCan borrow information across ages to estimate the age effect\n\nLinear/spline function for age would borrow information\nSeparate indicator variable for each age would borrow less information (would still assume that all 19.1 and 19.2 year olds are the same)\n\n\n\nAdjustment for two factors: Age and Sex\n\nStratified analyses\n\nCalculate separate means by age and sex, combine using weight averages as before\nThis method adjusts for the interaction of age and sex (in addition to age and sex main effects)\n\nMultiple regression\n\n“We adjusted for age and sex...” or “Holding age and sex constant, we found ...”\nAlmost certainly the research adjusted for age and sex, but not the interaction of the two variables (but they could have)"
  },
  {
    "objectID": "notes/Lec07.MultivariableModels.html#stratified-analysis",
    "href": "notes/Lec07.MultivariableModels.html#stratified-analysis",
    "title": "Multivariable Models",
    "section": "2 Stratified Analysis",
    "text": "2 Stratified Analysis\n\n2.1 Methods\n\nGeneral approach to conducting a stratified analysis\n\nDivide the data into strata based on all combinations of the “adjustment” covariates\n\ne.g. every combination of age, gender, race, SES, etc.\n\nWithin each strata, perform an analysis comparing responses across POI groups\nUse (weighted) average of estimated associations across groups\n\nCombining responses: Easy if estimates are independent and approximately Normally distributed\n\nFor independent strata \\(k\\), \\(k = 1, \\ldots, K\\)\n\nEstimate in stratum \\(k\\): \\(\\hat{\\theta}_k \\sim N (\\theta_k, se^2_k)\\)\nWeight in stratum \\(k\\): \\(w_k\\)\nStratified estimate is\n\\[\\hat{\\theta} = \\frac{\\sum_{k=1}^K w_k \\hat{\\theta}_k}{\\sum_{k=1}^K w_k} \\sim N\\left(\\frac{\\sum_{k=1}^K w_k \\theta_k}{\\sum_{k=1}^K w_k}, \\frac{\\sum_{k=1}^K w_k^2 se^2_k}{\\left(\\sum_{k=1}^K w_k\\right)^2} \\right)\\]\n\n\nHow to choose the weights?\n\nScientific role of the stratified estimate\n\nJust because I have more women in my sample than men, does that mean I should weight my estimate towards women? Maybe, maybe not.\n\nStatistical precision of the stratified estimate\n\nJust because the data are more variable in women than men, does that mean I should down-weight women? Maybe, maybe not.\nWeight usually chosen on statistical criteria\n\n\nWeights should be chosen based on the statistical role of the adjustment variable\n\nEffect modifiers\nConfounding\nPrecision\n\n\n\n\n2.2 Weights for Effect Modification\n\nScientific criteria\n\nSometimes we anticipate effect modification by some variables, but\n\nWe do not choose to report estimates of the association between the response and POI in each stratum separately\n\ne.g. political polls, age adjusted incidence rates\n\nWe are interested in estimating the “average association” for a population\n\n\nChoosing weights according to scientific importance\n\nWant to estimate the average effect in some population of interest\n\nThe real population, or,\nSome standard population used for comparisons\n\nExample: Ecologic studies comparing incidence of hip fractures across countries\n\nHip fracture rates increase with age\nIndustrialized countries and developing world have very different age distributions\nChoose a standard age distribution to remove confounding by age\n\n\nComment on oversampling\n\nIn political polls or epidemiologic studies we sometimes oversample some strata in order to gain precision\n\nFor fixed maximal sample size, we gain most precision if stratum samples size is proportional to weight times standard deviation of measurements in stratum\nExample: Oversample swing-voters relative to individuals who we can be more certain about their voting preferences\n\nFor independent strata \\(k\\), \\(k = 1, \\ldots, K\\)\n\nSample size in stratum \\(k\\): \\(n_k\\)\nEstimate in stratum \\(k\\): \\(\\hat{\\theta}_k \\sim N\\left(\\theta_k, se^2_k = \\frac{V_k}{n_k} \\right)\\)\nImportance weight for stratum \\(k\\): \\(w_k\\)\nOptimal sample size when \\(N = \\sum_{k=1}^K n_k\\) is:\n\\[\\frac{w_1 \\sqrt{V_1}}{n_1} = \\frac{w_2 \\sqrt{V_2}}{n_2} = \\ldots = \\frac{w_k \\sqrt{V_k}}{n_k}\\]\n\n\n\n\n\n2.3 Weights for Confounders and Precision Variables\n\nIf the true association is the same in each stratum, we are free to consider statistical criteria\n\nIt is very unlikely that there is no effect modification in truth, but is it small enough to ignore?\n\nStatistical criteria\n\nMaximize precision of stratified estimates by minimizing the standard error\n\nOptimal statistical weights\n\nFor independent strata \\(k\\), \\(k = 1, \\ldots, K\\)\n\nSample size in stratum \\(k\\): \\(n_k\\)\nEstimate in stratum \\(k\\): \\(\\hat{\\theta}_k \\sim N\\left(\\theta_k, se^2_k = \\frac{V_k}{n_k} \\right)\\)\nImportance weight for stratum \\(k\\): \\(w_k\\)\nOptimal sample size when \\(N = \\sum_{k=1}^K n_k\\) is:\n\\[\\frac{w_1 \\sqrt{V_1}}{n_1} = \\frac{w_2 \\sqrt{V_2}}{n_2} = \\ldots = \\frac{w_k \\sqrt{V_k}}{n_k}\\]\n\n\nWe often ignore the aspect that variability may differ across strata\n\nSimplifies so that we choose weight by sample size for each stratum\n\nExample: Mantel-Haenszel Statistic\n\nPopular method used to create a common odds ratio estimate across strata\n\nOne way to combine a binary response variable and binary predictor across various strata\n\nHypothesis test comparing odds (proportions) across two groups\n\nAdjust for confounding in a stratified analysis\nWeights chosen for statistical precision\n\nApproximate weighting of difference in proportions based on harmonic means of sample sizes in each stratum\n\nUsually viewed as a weighted odds ratio\n(Why not weight by log odds or probabilities?)\n\nFor independent strata \\(k\\), \\(k = 1, \\ldots, K\\)\n\nSample size in stratum \\(k\\): \\(n_{1k}, n_{0k}\\)\nEstimates in stratum \\(k\\): \\(\\hat{p}_{1k}, \\hat{p}_{2k}\\)\nPrecision weight for stratum \\(k\\):\n\\[w_k = \\frac{n_{1k} n_{0k}}{n_{1k} + n_{0k}} \\div \\sum_{k=1}^K \\frac{n_{1k} n_{0k}}{n_{1k} + n_{0k}}\\]\nThese weights work well in practice, and are not as complicated as some other weighting systems\n\n\nOdds of being full professor by sex\n\n\n. cc full female if year==95, by(field)\n\n           field |       OR       [95% Conf. Interval]   M-H Weight\n-----------------+-------------------------------------------------\n            Arts |   .5384615      .2927119   .9835152     16.54545 (exact)\n           Other |   .2540881      .1870044   .3440217      91.6448 (exact)\n            Prof |   .3434705      .1640076   .7048353     14.42581 (exact)\n-----------------+-------------------------------------------------\n           Crude |    .290421       .226544   .3715365              (exact)\n    M-H combined |   .3029764      .2378934    .385865              \n-------------------------------------------------------------------\nTest of homogeneity (M-H)      chi2(2) =     5.47  Pr&gt;chi2 = 0.0648\n\n                   Test that combined OR = 1:\n                                Mantel-Haenszel chi2(1) =     99.10\n                                                Pr&gt;chi2 =    0.0000\n\nQuestions about output\n\nWhat hypothesis is being tested by the “Test of Homogeneity”?\nShould we use the test of homogeneity to decide if we need to use the M-H adjustment?\nCompare the Crude and M-H combined OR\n\nIs there evidence of confounding?\nWould there be evidence of confounding if the M-H estimate was further from the null (got smaller in this case)?\n\nHow would you determine if field is a precision variable from the output?\n\nR code for similar output as above\n\n\n\nCode\nsalary &lt;- read.csv(file=\"data/salary.csv\")\nsalary.95 &lt;- subset(salary, year==95)\nsalary.95$female &lt;- factor((salary.95$sex==\"F\")+0,levels=0:1, labels=c(\"Male\",\"Female\"))\nsalary.95$full &lt;- factor((salary.95$rank==\"Full\")+0,levels=0:1, labels=c(\"Assist/Assoc\",\"Full\"))\nsalary.95$field &lt;- factor(salary.95$field)\nlibrary(rms)\npartial.tables &lt;- xtabs(~female + full + field, salary.95)\npartial.tables\n\n\n, , field = Arts\n\n        full\nfemale   Assist/Assoc Full\n  Male             70   70\n  Female           52   28\n\n, , field = Other\n\n        full\nfemale   Assist/Assoc Full\n  Male            303  477\n  Female          205   82\n\n, , field = Prof\n\n        full\nfemale   Assist/Assoc Full\n  Male             96  172\n  Female           26   16\n\n\nCode\nmarginal.table &lt;- xtabs(~female + full, salary.95)\nmarginal.table\n\n\n        full\nfemale   Assist/Assoc Full\n  Male            469  719\n  Female          283  126\n\n\nCode\nlibrary(epiR)\nepi.2by2(partial.tables)\n\n\n             Outcome +    Outcome -      Total                 Inc risk *\nExposed +          469          719       1188     39.48 (36.69 to 42.32)\nExposed -          283          126        409     69.19 (64.47 to 73.64)\nTotal              752          845       1597     47.09 (44.62 to 49.57)\n\n\nPoint estimates and 95% CIs:\n-------------------------------------------------------------------\nInc risk ratio (crude)                         0.57 (0.52, 0.63)\nInc risk ratio (M-H)                           0.58 (0.53, 0.64)\nInc risk ratio (crude:M-H)                     0.98\nInc odds ratio (crude)                         0.29 (0.23, 0.37)\nInc odds ratio (M-H)                           0.30 (0.24, 0.39)\nInc odds ratio (crude:M-H)                     0.96\nAttrib risk in the exposed (crude) *           -29.72 (-34.98, -24.45)\nAttrib risk in the exposed (M-H) *             -28.77 (-39.96, -17.59)\nAttrib risk (crude:M-H)                        1.03\n-------------------------------------------------------------------\n M-H test of homogeneity of IRRs: chi2(2) = 6.957 Pr&gt;chi2 = 0.031\n M-H test of homogeneity of ORs: chi2(2) = 5.604 Pr&gt;chi2 = 0.061\n Test that M-H adjusted OR = 1:  chi2(1) = 99.101 Pr&gt;chi2 = &lt;0.001\n Wald confidence limits\n M-H: Mantel-Haenszel; CI: confidence interval\n * Outcomes per 100 population units \n\n\n\nCan compare the M-H results to results obtained running logistic regression\n\n\nUnadjusted OR from logistic regression\n\n\nCode\nm.unadj &lt;- lrm(full ~ female, data=salary.95)\ndd &lt;- datadist(salary.95)\noptions(datadist='dd')\nm.unadj\n\n\nLogistic Regression Model\n \n lrm(formula = full ~ female, data = salary.95)\n \n                        Model Likelihood    Discrimination    Rank Discrim.    \n                              Ratio Test           Indexes          Indexes    \n Obs          1597    LR chi2     109.43    R2       0.088    C       0.614    \n  Assist/Assoc 752    d.f.             1    g        0.471    Dxy     0.227    \n  Full         845    Pr(&gt; chi2) &lt;0.0001    gr       1.602    gamma   0.550    \n max |deriv| 4e-14                          gp       0.113    tau-a   0.113    \n                                            Brier    0.232                     \n \n               Coef    S.E.   Wald Z Pr(&gt;|Z|)\n Intercept      0.4273 0.0594   7.20 &lt;0.0001 \n female=Female -1.2364 0.1224 -10.10 &lt;0.0001 \n \n\n\nCode\nhtml(summary(m.unadj))\n\n\n\n\n\nEffects   Response: full\n\n\n\nLow\nHigh\nΔ\nEffect\nS.E.\nLower 0.95\nUpper 0.95\n\n\n\n\nfemale --- Female:Male\n1\n2\n\n-1.2360\n0.1224\n-1.4760\n-0.9964\n\n\n Odds Ratio\n1\n2\n\n0.2904\n\n0.2285\n0.3692\n\n\n\n\n\nLogistic regression                               Number of obs   =       1597\n                                                  LR chi2(1)      =     109.43\n                                                  Prob &gt; chi2     =     0.0000\nLog likelihood =  -1049.533                       Pseudo R2       =     0.0495\n\n------------------------------------------------------------------------------\n        full | Odds Ratio   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n      female |    .290421    .035561   -10.10   0.000     .2284555    .3691939\n------------------------------------------------------------------------------\n\n\nUnadjusted ORs by field\n\n\nCode\nm.unadj.arts &lt;- lrm(full ~ female, data=salary.95[salary.95$field==\"Arts\",])\nm.unadj.other &lt;- lrm(full ~ female, data=salary.95[salary.95$field==\"Other\",])\nm.unadj.prof &lt;- lrm(full ~ female, data=salary.95[salary.95$field==\"Prof\",])\n\n\n\n\n2.3.1 In Arts Strata\n\n\nCode\nm.unadj.arts\n\n\nLogistic Regression Model\n \n lrm(formula = full ~ female, data = salary.95[salary.95$field == \n     \"Arts\", ])\n \n                       Model Likelihood    Discrimination    Rank Discrim.    \n                             Ratio Test           Indexes          Indexes    \n Obs           220    LR chi2      4.69    R2       0.028    C       0.570    \n  Assist/Assoc 122    d.f.            1    g        0.288    Dxy     0.141    \n  Full          98    Pr(&gt; chi2) 0.0304    gr       1.333    gamma   0.300    \n max |deriv| 2e-09                         gp       0.070    tau-a   0.070    \n                                           Brier    0.242                     \n \n               Coef    S.E.   Wald Z Pr(&gt;|Z|)\n Intercept      0.0000 0.1690  0.00  1.0000  \n female=Female -0.6190 0.2890 -2.14  0.0322  \n \n\n\nCode\nsummary(m.unadj.arts)\n\n\n             Effects              Response : full \n\n Factor               Low High Diff. Effect   S.E.    Lower 0.95 Upper 0.95\n female - Female:Male 1   2    NA    -0.61904 0.28899 -1.18550   -0.052625 \n  Odds Ratio          1   2    NA     0.53846      NA  0.30561    0.948740 \n\n\n\n\n2.3.2 In Other Strata\n\n\nCode\nm.unadj.other\n\n\nLogistic Regression Model\n \n lrm(formula = full ~ female, data = salary.95[salary.95$field == \n     \"Other\", ])\n \n                        Model Likelihood    Discrimination    Rank Discrim.    \n                              Ratio Test           Indexes          Indexes    \n Obs          1067    LR chi2      91.17    R2       0.109    C       0.628    \n  Assist/Assoc 508    d.f.             1    g        0.539    Dxy     0.257    \n  Full         559    Pr(&gt; chi2) &lt;0.0001    gr       1.715    gamma   0.595    \n max |deriv| 2e-13                          gp       0.128    tau-a   0.128    \n                                            Brier    0.229                     \n \n               Coef    S.E.   Wald Z Pr(&gt;|Z|)\n Intercept      0.4538 0.0735  6.18  &lt;0.0001 \n female=Female -1.3701 0.1499 -9.14  &lt;0.0001 \n \n\n\nCode\nhtml(summary(m.unadj.other))\n\n\n\n\n\nEffects   Response: full\n\n\n\nLow\nHigh\nΔ\nEffect\nS.E.\nLower 0.95\nUpper 0.95\n\n\n\n\nfemale --- Female:Male\n1\n2\n\n-1.3700\n0.1499\n-1.6640\n-1.0760\n\n\n Odds Ratio\n1\n2\n\n0.2541\n\n0.1894\n0.3409\n\n\n\n\n\n\n\n2.3.3 In Prof(essional) Strata\n\n\nCode\nm.unadj.prof\n\n\nLogistic Regression Model\n \n lrm(formula = full ~ female, data = salary.95[salary.95$field == \n     \"Prof\", ])\n \n                       Model Likelihood    Discrimination    Rank Discrim.    \n                             Ratio Test           Indexes          Indexes    \n Obs           310    LR chi2     10.10    R2       0.043    C       0.564    \n  Assist/Assoc 122    d.f.            1    g        0.251    Dxy     0.128    \n  Full         188    Pr(&gt; chi2) 0.0015    gr       1.286    gamma   0.489    \n max |deriv| 9e-09                         gp       0.061    tau-a   0.061    \n                                           Brier    0.231                     \n \n               Coef    S.E.   Wald Z Pr(&gt;|Z|)\n Intercept      0.5831 0.1274  4.58  &lt;0.0001 \n female=Female -1.0687 0.3423 -3.12  0.0018  \n \n\n\nCode\nhtml(summary(m.unadj.prof))\n\n\n\n\n\nEffects   Response: full\n\n\n\nLow\nHigh\nΔ\nEffect\nS.E.\nLower 0.95\nUpper 0.95\n\n\n\n\nfemale --- Female:Male\n1\n2\n\n-1.0690\n0.3423\n-1.7400\n-0.3977\n\n\n Odds Ratio\n1\n2\n\n0.3435\n\n0.1756\n0.6719\n\n\n\n\n\n. sort field\n. by field: logistic full female if year==95\n\n----------------------------------------------------------------------------------\n-&gt; field = Arts\n\nLogistic regression                               Number of obs   =        220\n                                                  LR chi2(1)      =       4.69\n                                                  Prob &gt; chi2     =     0.0304\nLog likelihood = -148.83634                       Pseudo R2       =     0.0155\n\n------------------------------------------------------------------------------\n        full | Odds Ratio   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n      female |   .5384615   .1556108    -2.14   0.032      .305608    .9487343\n------------------------------------------------------------------------------\n\n----------------------------------------------------------------------------------\n-&gt; field = Other\n\nLogistic regression                               Number of obs   =       1067\n                                                  LR chi2(1)      =      91.17\n                                                  Prob &gt; chi2     =     0.0000\nLog likelihood = -692.78622                       Pseudo R2       =     0.0617\n\n------------------------------------------------------------------------------\n        full | Odds Ratio   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n      female |   .2540881   .0380877    -9.14   0.000     .1894041    .3408624\n------------------------------------------------------------------------------\n\n----------------------------------------------------------------------------------\n-&gt; field = Prof\n\nLogistic regression                               Number of obs   =        310\n                                                  LR chi2(1)      =      10.10\n                                                  Prob &gt; chi2     =     0.0015\nLog likelihood = -202.74823                       Pseudo R2       =     0.0243\n\n------------------------------------------------------------------------------\n        full | Odds Ratio   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n      female |   .3434705   .1175803    -3.12   0.002      .175589    .6718641\n------------------------------------------------------------------------------\n\n\nOR from multivariable logistic regression controlling for field\n\n\nCode\nm.adj &lt;- lrm(full ~ female + field, data=salary.95, x=TRUE, y=TRUE)\nm.adj\n\n\nLogistic Regression Model\n \n lrm(formula = full ~ female + field, data = salary.95, x = TRUE, \n     y = TRUE)\n \n                        Model Likelihood    Discrimination    Rank Discrim.    \n                              Ratio Test           Indexes          Indexes    \n Obs          1597    LR chi2     114.38    R2       0.092    C       0.632    \n  Assist/Assoc 752    d.f.             3    g        0.542    Dxy     0.264    \n  Full         845    Pr(&gt; chi2) &lt;0.0001    gr       1.719    gamma   0.373    \n max |deriv| 1e-13                          gp       0.130    tau-a   0.132    \n                                            Brier    0.232                     \n \n               Coef    S.E.   Wald Z Pr(&gt;|Z|)\n Intercept      0.1902 0.1461  1.30  0.1930  \n female=Female -1.1998 0.1236 -9.71  &lt;0.0001 \n field=Other    0.2220 0.1547  1.43  0.1513  \n field=Prof     0.4110 0.1856  2.21  0.0268  \n \n\n\nCode\nhtml(summary(m.adj))\n\n\n\n\n\nEffects   Response: full\n\n\n\nLow\nHigh\nΔ\nEffect\nS.E.\nLower 0.95\nUpper 0.95\n\n\n\n\nfemale --- Female:Male\n1\n2\n\n-1.2000\n0.1236\n-1.44200\n-0.95760\n\n\n Odds Ratio\n1\n2\n\n0.3013\n\n0.23650\n0.38380\n\n\nfield --- Arts:Other\n2\n1\n\n-0.2220\n0.1547\n-0.52530\n0.08126\n\n\n Odds Ratio\n2\n1\n\n0.8009\n\n0.59140\n1.08500\n\n\nfield --- Prof:Other\n2\n3\n\n0.1889\n0.1353\n-0.07635\n0.45420\n\n\n Odds Ratio\n2\n3\n\n1.2080\n\n0.92650\n1.57500\n\n\n\n\n\n. xi: logistic full female i.field if year==95\ni.field           _Ifield_1-3         (_Ifield_1 for field==Arts omitted)\n\nLogistic regression                               Number of obs   =       1597\n                                                  LR chi2(3)      =     114.38\n                                                  Prob &gt; chi2     =     0.0000\nLog likelihood = -1047.0583                       Pseudo R2       =     0.0518\n\n------------------------------------------------------------------------------\n        full | Odds Ratio   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n      female |   .3012563   .0372301    -9.71   0.000     .2364515    .3838222\n   _Ifield_2 |   1.248611   .1932157     1.43   0.151     .9219526    1.691009\n   _Ifield_3 |   1.508263   .2798894     2.21   0.027     1.048381    2.169877\n------------------------------------------------------------------------------\n\nM-H OR (.303) differs slightly from the multiple logistic regression OR for (.301).\nConceptually, the are attempting to do the same thing, but are using different weights\nMultivariable logistic regression would allow you to control for continuous covariates without complete stratification\n\n\n\n2.3.4 Logistic regression with robust standard error estimates\n\nWe could also fit the logistic regression models using robust standard error estimate\nWe did not fit a saturated model to exactly predict the probability of being a full professor in all 6 combinations of field and female. So, it is possible that we have a poorly fitting mean model, and because of the mean-variance relationship, our variance estimates could be poor too.\n\n\n\nCode\nrobcov(m.adj)\n\n\nLogistic Regression Model\n \n lrm(formula = full ~ female + field, data = salary.95, x = TRUE, \n     y = TRUE)\n \n                        Model Likelihood    Discrimination    Rank Discrim.    \n                              Ratio Test           Indexes          Indexes    \n Obs          1597    LR chi2     114.38    R2       0.092    C       0.632    \n  Assist/Assoc 752    d.f.             3    g        0.542    Dxy     0.264    \n  Full         845    Pr(&gt; chi2) &lt;0.0001    gr       1.719    gamma   0.373    \n max |deriv| 1e-13                          gp       0.130    tau-a   0.132    \n                                            Brier    0.232                     \n \n               Coef    S.E.   Wald Z Pr(&gt;|Z|)\n Intercept      0.1902 0.1498  1.27  0.2043  \n female=Female -1.1998 0.1236 -9.71  &lt;0.0001 \n field=Other    0.2220 0.1589  1.40  0.1624  \n field=Prof     0.4110 0.1893  2.17  0.0300  \n \n\n\nCode\nhtml(summary(robcov(m.adj)))\n\n\n\n\n\nEffects   Response: full\n\n\n\nLow\nHigh\nΔ\nEffect\nS.E.\nLower 0.95\nUpper 0.95\n\n\n\n\nfemale --- Female:Male\n1\n2\n\n-1.2000\n0.1236\n-1.44200\n-0.95750\n\n\n Odds Ratio\n1\n2\n\n0.3013\n\n0.23640\n0.38380\n\n\nfield --- Arts:Other\n2\n1\n\n-0.2220\n0.1589\n-0.53350\n0.08943\n\n\n Odds Ratio\n2\n1\n\n0.8009\n\n0.58650\n1.09400\n\n\nfield --- Prof:Other\n2\n3\n\n0.1889\n0.1357\n-0.07701\n0.45490\n\n\n Odds Ratio\n2\n3\n\n1.2080\n\n0.92590\n1.57600\n\n\n\n\n\n\nA saturated model would include the interaction of female with field\n\n\n\nCode\nm.saturated &lt;- lrm(full ~ field + female + field*female, data=salary.95)\nm.saturated\n\n\nLogistic Regression Model\n \n lrm(formula = full ~ field + female + field * female, data = salary.95)\n \n                        Model Likelihood    Discrimination    Rank Discrim.    \n                              Ratio Test           Indexes          Indexes    \n Obs          1597    LR chi2     119.75    R2       0.096    C       0.635    \n  Assist/Assoc 752    d.f.             5    g        0.561    Dxy     0.269    \n  Full         845    Pr(&gt; chi2) &lt;0.0001    gr       1.753    gamma   0.379    \n max |deriv| 4e-13                          gp       0.134    tau-a   0.134    \n                                            Brier    0.231                     \n \n                             Coef    S.E.   Wald Z Pr(&gt;|Z|)\n Intercept                    0.0000 0.1690  0.00  1.0000  \n field=Other                  0.4538 0.1843  2.46  0.0138  \n field=Prof                   0.5831 0.2117  2.76  0.0059  \n female=Female               -0.6190 0.2890 -2.14  0.0322  \n field=Other * female=Female -0.7510 0.3256 -2.31  0.0211  \n field=Prof * female=Female  -0.4496 0.4480 -1.00  0.3156  \n \n\n\n\nWe can compare the saturated to the adjusted model using a likelihood ratio test\n\n\n\nCode\nrms::lrtest(m.saturated, m.adj)\n\n\n\nModel 1: full ~ field + female + field * female\nModel 2: full ~ female + field\n\nL.R. Chisq       d.f.          P \n5.37507001 2.00000000 0.06804847 \n\n\n\nWe can also compare the predicted probabilities directly\n\n\n\nCode\n# Predict probability of being a full professor at all 6 combinations of field and salary\nnewdata &lt;- expand.grid(female=levels(salary.95$female),\n                       field=levels(salary.95$field)\n                       )\nnewdata$phat.adj &lt;- predict(m.adj, newdata, type = \"fitted\")\nnewdata$phat.sat &lt;- predict(m.saturated, newdata, type = \"fitted\")\nnewdata\n\n\n  female field  phat.adj  phat.sat\n1   Male  Arts 0.5473986 0.5000000\n2 Female  Arts 0.2670525 0.3500000\n3   Male Other 0.6016146 0.6115385\n4 Female Other 0.3126850 0.2857143\n5   Male  Prof 0.6459134 0.6417910\n6 Female  Prof 0.3546480 0.3809524\n\n\n\nRecall the partial tables of full by female by field given above. Here is the Arts Table. Note that the saturated model matches the observed probability of being a full professor in the Arts group exactly.\n\n\n\nCode\npartial.tables[,,'Arts']\n\n\n        full\nfemale   Assist/Assoc Full\n  Male             70   70\n  Female           52   28\n\n\nCode\n70/(70+70)\n\n\n[1] 0.5\n\n\nCode\n28/(28+52)\n\n\n[1] 0.35\n\n\n\nAlso matches for Other and Professional\n\n\n\nCode\npartial.tables[,,'Other']\n\n\n        full\nfemale   Assist/Assoc Full\n  Male            303  477\n  Female          205   82\n\n\nCode\n477/(477+303)\n\n\n[1] 0.6115385\n\n\nCode\n82/(82+205)\n\n\n[1] 0.2857143\n\n\n\n\nCode\npartial.tables[,,'Prof']\n\n\n        full\nfemale   Assist/Assoc Full\n  Male             96  172\n  Female           26   16\n\n\nCode\n172/(172+96)\n\n\n[1] 0.641791\n\n\nCode\n16/(16+26)\n\n\n[1] 0.3809524"
  },
  {
    "objectID": "notes/Lec07.MultivariableModels.html#multivariable-regression",
    "href": "notes/Lec07.MultivariableModels.html#multivariable-regression",
    "title": "Multivariable Models",
    "section": "3 Multivariable Regression",
    "text": "3 Multivariable Regression\n\n3.1 General Regression Setting\n\nTypes of variables\n\nBinary data: e.g. sex, death\nNominal (unordered categorical) data: e.g. race, martial status\nOrdinal (ordered categorical data): e.g. cancer stage, asthma severity\nQuantitative data: e.g. age, blood pressure\nRight censored data: e.g. time to death\n\nWhich regression model you choose to use is based on the parameter being compared across groups\n\n\n\n\n\nParameter\nApproach\n\n\n\n\nMeans\nLinear regression\n\n\nGeometric means\nLinear regression on log scale\n\n\nOdds\nLogistic regression\n\n\nRates\nPoisson regression\n\n\nHazards\nProportional Hazards (Cox) regression\n\n\n\n\n\nGeneral notation for variables and parameters\n\n\n\n\n\n\n\n\n\n\\(Y_i\\)\nResponse measured on the \\(i\\)th subject\n\n\n\\(X_i\\)\nValue of the predictor measured on the \\(i\\)th subject\n\n\n\\(W_{1i}, W_{2i}, \\ldots\\)\nValue of the adjustment variable for the \\(i\\)th subject\n\n\n\\(\\theta_i\\)\nParameter summarizing distribution of \\(Y_i | X_i\\)\n\n\n\n\n\nThe parameter (\\(\\theta_i\\)) might be the mean, geometric mean, odds, rate, instantaneous risk of an event (hazard), etc.\nIn multiple linear regression on means, \\(\\theta_i = E[Y_i | X_i, W_{1i}, W_{1i}, \\ldots]\\)\nChoice of correct \\(\\theta_i\\) should be based on scientific understanding of problem\nGeneral notation for multiple regression model\n\n\\(g(\\theta_i) = \\beta_0 + \\beta_1 \\times X_i + \\beta_2 \\times W_{1i} + \\beta_3 \\times W_{2i} + \\ldots\\)\n\n\n\n\\(g( )\\)\nLink function used for modeling\n\n\n\n\n\\(\\beta_0\\)\nIntercept\n\n\n\\(\\beta_1\\)\nSlope for predictor of interest \\(X\\)\n\n\n\\(\\beta_j\\)\nSlope for covariate \\(W_{j-1}\\)\n\n\n\nThe link function is often either none (for modeling means) or log (for modeling geometric means, odds, hazards)\n\n\n\n\n3.2 General Uses of Multivariable Regression\n\nBorrowing information\n\nUse other groups to make estimates in groups with sparse data\n\nIntuitively, 67 and 69 year olds would provide some relevant information about 68 year olds\nAssuming a straight line relationship tells us about other, even more distant, individuals\nIf we do not want to assume a straight line, we may only want to borrow information from nearby groups\n\n\nDefining “Contrasts”\n\nDefine a comparison across groups to use when answering scientific questions\nIf the straight line relationship holds, the slope for the POI is the difference in parameter between groups differing by 1 unit in \\(X\\) when all other covariates are held constant\nIf a non-linear relationship in parameter, the slope is still the average difference in parameter between groups differing by 1 unit in \\(X\\) when all other covariates are held constant\n\nSlope is a (first order or linear) test for trend in the parameter\nStatistical jargon: “a contrast” across groups\n\n\nThe major difference between different regression models is the interpretation of the parameters\n\nHow do I want to summarize the outcome?\n\nMean, geometric mean, odds, hazard\n\nHow do I want to compare groups?\n\nDifference, ratio\n\n\nIssues related to the inclusion of covariates remains the same\n\nAddress the scientific question: Predictor of interest, effect modification\nAddress confounding\nIncrease precision\n\nInterpretation of parameters\n\nIntercept\n\nCorresponds to a population with all modeled covariates equal to zero\n\nQuite often, this will be outside the range of the data so that the intercept has no meaningful interpretation by itself\n\n\nSlope\n\nA comparison between groups differing by 1 unit in corresponding covariate, but agreeing on all other modeled covariates\n\nSometimes impossible to use this interpretation when modeling interactions or complex dose-response curves (e.g. a model with age and age-squared)\n\n\n\nStratification versus regression\n\nGenerally, any stratified analysis could be performed as a regression model\n\nStratification adjusts for covariates and all interactions among those covariates\nOur habit in regression is to just adjust for covariates as main effects, and consider interactions less often\n\n\n\n\n\n3.3 Software\n\nIn Stata or R, we use the same commands as were used for simple regression models\n\nWe just list more variable names\nInterpretations of the CIs, p-values for coefficients estimates now relate to new scientific interpretations of intercept and slopes\nTest of entire regression model also provided (a test that all slopes are equal to zero)\n\n\n\n\n3.4 Example: FEV and Smoking\n\nScientific question: Is the maximum forced expiatory volume (FEV) related to smoking status in children?\nAge ranges from 3 to 19, but no child under 9 smokes in the sample\nModels we will compare\n\nUnadjusted (simple) model: FEV and smoking\nAdjusted for age: FEV and smoking with age (confounder)\nAdjusted for age, height: FEV and smoking with age (confounder) and height (precision)\n\n\n\n\nCode\nfev &lt;- read.csv(file=\"data/FEV.csv\")\n\n# Restrict to ages 9 and above\nfev &lt;- fev[fev$age&gt;=9,]\n\nfev$logfev &lt;- log(fev$fev)\nfev$loght &lt;- log(fev$height)\nfev$smoker &lt;- (fev$smoke==\"current smoker\") + 0\n\n#table(fev$smoker, fev$smoke)\n\nlibrary(rms)\nm.unadj &lt;- ols(logfev ~ smoker, data=fev, x=TRUE, y=TRUE)\nm.adj1 &lt;- ols(logfev ~ smoker + age, data=fev, x=TRUE, y=TRUE)\nm.adj2 &lt;- ols(logfev ~ smoker + age + loght, data=fev, x=TRUE, y=TRUE)\n\noptions(print='html')\nprint(robcov(m.unadj))\n\n\nLinear Regression Model\n \n ols(formula = logfev ~ smoker, data = fev, x = TRUE, y = TRUE)\n \n                 Model Likelihood    Discrimination    \n                       Ratio Test           Indexes    \n Obs     439    LR chi2      9.39    R2       0.021    \n sigma0.2477    d.f.            1    R2 adj   0.019    \n d.f.    437    Pr(&gt; chi2) 0.0022    g        0.026    \n \n Residuals\n \n       Min        1Q    Median        3Q       Max \n -0.681105 -0.159026 -0.005253  0.155787  0.698480 \n \n \n           Coef   S.E.   t     Pr(&gt;|t|)\n Intercept 1.0582 0.0129 82.00 &lt;0.0001 \n smoker    0.1023 0.0316  3.24 0.0013  \n \n\n\nCode\nprint(robcov(m.adj1))\n\n\nLinear Regression Model\n \n ols(formula = logfev ~ smoker + age, data = fev, x = TRUE, y = TRUE)\n \n                 Model Likelihood    Discrimination    \n                       Ratio Test           Indexes    \n Obs     439    LR chi2    157.34    R2       0.301    \n sigma0.2095    d.f.            2    R2 adj   0.298    \n d.f.    436    Pr(&gt; chi2) 0.0000    g        0.148    \n \n Residuals\n \n       Min        1Q    Median        3Q       Max \n -0.610707 -0.150642  0.002797  0.146192  0.538299 \n \n \n           Coef    S.E.   t     Pr(&gt;|t|)\n Intercept  0.3518 0.0573  6.14 &lt;0.0001 \n smoker    -0.0513 0.0343 -1.50 0.1347  \n age        0.0636 0.0051 12.42 &lt;0.0001 \n \n\n\nCode\nprint(robcov(m.adj2))\n\n\nLinear Regression Model\n \n ols(formula = logfev ~ smoker + age + loght, data = fev, x = TRUE, \n     y = TRUE)\n \n                 Model Likelihood    Discrimination    \n                       Ratio Test           Indexes    \n Obs     439    LR chi2    487.04    R2       0.670    \n sigma0.1441    d.f.            3    R2 adj   0.668    \n d.f.    435    Pr(&gt; chi2) 0.0000    g        0.233    \n \n Residuals\n \n      Min       1Q   Median       3Q      Max \n -0.50786 -0.08993  0.01061  0.09559  0.37788 \n \n \n           Coef     S.E.   t      Pr(&gt;|t|)\n Intercept -11.0946 0.5130 -21.63 &lt;0.0001 \n smoker     -0.0536 0.0241  -2.23 0.0265  \n age         0.0215 0.0035   6.21 &lt;0.0001 \n loght       2.8697 0.1274  22.52 &lt;0.0001 \n \n\n\n\n. regress logfev smoker if age&gt;=9, robust\n\nLinear regression                                      Number of obs =     439\n                                                       F(  1,   437) =   10.45\n                                                       Prob &gt; F      =  0.0013\n                                                       R-squared     =  0.0212\n                                                       Root MSE      =  .24765\n\n------------------------------------------------------------------------------\n             |               Robust\n      logfev |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n      smoker |   .1023056   .0316539     3.23   0.001     .0400927    .1645184\n       _cons |    1.05817   .0129335    81.82   0.000     1.032751     1.08359\n------------------------------------------------------------------------------\n\n. \n. regress logfev smoker age if age&gt;=9, robust\n\nLinear regression                                      Number of obs =     439\n                                                       F(  2,   436) =   82.28\n                                                       Prob &gt; F      =  0.0000\n                                                       R-squared     =  0.3012\n                                                       Root MSE      =  .20949\n\n------------------------------------------------------------------------------\n             |               Robust\n      logfev |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n      smoker |  -.0513495   .0343822    -1.49   0.136     -.118925     .016226\n         age |   .0635957   .0051401    12.37   0.000     .0534932    .0736981\n       _cons |   .3518165   .0575011     6.12   0.000     .2388028    .4648303\n------------------------------------------------------------------------------\n\n. \n. regress logfev smoker age loght if age &gt;=9, robust\n\nLinear regression                                      Number of obs =     439\n                                                       F(  3,   435) =  284.22\n                                                       Prob &gt; F      =  0.0000\n                                                       R-squared     =  0.6703\n                                                       Root MSE      =  .14407\n\n------------------------------------------------------------------------------\n             |               Robust\n      logfev |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n      smoker |  -.0535896   .0241879    -2.22   0.027    -.1011293     -.00605\n         age |   .0215295   .0034817     6.18   0.000     .0146864    .0283725\n       loght |   2.869658   .1279943    22.42   0.000     2.618093    3.121222\n       _cons |  -11.09461   .5153323   -21.53   0.000    -12.10746   -10.08176\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "notes/Lec07.MultivariableModels.html#unadjusted-versus-adjusted-models",
    "href": "notes/Lec07.MultivariableModels.html#unadjusted-versus-adjusted-models",
    "title": "Multivariable Models",
    "section": "4 Unadjusted versus Adjusted Models",
    "text": "4 Unadjusted versus Adjusted Models\n\n4.1 General comparison\n\nAdjusting for covariates changes the scientific question\n\nUnadjusted models: Slope compares parameters across groups differing by 1 unit in the modeled predictor\n\nGroups may also differ with respect to other variables\n\nAdjusted models: Slope compares parameters across groups differing by 1 unit in the modeled predictor but similar with respect to other model covariates\n\nInterpretation of Slopes\n\nUnadjusted model: \\(g(\\theta | X_i) = \\beta_0 + \\beta_1 \\times X_i\\)\n\n\\(\\beta_1\\): Compares \\(\\theta\\) for groups differing by 1 unit in \\(X\\)\n\n(The distribution of \\(W\\) might differ across groups being compared)\n\n\nAdjusted model: \\(g(\\theta | X_i, W_i) = \\gamma_0 + \\gamma_1 \\times X_i + \\gamma_2 \\times W_i\\)\n\n\\(\\gamma_1\\): Compares \\(\\theta\\) for groups differing by 1 unit in \\(X\\), but agreeing on their values of \\(W\\)\n\n\nComparing unadjusted and adjusted models\n\nScience questions\n\nWhen does \\(\\gamma_1 = \\beta_1\\)?\nWhen does \\(\\hat{\\gamma}_1 = \\hat{\\beta}_1\\)?\n\nStatistics questions\n\nWhen does \\(se(\\hat{\\gamma_1}) = se(\\hat{\\beta_1})\\)?\nWhen does \\(\\hat{se}(\\hat{\\gamma_1}) = \\hat{se}(\\hat{\\beta_1})\\)\n\nIn above, note placement of the \\(\\hat{ }\\) (“hat”) which signifies estimates of population parameters\nLack of a hat indicates “the truth” in the population\nWhen \\(\\hat{\\gamma}_1 = \\hat{\\beta}_1\\) (the formulas are the same), then it must be the case that \\(se(\\hat{\\gamma_1}) = se(\\hat{\\beta_1})\\)\n\nBut our estimates of the standard errors may not be the same\n\nExample of when \\(\\gamma_1 = \\beta_1\\)\n\nWant to compare smokers to non-smokers (POI) with respect to their FEV (response) and have conducted a randomized experiment in which boys and girls (\\(W\\)) are equally represented as smokers and non-smokers\nWhen we compare a random smoker to a random non-smoker, that average difference will be the same if we adjust for gender or not\nNumbers in unadjusted and adjusted analyses are the same, but interpretation is different\n\n\nAnswering these four questions cannot be done in the general case\n\nHowever, in linear regression we can derive exact results\nThese exact results can serve as a basis for examination of other regression models\n\nLogistic regression\nPoisson regression\nProportional hazards regression\n\n\n\n\n\n4.2 Comparison of Adjusted and Unadjusted Models in Linear Regression\n\n4.2.1 Interpretation of Slopes\n\nUnadjusted model: \\(E[Y_i | X_i] = \\beta_0 + \\beta_1 \\times X_i\\)\n\n\\(\\beta_1\\): Difference in mean \\(Y\\) for groups differing by 1 unit in \\(X\\)\n\n(The distribution of \\(W\\) might differ across groups being compared)\n\n\nAdjusted model: \\(E[Y_i | X_i, W_i] = \\gamma_0 + \\gamma_1 \\times X_i + \\gamma_2 \\times W_i\\)\n\n\\(\\gamma_1\\): Difference in mean \\(Y\\) for groups differing by 1 unit in \\(X\\), but agreeing in their values of \\(W\\)\n\n\n\n\n4.2.2 Relationships: True Slopes\n\nThe slope of the unadjusted model will tend to be\n\\[\\beta_1 = \\gamma_1 + \\rho_{XW} \\frac{\\sigma_W}{\\sigma_X} \\gamma_2\\]\nHence, true adjusted and unadjusted slopes for \\(X\\) are estimating the same quantity on if\n\n\\(\\rho_{XW} = 0\\) (\\(X\\) and \\(W\\) are truly uncorrelated), OR\n\\(\\gamma_2 = 0\\) (no association between \\(W\\) and \\(Y\\) after adjusting for \\(X\\))\n\n\n\n\n4.2.3 Relationships: Estimated Slopes\n\nThe estimated slope of the unadjusted model will be\n\\[\\hat{\\beta}_1 = \\hat{\\gamma}_1\\left(1 + \\hat{\\gamma}_2 r_{XW} \\left[\\frac{s_W}{s_X(r_{YX} - r_{YW} r_{XW}} \\right] \\right)\\]\nHence, estimated adjusted and unadjusted slopes for \\(X\\) are equal only if\n\n\\(r_{XW} = 0\\) (\\(X\\) and \\(W\\) are uncorrelated in the sample, which can be arranged by experimental design, OR\n\\(\\hat{\\gamma}_2 = 0\\) (which cannot be predetermined because \\(Y\\) is random)\n\n\n\n\n4.2.4 Relationships: True SE\n\nUnadjusted model: \\([se(\\hat{\\beta}_1)]^2 = \\frac{Var(Y|X)}{n Var(X)}\\)\nAdjusted model: \\([se(\\hat{\\gamma}_1)]^2 = \\frac{Var(Y|X, W)}{n Var(X)(1 - r^2_{XW})}\\)\n\\[\\begin{aligned}\n    Var(Y|X) & = & \\gamma_2^2 Var(W|X) + Var(Y | X, W) \\\\\n    \\sigma^2_{Y|X} & = & \\gamma_2^2 \\sigma^2_{W|X} + \\sigma^2_{Y|X,W}\n   \\end{aligned}\\]\nHence, \\(se(\\hat{\\beta}_1) = se(\\hat{\\gamma}_1)\\) if,\n\n\\(r_{XW} = 0\\) AND\n\\(\\gamma_2 = 0\\) OR \\(Var(W|X) = 0\\)\n\n\n\n\n4.2.5 Relationships: Estimated SE\n\nUnadjusted model: \\([\\hat{se}(\\hat{\\beta}_1)]^2 = \\frac{SSE(Y|X)/(n-2)}{(n-1) s^2_X}\\)\nAdjusted model: \\([\\hat{se}(\\hat{\\gamma}_1)]^2 = \\frac{SSE(Y|X, W)/(n-3)}{(n-1) s^2_X (1 - r^2_{XW})}\\)\n\\[\\begin{aligned}\n    SSE(Y|X) & = & \\sum(Y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 \\times X_i)^2 \\\\\n    SSE(Y|X, W) & = & \\sum(Y_i - \\hat{\\gamma}_0 - \\hat{\\gamma}_1 \\times X_i - \\hat{\\gamma}_2 \\times W_i)^2\n   \\end{aligned}\\]\nHence, \\(\\hat{se}(\\hat{\\beta}_1) = \\hat{se}(\\hat{\\gamma}_1)\\) if,\n\n\\(r_{XW} = 0\\) AND\n\\(SSE(Y|X)/(n-2) = SSE(Y|X,W)/(n-3)\\)\n\nNote than when calculated on the same data,\n\n\\(SSE(Y|X) \\geq SSE(Y | X, W)\\)\n\nNow \\(\\hat{\\beta}_1 = \\hat{\\gamma}_1\\) if\n\n\\(\\hat{\\gamma}_2 = 0\\), in which case \\(SSE(Y | X) = SSE(Y | X, W)\\), OR\n\\(r_{XW} = 0\\), and \\(SSE(Y | X) &gt; SSE(Y | X, W)\\) if \\(\\hat{\\gamma}_2 \\neq 0\\)\n\n\n\n\n4.2.6 Summary: Special Cases\n\nWe are interested in knowing the behavior of unadjusted and adjusted models according to whether\n\n\\(X\\) and \\(W\\) are uncorrelated\n\\(W\\) is associated with \\(Y\\) after adjustment for \\(X\\)\n\nThe 4 key cases are summarized below\n\n\n\n\n\\(r_{XW} = 0\\)\n\\(r_{XW} \\neq 0\\)\n\n\n\n\n\\(\\gamma_2 \\neq 0\\)\nPrecision\nConfounding\n\n\n\\(\\gamma_2 = 0\\)\nIrrelevant\nVariance Inflation\n\n\n\nWhen \\(X\\) is associated with \\(W\\), and \\(W\\) is associated with \\(Y\\) after we control for \\(X\\), that is what we call confounding\nWhen \\(X\\) is associated with \\(W\\), and \\(W\\) is not associated with \\(Y\\) after we control for \\(X\\), this inflates the variance of the association between \\(X\\) and \\(Y\\) (more on this follows)\nWhen \\(X\\) is not associated with \\(W\\), and \\(W\\) is associated with \\(Y\\) after we control for \\(X\\), this increases the precision of our estimate of the association between \\(X\\) and \\(Y\\)\nWhen \\(X\\) is not associated with \\(W\\), and \\(W\\) is not associated with \\(Y\\) after we control for \\(X\\), there is no reason to be concerned with modeling \\(W\\)\n\n\n\n\n4.3 Precision variables\n\n4.3.1 Precision in Linear Regression\n\nAdjusting for a true precision variable should not impact the point estimate of the association between the POI and the response, but will decrease the standard error\n\\(X, W\\) independent in the population (or a completely randomized experiment) AND \\(W\\) is associated with \\(Y\\) independent of \\(X\\)\n\n\\(\\rho_{XW} = 0\\)\n\\(\\gamma_2 \\neq 0\\)\n\n\n\n\n\n\n\n\n\nTrue Value\nEstimates\n\n\n\n\nSlopes\n\\(\\beta_1 = \\gamma_1\\)\n\\(\\hat{\\beta}_1 \\approx \\hat{\\gamma}_1\\)\n\n\nStd Errors\n\\(se(\\hat{\\beta}_1) &gt; se(\\hat{\\gamma}_1)\\)\n\\(\\hat{se}(\\hat{\\beta}_1) &gt; \\hat{se}(\\hat{\\gamma}_1)\\)\n\n\n\n\n\n\n\n4.3.2 Precision in Logistic Regression\n\nCan no longer use the formulas for linear regression\nAdjusting for a precision variable\n\nDeattenuates slope away from the null\nStandard errors reflect the mean-variance relationship\n\n\n\n\n\n\n\n\n\nTrue Value\nEstimates\n\n\n\n\nSlopes if \\(\\beta_1 &gt; 0\\)\n\\(\\beta_1 &lt; \\gamma_1\\)\n\\(\\hat{\\beta}_1 &lt; \\hat{\\gamma}_1\\)\n\n\nSlopes if \\(\\beta_1 &lt; 0\\)\n\\(\\beta_1 &gt; \\gamma_1\\)\n\\(\\hat{\\beta}_1 &gt; \\hat{\\gamma}_1\\)\n\n\nStd Errors\n\\(se(\\hat{\\beta}_1) &lt; se(\\hat{\\gamma}_1)\\)\n\\(\\hat{se}(\\hat{\\beta}_1) &lt; \\hat{se}(\\hat{\\gamma}_1)\\)\n\n\n\n\nNote that the standard errors will be smaller in the unadjusted model due to the mean-variance relationship\n\nProportions have minimum variance when \\(p\\) gets close to 0 or 1, and maximum variance at \\(p = 0.5\\)\nOdds have minimum variance when \\(p\\) is \\(0.5\\)\nPrecision variables should be driving probabilities toward 0 or 1\n\n\n\n\n4.3.3 Precision in Poisson Regression\n\nAdjusting for a precision variable will\n\nHave no effect on the slope (log ratios are linear in log means)\nStandard errors reflect the mean-variance relationship (virtually no effect on power)\n\n\n\n\n\n\n\n\n\nTrue Value\nEstimates\n\n\n\n\nSlopes\n\\(\\beta_1 = \\gamma_1\\)\n\\(\\hat{\\beta}_1 \\approx \\hat{\\gamma}_1\\)\n\n\nStd Errors\n\\(se(\\hat{\\beta}_1) \\approx se(\\hat{\\gamma}_1)\\)\n\\(\\hat{se}(\\hat{\\beta}_1) \\approx \\hat{se}(\\hat{\\gamma}_1)\\)\n\n\n\n\n\n\n\n4.3.4 Precision in PH Regression\n\nAdjusting for a precision variable\n\nDeattenuates slope away from the null\nStandard errors stay fairly constant (complicated result of binomial mean-variance relationship)\n\n\n\n\n\n\n\n\n\nTrue Value\nEstimates\n\n\n\n\nSlopes if \\(\\beta_1 &gt; 0\\)\n\\(\\beta_1 &lt; \\gamma_1\\)\n\\(\\hat{\\beta}_1 &lt; \\hat{\\gamma}_1\\)\n\n\nSlopes if \\(\\beta_1 &lt; 0\\)\n\\(\\beta_1 &gt; \\gamma_1\\)\n\\(\\hat{\\beta}_1 &gt; \\hat{\\gamma}_1\\)\n\n\nStd Errors\n\\(se(\\hat{\\beta}_1) \\approx se(\\hat{\\gamma}_1)\\)\n\\(\\hat{se}(\\hat{\\beta}_1) \\approx \\hat{se}(\\hat{\\gamma}_1)\\)\n\n\n\n\nWill get some gain in power due to deattenuation of \\(\\beta_1\\) while standard errors remain similar\nHowever, it is rare to have PH assumption hold for both adjusted and unadjusted models\n\n\n\n4.3.5 Stratified Randomization in Linear Regression\n\nStratified randomization in a designed experiment\n\n\\(r_{XW} = 0\\)\n\\(\\gamma_2 \\neq 0\\)\n\n\n\n\n\n\n\n\n\nTrue Value\nEstimates\n\n\n\n\nSlopes\n\\(\\beta_1 = \\gamma_1\\)\n\\(\\hat{\\beta}_1 = \\hat{\\gamma}_1\\)\n\n\nStd Errors\n\\(se(\\hat{\\beta}_1) = se(\\hat{\\gamma}_1)\\)\n\\(\\hat{se}(\\hat{\\beta}_1) &gt; \\hat{se}(\\hat{\\gamma}_1)\\)\n\n\n\n\nNeed to adjust for the blocking variable in regression analysis to get improved standard error\n\n\n\n\n4.4 Confounding variables\n\n4.4.1 Confounding in Linear Regression\n\nCausally associated with the response and associated with POI in the sample\n\n\\(r_{XW} \\neq 0\\)\n\\(\\gamma_2 \\neq 0\\)\n\n\n\n\n\n\n\n\n\nTrue Value\nEstimates\n\n\n\n\nSlopes\n\\(\\beta_1 = \\gamma_1 + \\rho_{XW} \\frac{\\sigma_W}{\\sigma_X} \\gamma_2\\)\n\\(\\hat{\\beta}_1 = \\hat{\\gamma}_1\\left(1 + \\hat{\\gamma}_2 r_{XW} \\left[\\frac{s_W}{s_X(r_{YX} - r_{YW} r_{XW}} \\right] \\right)\\)\n\n\nStd Errors\n\\(se(\\hat{\\beta}_1) \\{&gt;, =, or &lt;\\} se(\\hat{\\gamma}_1)\\)\n\\(\\hat{se}(\\hat{\\beta}_1) \\{&gt;, =, or &lt;\\} \\hat{se}(\\hat{\\gamma}_1)\\)\n\n\n\n\nSlopes could increase or decrease\nStandard errors could increase, decrease, or stay the same\n\nCompetition of greater precision with variance inflation\n\n\n\n\n4.4.2 Confounding in Other Regressions\n\nWith logistic, Poisson, PH regression we cannot write down formula, but\n\nAs in linear regression, anything can happen\n\n\n\n\n\n\n\n\n\nTrue Value\nEstimates\n\n\n\n\nSlopes\n\\(\\beta_1 \\{&gt;, =, or &lt;\\} \\gamma_1\\)\n\\(\\hat{\\beta}_1 \\{&gt;, =, or &lt;\\} \\hat{\\gamma}_1\\)\n\n\nStd Errors\n\\(se(\\hat{\\beta}_1) \\{&gt;, =, or &lt;\\} se(\\hat{\\gamma}_1)\\)\n\\(\\hat{se}(\\hat{\\beta}_1) \\{&gt;, =, or &lt;\\} \\hat{se}(\\hat{\\gamma}_1)\\)\n\n\n\n\nSlopes could increase or decrease\nStandard errors could increase, decrease, or stay the same\n\n\n\n\n4.5 Variance Inflation\n\n4.5.1 Variance Inflation in Linear Regression\n\nAssociated with POI in the sample, but not associated with response\n\n\\(r_{XW} \\neq 0\\)\n\\(\\gamma_2 = 0\\)\n\n\n\n\n\n\n\n\n\nTrue Value\nEstimates\n\n\n\n\nSlopes\n\\(\\beta_1 = \\gamma_1\\)\n\\(\\hat{\\beta}_1 = \\hat{\\gamma}_1\\left(1 + \\hat{\\gamma}_2 r_{XW} \\left[\\frac{s_W}{s_X(r_{YX} - r_{YW} r_{XW}} \\right] \\right)\\)\n\n\nStd Errors\n\\(se(\\hat{\\beta}_1) &lt; se(\\hat{\\gamma}_1)\\)\n\\(\\hat{se}(\\hat{\\beta}_1) &lt; \\hat{se}(\\hat{\\gamma}_1)\\)\n\n\n\n\n\n\n\n4.5.2 Variance Inflation in Other Regressions\n\nWith logistic, Poisson, PH regression we cannot write down formula, but\n\nSimilar to linear regression\n\n\n\n\n\n\n\n\n\nTrue Value\nEstimates\n\n\n\n\nSlopes\n\\(\\beta_1 = \\gamma_1\\)\n\\(\\hat{\\beta}_1 \\{&gt;, =, or &lt;\\} \\hat{\\gamma}_1\\)\n\n\nStd Errors\n\\(se(\\hat{\\beta}_1) &lt; se(\\hat{\\gamma}_1)\\)\n\\(\\hat{se}(\\hat{\\beta}_1) &lt; \\hat{se}(\\hat{\\gamma}_1)\\)\n\n\n\n\n\n\n\n\n4.6 Irrelevant variables\n\nUncorrelated with POI in sample, and not associated with response\n\n\\(r_{XW} = 0\\)\n\\(\\gamma_2 = 0\\)\n\nInclusion of irrelevant variables results in slight loss in precision in all regressions\n\n\n\n\n\n\n\n\n\nTrue Value\nEstimates\n\n\n\n\nSlopes\n\\(\\beta_1 = \\gamma_1\\)\n\\(\\hat{\\beta}_1= \\hat{\\gamma}_1\\)\n\n\nStd Errors\n\\(se(\\hat{\\beta}_1) = se(\\hat{\\gamma}_1)\\)\n\\(\\hat{se}(\\hat{\\beta}_1) &lt; \\hat{se}(\\hat{\\gamma}_1)\\)"
  },
  {
    "objectID": "notes/Lec07.MultivariableModels.html#example-fev-and-smoking-in-children",
    "href": "notes/Lec07.MultivariableModels.html#example-fev-and-smoking-in-children",
    "title": "Multivariable Models",
    "section": "5 Example: FEV and Smoking in Children",
    "text": "5 Example: FEV and Smoking in Children\n\n5.1 Linear model for geometric means\n\nAssociation between lung function and self-reported smoking in children\nCompare geometric means of FEV of children who smoke to comparable non-smokers\nRestrict analysis to children 9 years and older\n\nNo smokers less than 9\nStill about 6 : 1 ratio of non-smokers to smokers\n\nLittle precision gained by keeping younger children\nBorrowing information from young kids problematic if not a linear relationship between log(FEV) and predictors\nWith confounding, want to get the model correct\n\n\nAcademic Exercise: Compare alternative models\n\nIn real life, we should choose a single model in advance of looking at the data\nHere we will observe what happens to parameter estimates and SE across models\n\nSmoking\nSmoking adjusted for age\nSmoking adjusted for age and height\n\n\n\n\n5.1.1 Unadjusted Model\n\n\nCode\nprint(robcov(m.unadj))\n\n\nLinear Regression Model\n \n ols(formula = logfev ~ smoker, data = fev, x = TRUE, y = TRUE)\n \n                 Model Likelihood    Discrimination    \n                       Ratio Test           Indexes    \n Obs     439    LR chi2      9.39    R2       0.021    \n sigma0.2477    d.f.            1    R2 adj   0.019    \n d.f.    437    Pr(&gt; chi2) 0.0022    g        0.026    \n \n Residuals\n \n       Min        1Q    Median        3Q       Max \n -0.681105 -0.159026 -0.005253  0.155787  0.698480 \n \n \n           Coef   S.E.   t     Pr(&gt;|t|)\n Intercept 1.0582 0.0129 82.00 &lt;0.0001 \n smoker    0.1023 0.0316  3.24 0.0013  \n \n\n\n. regress logfev smoker if age&gt;=9, robust\n\nLinear regression                                      Number of obs =     439\n                                                       F(  1,   437) =   10.45\n                                                       Prob &gt; F      =  0.0013\n                                                       R-squared     =  0.0212\n                                                       Root MSE      =  .24765\n\n------------------------------------------------------------------------------\n             |               Robust\n      logfev |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n      smoker |   .1023056   .0316539     3.23   0.001     .0400927    .1645184\n       _cons |    1.05817   .0129335    81.82   0.000     1.032751     1.08359\n------------------------------------------------------------------------------\n\nIntercept\n\nGeometric mean of FEV in nonsmokers is 2.88 l/sec\n\nThe scientific relevance is questionable here because we do not really know the population our sample represents\nCalculations \\(e^{1.05817} = 2.88\\)\nThe p-value is completely unimportant here as it is testing that the log geometric mean is 0, or that the geometric mean is 1. Why would we care?\n\nBecause smoker is a binary variable, the estimate corresponds to a geometric mean. In many regression models, the intercept will have not interpretation\n\nSmoking effect\n\nGeometric mean of FEV is \\(10.8\\%\\) higher in smokers than in nonsmokers (95% CI: \\(4.1\\%\\) to \\(17.9\\%\\) higher)\n\nThese results are atypical of what we might expect with no true difference between groups (\\(p = 0.001\\))\nCalculations: \\(e^{.102} = 1.108; e^{.040} = 1.041; e^{0.165} = 1.179\\)\n(Note that \\(e^x\\) is approximately \\((1+x)\\) for \\(x\\) close to 1)\n\nBecause smoker is a binary variable, this analysis is nearly identical to a two sample t-test allowing for unequal variances\n\n\n\n\n5.1.2 Adjusted for Age\n\n\nCode\nprint(robcov(m.adj1))\n\n\nLinear Regression Model\n \n ols(formula = logfev ~ smoker + age, data = fev, x = TRUE, y = TRUE)\n \n                 Model Likelihood    Discrimination    \n                       Ratio Test           Indexes    \n Obs     439    LR chi2    157.34    R2       0.301    \n sigma0.2095    d.f.            2    R2 adj   0.298    \n d.f.    436    Pr(&gt; chi2) 0.0000    g        0.148    \n \n Residuals\n \n       Min        1Q    Median        3Q       Max \n -0.610707 -0.150642  0.002797  0.146192  0.538299 \n \n \n           Coef    S.E.   t     Pr(&gt;|t|)\n Intercept  0.3518 0.0573  6.14 &lt;0.0001 \n smoker    -0.0513 0.0343 -1.50 0.1347  \n age        0.0636 0.0051 12.42 &lt;0.0001 \n \n\n\n. regress logfev smoker age if age&gt;=9, robust\n\nLinear regression                                      Number of obs =     439\n                                                       F(  2,   436) =   82.28\n                                                       Prob &gt; F      =  0.0000\n                                                       R-squared     =  0.3012\n                                                       Root MSE      =  .20949\n\n------------------------------------------------------------------------------\n             |               Robust\n      logfev |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n      smoker |  -.0513495   .0343822    -1.49   0.136     -.118925     .016226\n         age |   .0635957   .0051401    12.37   0.000     .0534932    .0736981\n       _cons |   .3518165   .0575011     6.12   0.000     .2388028    .4648303\n------------------------------------------------------------------------------\n\nIntercept\n\nGeometric mean of FEV in newborn nonsmokers is 1.42 l/sec\n\nIntercept corresponds to the log geometric mean in a group having all predictors equal to \\(0\\)\nThere is no scientific relevance here as we are extrapolating beyond the range of our data\nCalculations \\(e^{0.352} = 1.422\\)\n\n\nAge effect\n\nGeometric mean of FEV is \\(6.6\\%\\) higher for each year difference in age between two groups with the same smoking status (95% CI: \\(5.5\\%\\) to \\(7.6\\%\\) higher)\n\nThese results are highly atypical of what we might expect with no true difference in geometric means between age groups having similar smoking status (\\(p &lt; 0.001\\))\n\n\nSmoking effect (age adjusted interpretation)\n\nGeometric mean of FEV is \\(5.0\\%\\) lower in smokers than in nonsmokers of the same age (95% CI: \\(12.2\\%\\) lower to \\(1.6\\%\\) higher)\n\nThese results are not atypical of what we might expect with no true difference between groups of the same age (\\(p = 0.136\\))\nLack of statistical significance can also be noted by the fact that the CI for the ratio contain 1 or the CI for the percent difference contains 0\nCalculations: \\(e^{-.051} = 0.950; e^{-.119} = 0.888; e^{0.016} = 1.016\\)\n\n\nComparing unadjusted and age adjusted analyses\n\nMarked differences in effect of smoking suggests that there was indeed confounding\n\nAge is a relatively strong predictor of FEV\nAge is associated with smoking in the sample\n\nMean (SD) of age in analyzed smokers: 11.1 (2.04)\nMean (SD) of age in analyzed nonsmokers: 13.5 (2.34)\n\n\nEffect of age adjustment on precision\n\nLower Root MSE (0.209 vs 0.248) would tend to increase precision of estimate of smoking effect\nAssociation between smoking and age tends to lower precision\nNet effect: Less precision (adj SE 0.034 vs unadj SE 0.031)\n\n\n\n\n\n5.1.3 Adjusted for Age and Height\n\n\nCode\nprint(robcov(m.adj2))\n\n\nLinear Regression Model\n \n ols(formula = logfev ~ smoker + age + loght, data = fev, x = TRUE, \n     y = TRUE)\n \n                 Model Likelihood    Discrimination    \n                       Ratio Test           Indexes    \n Obs     439    LR chi2    487.04    R2       0.670    \n sigma0.1441    d.f.            3    R2 adj   0.668    \n d.f.    435    Pr(&gt; chi2) 0.0000    g        0.233    \n \n Residuals\n \n      Min       1Q   Median       3Q      Max \n -0.50786 -0.08993  0.01061  0.09559  0.37788 \n \n \n           Coef     S.E.   t      Pr(&gt;|t|)\n Intercept -11.0946 0.5130 -21.63 &lt;0.0001 \n smoker     -0.0536 0.0241  -2.23 0.0265  \n age         0.0215 0.0035   6.21 &lt;0.0001 \n loght       2.8697 0.1274  22.52 &lt;0.0001 \n \n\n\n. regress logfev smoker age loght if age &gt;=9, robust\n\nLinear regression                                      Number of obs =     439\n                                                       F(  3,   435) =  284.22\n                                                       Prob &gt; F      =  0.0000\n                                                       R-squared     =  0.6703\n                                                       Root MSE      =  .14407\n\n------------------------------------------------------------------------------\n             |               Robust\n      logfev |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n      smoker |  -.0535896   .0241879    -2.22   0.027    -.1011293     -.00605\n         age |   .0215295   .0034817     6.18   0.000     .0146864    .0283725\n       loght |   2.869658   .1279943    22.42   0.000     2.618093    3.121222\n       _cons |  -11.09461   .5153323   -21.53   0.000    -12.10746   -10.08176\n------------------------------------------------------------------------------\n\nIntercept\n\nGeometric mean of FEV in newborn nonsmokers who are 1 inch in height is 0.000015 l/sec\n\nIntercept corresponds to the log geometric mean in a group having all predictors equal to \\(0\\) (nonsmokers, age 0, log height 0)\nThere is no scientific relevance because there are no such people in our data in either our sample or the population\n\n\nAge effect\n\nGeometric mean of FEV is \\(2.2\\%\\) higher for each year difference in age between two groups with the same height and smoking status (95% CI: \\(1.5\\%\\) to \\(2.9\\%\\) higher for each year difference in age)\n\nThese results are highly atypical of what we might expect with no true difference in geometric means between age groups having similar smoking status and height (\\(p &lt; 0.001\\))\n\nNote that there is clear evidence that height confounded the age effect estimated in the analysis that only considered age and smoking, but there is still a clearly independent effect of age on FEV\n\nHeight effect\n\nGeometric mean of FEV is \\(31.5\\%\\) higher for each \\(10\\%\\) difference in height between two groups with the same age and smoking status (95% CI: \\(28.3\\%\\) to \\(34.6\\%\\) higher for each \\(10\\%\\) difference in height)\n\nThese results are highly atypical of what we might expect with no true difference in geometric means between height groups having similar smoking status and age (\\(p &lt; 0.001\\))\nCalculations: \\(1.1^{2.867} = 1.315\\)\n\nNote that the CI for the regression coefficient is consistent with the scientifically-hypothesize value of 3\n\nSmoking effect (age, height adjusted interpretation)\n\nGeometric mean of FEV is \\(5.2\\%\\) lower in smokers than in nonsmokers of the same age and height (95% CI: \\(9.6\\%\\) to \\(0.6\\%\\) lower)\n\nThese results are atypical of what we might expect with no true difference between groups of the same age and height (\\(p = 0.027\\))\nCalculations: \\(e^{-.054} = 0.948; e^{-.101} = 0.904; e^{-0.006} = .994\\)\n\n\nComparing age-adjusted to age- and height-adjusted analyses\n\nNo difference in effect of smoking suggests that there was no more confounding after age adjustment\nEffect of height adjustment on precision\n\nLower Root MSE (0.144 vs 0.209) would tend to increase precision of estimate of smoking effect\nLittle association between smoking and height after adjusting for age will not tend to lower precision\nNet effect: Higher precision (adj SE 0.024 vs unadj SE 0.034)\n\n\n\n\n\n\n5.2 Logistic model\n\nContinue our academic exercise using logistic regression\n\nDichotomize FEV at median (2.93)\nIn real life, we would not want to make a continuous variable like FEV into a binary variable\nHere we will observe what happens to parameter estimates and SE across models\n\nSmoking\nSmoking adjusted for age\nSmoking adjusted for height\nSmoking adjusted for age and height\n\n\n\n\n\nCode\nfev$fevbin &lt;- (fev$fev&gt;=2.93)+0\n\nlrm.unadj &lt;- lrm(fevbin ~ smoker, data=fev)\nlrm.adj1 &lt;- lrm(fevbin ~ smoker + age, data=fev)\nlrm.adj2 &lt;- lrm(fevbin ~ smoker + loght, data=fev)\nlrm.adj3 &lt;- lrm(fevbin ~ smoker + age + loght, data=fev)\n\n\n\n. logit fevbin smoker if age&gt;=9\nLogistic regression                               Number of obs   =        439\n                                                  LR chi2(1)      =      15.81\n                                                  Prob &gt; chi2     =     0.0001\nLog likelihood = -296.38409                       Pseudo R2       =     0.0260\n\n------------------------------------------------------------------------------\n      fevbin |      Coef.   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n      smoker |   1.120549   .2959672     3.79   0.000      .540464    1.700634\n       _cons |  -.1607732   .1037519    -1.55   0.121    -.3641231    .0425767\n------------------------------------------------------------------------------\n\n. \n. logit fevbin smoker age if age&gt;=9\nLogistic regression                               Number of obs   =        439\n                                                  LR chi2(2)      =      88.68\n                                                  Prob &gt; chi2     =     0.0000\nLog likelihood = -259.95032                       Pseudo R2       =     0.1457\n\n------------------------------------------------------------------------------\n      fevbin |      Coef.   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n      smoker |   .1971403   .3402482     0.58   0.562     -.469734    .8640146\n         age |    .470792   .0637495     7.39   0.000     .3458453    .5957386\n       _cons |  -5.360912   .7056265    -7.60   0.000    -6.743914   -3.977909\n------------------------------------------------------------------------------\n\n. \n. logit fevbin smoker loght if age&gt;=9\nLogistic regression                               Number of obs   =        439\n                                                  LR chi2(2)      =     224.11\n                                                  Prob &gt; chi2     =     0.0000\nLog likelihood = -192.23646                       Pseudo R2       =     0.3682\n\n------------------------------------------------------------------------------\n      fevbin |      Coef.   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n      smoker |   .4159311   .3646506     1.14   0.254    -.2987709    1.130633\n       loght |   33.08872   3.169516    10.44   0.000     26.87659    39.30086\n       _cons |  -137.6043   13.16723   -10.45   0.000    -163.4116    -111.797\n------------------------------------------------------------------------------\n\n. \n. logit fevbin smoker age loght if age&gt;=9\nLogistic regression                               Number of obs   =        439\n                                                  LR chi2(3)      =     230.19\n                                                  Prob &gt; chi2     =     0.0000\nLog likelihood = -189.19487                       Pseudo R2       =     0.3782\n\n------------------------------------------------------------------------------\n      fevbin |      Coef.   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n      smoker |    .047695   .3933191     0.12   0.903    -.7231963    .8185863\n         age |   .1753759   .0723176     2.43   0.015      .033636    .3171157\n       loght |   31.04791   3.289031     9.44   0.000     24.60152    37.49429\n       _cons |  -131.0647    13.5393    -9.68   0.000    -157.6013   -104.5282\n------------------------------------------------------------------------------\n\n5.2.1 Results Summarized\n\nCoefficients (logit scale)\n\n\n\nModel\nSmoker\nAge\nLog(Height)\n\n\n\n\nSmoke\n1.12\n\n\n\n\nSmoke+Age\n0.19\n0.47\n\n\n\nSmoke+Ht\n0.42\n\n33.1\n\n\nSmoke+Age+Ht\n0.47\n0.18\n31.0\n\n\n\nStd Errors (logit scale)\n\n\n\nModel\nSmoker\nAge\nLog(Height)\n\n\n\n\nSmoke\n0.30\n\n\n\n\nSmoke+Age\n0.34\n0.06\n\n\n\nSmoke+Ht\n0.36\n\n3.2\n\n\nSmoke+Age+Ht\n0.39\n0.07\n3.3\n\n\n\nAdjusting for the height\n\nComparing Smoke+Age to Smoke+Age+Ht\n\nHeight is being added as a precision variable\nDeattenuated the smoking effect (estimate changes from 0.19 to 0.47, which is further from 0)\nIncreased the standard error estimate (0.34 to 0.39)\nBoth of these change are predicted by the previous discusssion\n\nComparing Smoke to Smoke+Ht\n\nAttenuated the smoking effect (1.12 to 0.42, closer to 0)\nIncreased the standard error estimate\nHeight now also acting acting as a confounding variable (because we are not modeling age), so these changes were not predictable\n\n\nAdjusting for the confounding variable (age)\n\nComparing Smoke+Ht to Smoke+Age+Ht\n\nDeattenuated the estimated coefficient (0.42 to 0.47)\nIncreased the standard error\n\nComparing Smoke to Smoke+Age\n\nAttenuated the coefficient estimate (1.12 to 0.19)\nIncreased the standard error\n\nChanges in estimates and standard errors were not predictable for this, or any, confounder"
  },
  {
    "objectID": "notes/Lec11.DoseResponse.html",
    "href": "notes/Lec11.DoseResponse.html",
    "title": "Flexible Modeling of Continuous Predictors",
    "section": "",
    "text": "Code\nlibrary(rms)\nlibrary(ggplot2)\nlibrary(splines)\nlibrary(lspline)\nlibrary(lmtest)\nlibrary(sandwich)\nlibrary(car)"
  },
  {
    "objectID": "notes/Lec11.DoseResponse.html#overview",
    "href": "notes/Lec11.DoseResponse.html#overview",
    "title": "Flexible Modeling of Continuous Predictors",
    "section": "1.1 Overview",
    "text": "1.1 Overview\n\nWhen covariates are measured on a continuous scale, we have many different approaches to analysis\nTo date, we have focused primarily on approaches that facilitate interpretation of our parameter estimates\n\nUntransformed\n\nAdditive changes\nCompare group differing by \\(X\\) units in the covariate of interest\n\nLog transformed\n\nFold (multiplicative) changes\nCompares groups with a \\(k\\)-fold difference in the covariate of interest\n\n\nFlexible modeling of continuous covariates are another option\n\nInterpretation often relies on plots, except in special cases\nIt is best to pre-specify how we intend to model continuous covariates to avoid data-driven modeling decision\n\n\n\n1.1.1 Common continuous measures\n\nDose. Typically, dose of some treatment. Higher doses presumed to lead to greater (or lesser) response.\nExposure. Typically, greater precision (more power) to consider amount of exposure rather than just Exposed versus Unexposed\nMany others. Age, serum cholesterol, blood pressure, height, etc.\n“Dose-response” in the following notes that follow more generally refers to “doses” of any type of continuous covariate (“doses of age”, “doses of cholesterol”, etc.)\n\n\n\n1.1.2 Scientific questions when modeling dose-response\n\nWe have many choices for modeling dose-response\nConsider the scientific question when deciding on an appropriate model\nHow does the scientific question you want to answer relate to how you model the dose-response relationship?\n\nModels for detecting an association in subgroups defined by \\(X\\)\nModels for estimating the mean response in subgroups defined by \\(X\\)\nModels for predicting individual response in subgroups defined by \\(X\\)\n\nWill describe methods for\n\nModeling complex dose-response curves\nFlexible methods (non-parametric loess smooths, parameteric regression splines)"
  },
  {
    "objectID": "notes/Lec11.DoseResponse.html#hierarchical-models",
    "href": "notes/Lec11.DoseResponse.html#hierarchical-models",
    "title": "Flexible Modeling of Continuous Predictors",
    "section": "1.2 Hierarchical Models",
    "text": "1.2 Hierarchical Models\n\n1.2.1 Full and restricted models\n\nWhen testing for associations, we are implicitly comparing two models\n\n“Full” model\n\nUsually corresponds to alternative hypothesis\nContains all terms in the “restricted” model plus some terms that we want to test for inclusion\nExample: \\(g(\\theta) = \\beta_0 + \\beta_1 * X_1 + \\beta_2 * X_2\\)\n\n“Restricted” model\n\nUsually corresponds to the null hypothesis\nTerms in the model are the subset of the terms in the full model that are not being tested\nExample: \\(g(\\theta) = \\beta_0 + \\beta_1 * X_1\\)\n\nJargon: The restricted model is “nested” in the full model\n\nIf we set some parameters in the full model equal to zero, we will get the restricted model\nWe have very good methods for testing differences between nested models\n\nIs a model with weight and height as predictors better than a model with just weight?\n\nOur methods are not as good for comparing models that are not nested\n\nIs a model with just weight as a predictor better than a model with just height as a predictor?\n\n\nNon-nested models (e.g. \\(X_1\\) is weight, \\(X_2\\) is height)\n\n\\(g(\\theta) = \\beta_0 + \\beta_1 * X_1\\)\n\\(g(\\theta) = \\gamma_0 + \\gamma_2 * X_2\\)\n\n\n\n\n\n1.2.2 Scientific interpretation of full versus restricted models\n\nOur scientific interpretation of our statistical tests depends on the meaning of the restricted model compared to the full model\n\nConsider the the additional associations that are possible with the full model that are not possible with the restricted model\n\nAppears very straightforward when we are only considering one full model compared to one restricted model\n\nIn practice, we may have many restricted models in mind\nMultiple testing problem appears if we are considering many restricted models\n\n\n\n1.2.2.1 Example 1: Adjusted effects\n\nFull model: FEV vs smoking, age, height\nRestricted model: FEV vs age, height\nIf the full model provides no clear advantage over the restricted model, then we conclude that there is insufficient evidence to suggest an association between FEV and smoking when controlling for age and height\nNote how carefully I stated the above\n\n“Insufficient evidence” (not evidence that smoking is not associated)\nConditional on controlling for age and height. Smoking, alone, may be an important predictor of FEV. The question answered by the full and reduced model is a comparison of FEV “among individuals of the same age and height but differing in smoking status” not just individuals who differ in smoking status.\n\n\n\n\n1.2.2.2 Example 2: Tests of linearity\n\nFull model: Survival vs cholesterol and cholesterol\\(^2\\)\n\nLinear and quadratic terms for cholesterol\nModel allows for a parabolic shape for chelesterol\n\nRestricted model: Survival vs cholesterol\nIf the full model provides no clear advantage over the restricted model, we conclude that there is insufficient evidence to suggest a U-shaped trend in survival with cholesterol\nNote that a quadratic term does not include all possibilities for non-linearity. We will cover other (better) approaches later in the notes\n\n\n\n\n1.2.3 Statistical comparison of full and reduced models\n\nWald test\n\nFits only the full model\nTests if the parameter (or parameters) in the full model are equal to zero\nCan be used for robust and non-robust standard error estimates (fit using GEE)\n\nLikelihood Ratio test\n\nFits both the full and reduced model\nCompares the likelihood functions to determine if they are significantly different\nCan be used for non-robust models only (fit using likelihood)\n\nScore test\n\nOnly fits the reduced model\nConsiders the gradient of the likelihood function. After fitting the reduced model, looks at what is left over to see if it can be predicted by additional covariates\nNot generally reported in regression model setting, but some common tests are Score tests (e.g. log-rank test)\n\nIn large samples, the Wald, Likelihood Ratio, and Score tests converge\n\nGiven our focus on robust standard error estimates, we have used Wald tests almost exclusively\n\n\n\n\n1.2.4 Models with interactions (effect modification)\n\nModels to test for effect modification (interactions) are hierarchical\n\nFull Model: Include the interactions\nReduced Model: Set the interactions to be tested in the full model to zero\n\nBest scientific approach is to pre-specify the statistical model that will be used for analysis\nSometimes we choose a relatively large model including interactions\n\nAllows us to address more questions (e.g. effect modification)\nSometimes gives greater precision\n\nTrade-offs between more parameters to estimate versus smaller within group variability\n\n\nDeciding which parameters to test from the full model\n\nIt can be difficult to decide the statistical test that corresponds to specific scientific questions\nNeed to consider the restricted model that corresponds to your null hypothesis\n\nWhich parameters in the full model need to be set to zero to test your desired hypothesis\n\n\n\n\n1.2.4.1 Examples of different scientific tests from the same full model\n\nIn the following examples, the full model is the same but we change the restricted model based on the scientific question\n\nFull model: Survival vs sex, smoking, sex-smoking interaction\nExample Question 1: Is there effect modification?\n\nRestricted model: Survival vs sex and smoking\n\nTest that the sex-smoking interaction is zero\n\n\nExample question 2: Is there an association between survival and sex?\n\nRestricted model: Survival vs smoking\n\nTest that parameters for sex and sex-smoking interaction are zero\nDoes sex effect survival in either smokers or non-smokers?\n\n\nExample question 3: Is there an association between survival and smoking?\n\nRestricted model: Survival vs sex\n\nTest that parameters for smoking and sex-smoking interaction are zero\n\n\n\n\n\n\n1.2.4.2 Be careful when making data-driven model building decisions\n\nWe are often tempted to remove parameters that are not statistically significant before proceeding to other tests\n\nYou will find many textbooks that advocate using this analysis approach\n\ne.g. fit the interaction, if it is not significant, exclude it from the model\nSuch texts are invariable written by mathematicians with no understanding of the underlying science\nWhen you work in the same scientific area, you will eventually “discover” false-positive assocations if you use this model building technique\n\nSuch data-driven analyses tend to suggest that failure to reject the null means equivalence\n\nBut, if you make this conclusion, you are wrong (large p-values do not indicate equivalence)\nAbscence of evidence is not evidence of abscence\n\nA procedure where we fit one model, and then use the results to determine the next model we will fit tends to underestimate the true standard error\n\nA consequence of the genearl multiple testing problem\nWill lead to declaring statistical significance more often than you should (inflates type-I error rate)\nMay also lead to parameter estimates that are biased away from the null\n\nStatistical significance increases as a function of the effect size and the inverse of the standard error\nIf, by chance, you see a large effect in your sample, you will be more likely to declare significance\n\n\n\nHow we interpret “negative” studies\n\nThere are other many reasons for not rejecting the null hypothesis of zero slope\n\nThere may be no association\nThere may be an association, but not in the parameter considered (i.e. the mean response)\nThere may be an association in the parameter considered, but the best fitting line has zero slope (a curvilinear association in the parameter)\nThere may be a first order trend in the parameter, but we lack sufficient statistical precision to be confident that it truly exists (a type-II error)\n\nBe careful to not just assume that it the first reason, no association\nStatistically, there is no way of discriminating between these four possibilities unless you have an infinite sample size"
  },
  {
    "objectID": "notes/Lec11.DoseResponse.html#modeling-complex-dose-response",
    "href": "notes/Lec11.DoseResponse.html#modeling-complex-dose-response",
    "title": "Flexible Modeling of Continuous Predictors",
    "section": "1.3 Modeling Complex Dose-Response",
    "text": "1.3 Modeling Complex Dose-Response\n\n\n\nAll models are wrong, but some are useful.\n\n\n\n1.3.1 S-Shaped dose-response curves\n\nIt is common in biology, medicine, and science to find “S-shaped” dose-response curves\n\n\n\nCode\nset.seed(1234)\nn &lt;- 100\nplotdata &lt;- data.frame(x=1:n,\n                       y=20 + 50 / (1+exp(-.15*(1:n-50))))\nplotdata$y &lt;- plotdata$y + rnorm(n,0,4)\nggplot(plotdata, aes(x=x, y=y)) + geom_point() + theme_bw() + geom_smooth()\n\n\n\n\n\nS-shaped dose response curve fit with loess smoother\n\n\n\n\n\nIf we happen to sample over the entire range of \\(X\\) (in this example, from 0 to 100), we will would observe all parts of the S-curve\nHowever, if we were only to sample from a limited range of \\(X\\), we might observe only parts of the curve where it was approximately linear\n\n\n\nCode\nplotdata$xint &lt;- cut(plotdata$x, c(0,30,70,100))\nggplot(plotdata, aes(x=x, y=y)) + facet_wrap(~ xint) + geom_point() + theme_bw() + geom_smooth()\n\n\n\n\n\nThe S-shaped dose response curve fit is approximately linear within three ranges of X.\n\n\n\n\n\nEven if we expect a complex dose response relationship, we must also consider the range of dose that we are sampling over when considering the needed complexity of the model\n\n\n1.3.1.1 Loess smoothing\n\nThe default smooth for ‘geom_smooth’ when there are fewer than 1000 data points is a loess smooth. These fits appear in the plots above\nLoess is a non-parametric smooth where fitting is done locally.\nThat is, for the fit at point x, the fit is made using points in a neighborhood of x, weighted by their distance from x\nThere are arguments to control the degree of smoothing, or how close we consider points to be in the “neighborhood of x”\n‘loess’ provides a nice visual representation to variety of data without having to specify a model\nWe will primarily be concerned with parametric smoothers as the parameters can be estimated (and in special cases interpreted) from multivariable regression models\n\n\n\n\n1.3.2 Linear predictors\n\nThe most commonly-used regression models all consider “linear predictors”\n\nLinear refers to linear in the parameters (\\(\\beta\\)s), and is not a reference to transformations of the predictors\nThe modeled predictors can be transformations of the scientific measurements and still be linear (in the parameters). These models are linear (in the parameters)\n\n\\(g[\\theta | X_i] = \\beta_0 + \\beta_1 \\textrm{log}(X_i)\\)\n\\(g[\\theta | X_i] = \\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2\\)\n\nExamples of models that are not linear in the parameters (We do not consider such models)\n\n\\(g[\\theta | X_i] = \\beta_0 X_i^{\\beta_1}\\)\n\\(g[\\theta | X_i] = \\beta_0 + \\beta_1 \\textrm{exp}(-\\beta_2 X_i)\\)\n\\(g[\\theta | X_i] = \\frac{\\beta_0 + \\beta_1 X_i}{1 + \\beta_2 X_i}\\)\n\n\n\n\n\n1.3.3 Types of transformations\n\nWe transform predictors to provide more flexible description of complex associations between the response and some scientific measure\n\nThreshold effects\nExponentially increasing effects\nU-shaped functions\nS-shaped functions\netc.\n\nFlexible modeling of continuous predictors was briefly covered in the notes on Poisson regression. We used a flexible dose-response model to examine the association between BMI and reflux rate by Esophagitis status\n\n\n\nCode\nbmi.data &lt;- read.csv(\"data/bmi.csv\", header=TRUE)\n# Events are pH less than 4\nbmi.data$events &lt;- bmi.data$totalmins4\nm.spline2.adj &lt;- glm(events ~ ns(bmi,4) + offset(log(totalmins)) + esop, data=bmi.data, family=\"poisson\")\nm.spline3.adj &lt;- lm(events / totalmins ~ ns(bmi,4) + esop, data=bmi.data)\npar(mfrow=c(1,2), mar=c(5,4,4,0.5))\nplot(18:40, exp(predict(m.spline2.adj, newdata=data.frame(bmi=18:40, totalmins=720, esop=1), type=\"link\")), type='l', ylab=\"Predicted number of events per day\", xlab=\"BMI\", ylim=c(0,100), main=\"Poisson Reg\")\naxis(4, labels=FALSE, ticks=TRUE)\nlegend(\"bottomright\", c(\"Esophagitis Pos\",\"Esophagitis Neg\"), inset=0.05, col=1, lty=1:2)\nlines(18:40, exp(predict(m.spline2.adj, newdata=data.frame(bmi=18:40, totalmins=720, esop=0), type=\"link\")), lty=2)\nplot(18:40, 720*predict(m.spline3.adj, newdata=data.frame(bmi=18:40, esop=1), type=\"response\"), type='l', col='Blue', ylab=\"\", xlab=\"BMI\", ylim=c(0,100), main=\"Linear Reg\", axes=FALSE)\naxis(1)\naxis(4)\naxis(2, labels=FALSE, ticks=TRUE)\nbox()\nlines(18:40, 720*predict(m.spline3.adj, newdata=data.frame(bmi=18:40, esop=0), type=\"response\"), type='l', col='Blue', lty=2)\nlegend(\"bottomright\", c(\"Esophagitis Pos\",\"Esophagitis Neg\"), inset=0.05, col=\"Blue\", lty=1:2)\n\n\n\n\n\n\n1:1 Transformations\n\nSometimes we can transform 1 scientific measurement into 1 modeled predictor and acheve approximate linearity\nEx: log transformations will sometimes address apparent threshold effects\n\n\n\n\nCode\nset.seed(33)\nn &lt;- 200\n\nx &lt;- runif(n)\nlogx &lt;- log(x)\n\ny &lt;- 0 + logx + rnorm(n)\npar(mfrow=c(1,2))\nplot(x,y, main=\"Untransformed\")\nlines(lowess(y~x), lwd=2)\n\nplot(logx, y, main=\"Log Transformed X\")\nlines(lowess(y ~ logx), lwd=2)\n\n\n\n\n\nLog transformation addresses threshold effect in untransformed X\n\n\n\n\n\nEx: cubing height produces a more linear association with FEV\n\n\n\nCode\nfev &lt;- read.csv(file=\"data/FEV.csv\")\npar(mfrow=c(1,2))\nplot(fev$height, fev$fev, main=\"Untransformed\", pch='.', xlab=\"Height\",ylab=\"FEV\")\nlines(lowess(fev$fev ~ fev$height), lwd=2)\n\nfev$height3 &lt;- fev$height^3\nplot(fev$height3, fev$fev, main=\"Cubed Height\", pch='.', xlab=\"Height^3\",ylab=\"FEV\")\nlines(lowess(fev$fev ~ fev$height3), lwd=2)\n\n\n\n\n\nCubing height produces a more linear association with FEV\n\n\n\n\n\n1:Many transformations\n\nSometimes we transform 1 scientific measurement into several modeled predictors\n\nPolynomial regression\nDummy variables (factor or class variables)\nPiecewise linear\nSplines\n\n\nPolynomial regression\n\nFit linear term plus higher order terms (squared, cubic, ...)\nCan fit an arbitrarily-complex function\n\nAn \\(n\\)-th order polynomial can fit \\(n+1\\) points exactly\n\nGenerally, very difficult to interpret parameters\n\nI will usually graph functions when I want an interpretation\n\nSpecial uses\n\n2nd order (quadratic) model to look for U-shaped trends (e.g. alcohol consumption and cardiovascular risk? Potential hypothesis being that moderate consumption of alcohol beneficial relative to either no consumption or high consumption)\nTests for linearity achieved by testing that all higher order terms have parameters equal to zero (hierarchical model)\nFull Model: \\(g(\\theta) = \\beta_0 + \\beta_1 * X + \\beta_2 * X^2 + \\beta_3 * X^3\\)\nReduced Model: \\(g(\\theta) = \\beta_0 + \\beta_1 * X\\)\n\n\n\n\n\n1.3.4 Example: FEV and Height association\n\nWe can try to assess whether any association between mean FEV and height follows a straight line association\nFit a third order (cubic) polynomial due to the known scientific relationship between volume and height (using robust standard error estimates)\n\n\n\nCode\n# Create squared and cubic terms\nfev$ht2 &lt;- fev$height^2\nfev$ht3 &lt;- fev$height^3\n\nm1 &lt;- lm(fev ~ height + ht2 + ht3, data=fev)\ncoeftest(m1, vcov=sandwich)\n\n\n\nt test of coefficients:\n\n               Estimate  Std. Error t value Pr(&gt;|t|)\n(Intercept)  4.5691e-01  1.2339e+01  0.0370   0.9705\nheight       3.0595e-02  6.3266e-01  0.0484   0.9614\nht2         -1.5222e-03  1.0746e-02 -0.1416   0.8874\nht3          2.5797e-05  6.0481e-05  0.4265   0.6699\n\n\n\nFrom the model output, is height a significant predictor of FEV?\nNote that the p-values for each term are not significant\n\nBut each of these tests are addressing irrelevant questions\n\nheight: After adjusting for the 2nd and 3rd order terms relationships, is the linear term important?\nhtsqr: After adjusting for the linear and 3rd order terms relationships, is the quadratic term important?\nhtcub: After adjusting for the linear and 2nd order terms relationships, is the cubic term important?\n\nWe need to test if test the 2nd and 3rd order terms simultaneously\n\nWill test if height-FEV has a linear relationship\n\n\n\n\n\nCode\nlinearHypothesis(m1, c(\"ht2\",\n                       \"ht3\"),\n                 vcov=sandwich(m1))\n\n\nLinear hypothesis test\n\nHypothesis:\nht2 = 0\nht3 = 0\n\nModel 1: restricted model\nModel 2: fev ~ height + ht2 + ht3\n\nNote: Coefficient covariance matrix supplied.\n\n  Res.Df Df      F   Pr(&gt;F)    \n1    652                       \n2    650  2 30.641 1.92e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nWe find clear evidence that the trend in mean FEV versus height is non-linear (\\(p &lt; 0.001\\))\nNote that if we had seen \\(p &gt; 0.05\\), we could not be sure it was linear. The relationship may have been non-linear, but not in a way the cubic polynomial could detect\n\n\n1.3.4.1 Example: log FEV and polynomial height\n\nWe can try to assess whether any association between mean log FEV and height follows a straight line relationship\nAgain, we will fit a third order (cubic) polynomial, but this time we do not have any good scientific justification for such a model\n\n\n\nCode\nfev$logfev &lt;- log(fev$fev)\n\nm2 &lt;- lm(logfev ~ height + ht2 + ht3, data=fev)\ncoeftest(m2, vcov=sandwich)\n\n\n\nt test of coefficients:\n\n               Estimate  Std. Error t value Pr(&gt;|t|)\n(Intercept) -2.7918e+00  4.9700e+00 -0.5617   0.5745\nheight       7.0664e-02  2.4759e-01  0.2854   0.7754\nht2         -1.8281e-04  4.0899e-03 -0.0447   0.9644\nht3          3.2398e-07  2.2404e-05  0.0145   0.9885\n\n\n\nNote that again, the p-values for the individual terms are not significant and are still addressing uninteresting scientific questions\nTest the squared and cubed terms simultaneously\n\n\n\nCode\nlinearHypothesis(m2, c(\"ht2\",\n                       \"ht3\"),\n                 vcov=sandwich(m2))\n\n\nLinear hypothesis test\n\nHypothesis:\nht2 = 0\nht3 = 0\n\nModel 1: restricted model\nModel 2: logfev ~ height + ht2 + ht3\n\nNote: Coefficient covariance matrix supplied.\n\n  Res.Df Df      F Pr(&gt;F)\n1    652                 \n2    650  2 0.2945  0.745\n\n\n\nWe do not find clear evidence that the trend in mean log FEV versus height is non-linear\n\nThis does not prove linearity because it could have been nonlinear in a way that a cubic polynomial could not detect\nHowever, my guess it the cubic polynomial would have picked up most reasonable patterns of non-linearity likely to occur in this setting\n\nWe have not addressed the question of whether log FEV is associated with height\n\nThis question could have been addressed in the cubic model by\n\nTesting all three height-derived variable simultaneously\nOR looking at the overall F-test (because only height variables are in the model)\n\nAnother alternative would be to fit a model with only the linear term for height\n\nIn general, it is a very bad idea to go fishing for models, so pick an approach beforehand and use that\nIf I suspected a complex dose-response relationship beforehand, I would fit the complex model and test all of the height coefficients\nIf I just cared about showing a first order trend, the following output would answer that question\n\n\n\n\n\nCode\nm3 &lt;- lm(logfev ~ height, data=fev)\ncoeftest(m3, vcov=sandwich)\n\n\n\nt test of coefficients:\n\n             Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept) -2.271312   0.068447 -33.184 &lt; 2.2e-16 ***\nheight       0.052119   0.001121  46.494 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n1.3.5 Dummy Variables\n\nIndicator variables for all but one group\nThis is the only appropriate method for modeling modeling nominal (unordered) categorical variables\n\nExample: Marital status using indicator for\n\nMarried (married=1, everything else=0)\nWidowed (widowed=1, everything else=0)\nDivorced (divorced=1, everything else=0)\nSingle would then be the intercept\n\n\nWhile it is the only approach for nominal variables, dummy variable coding is often used for other settings\nMakes regression equivalent to Analysis of Variance (ANOVA)\n\nANOVA is an old technique developed by R.A. Fisher in 1921\nCan reproduce results of ANOVA exactly with regression, so ANOVA is of limited use today (but will find it around)\n\n\n\n1.3.5.1 Example: Mean salary by Field\n\nField is a nominal variable, so we must use dummy variables\n\nI decided to use Other as a reference group. By default, R will order alphabetically so Arts would have been the default reference group\n\n\n\n\nCode\nsalary &lt;- read.csv(\"data/salary.csv\")\nsalary &lt;- salary[salary$year==95,]\n\nsalary$field &lt;- factor(salary$field)\nsalary$field &lt;- relevel(salary$field, ref=\"Other\")\n\nm4 &lt;- lm(salary ~ field, data=salary)\ncoeftest(m4, vcov=sandwich)\n\n\n\nt test of coefficients:\n\n            Estimate Std. Error  t value  Pr(&gt;|t|)    \n(Intercept)  6291.64      61.01 103.1252 &lt; 2.2e-16 ***\nfieldArts   -1013.56     104.73  -9.6775 &lt; 2.2e-16 ***\nfieldProf    1225.03     133.56   9.1718 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\ncoefci(m4, vcov=sandwich)\n\n\n                2.5 %    97.5 %\n(Intercept)  6171.970 6411.3056\nfieldArts   -1218.985 -808.1269\nfieldProf     963.052 1487.0131\n\n\n\nInterpretation based on coding used\n\nIntercept corresponds to the mean salary in the Other field\n\nThese faculty will have both arts==0 and prof==0\nEstimated mean salary of $6292 per month (95% CI: [6172, 6411])\nHighly statistically different from $0 per month (a worthless test)\n\nSlope for arts is difference in mean salary between Fine Arts and Other fields\n\nFine Arts faculty will have arts==1 and prof==0; Other faculty will have arts==0 and prof==0\nEstimated difference in mean salary is $1014 lower (95% CI: [808, 1219] lower salary)\nHighly statistically different from $0 per month (a useful test, if we specified it a priori)\n\nSlope for prof is difference in mean salary between Professional and Other fields\n\nProfessional faculty will have arts==0 and prof==1; Other faculty will have arts==0 and prof==0\nEstimated difference in mean salary is $1225 higher (95% CI: [963, 1487] higher salary)\nHighly statistically different from $0 per month (a useful test, if we specified it a priori)\n\n\nBecause we modeled the three groups with two predictors plus an intercept, the estimates agree exactly with the sample means\n\nNo borrowing of information across field\nIf we had used a different reference group rather than Other, would also see agreement between model estimates and sample means\n\nHypothesis test: To test for mean salary differences by field\n\nWe have modeled field using two variables\nBoth slopes would have to be zero for there to be no association between field and mean salary\nWant to simultaneously tests the two slopes\n\n\n\n\nCode\nlinearHypothesis(m4, c(\"fieldArts\",\"fieldProf\"),\n                 vcov=sandwich(m4))\n\n\nLinear hypothesis test\n\nHypothesis:\nfieldArts = 0\nfieldProf = 0\n\nModel 1: restricted model\nModel 2: salary ~ field\n\nNote: Coefficient covariance matrix supplied.\n\n  Res.Df Df      F    Pr(&gt;F)    \n1   1596                        \n2   1594  2 121.08 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nWe can conclude there is a there is a significant difference in salary comparing faculty with different fields (\\(P&lt;0.001\\))\n\n\n\n1.3.5.2 Continuous variables\n\nWe can also use dummy variables to represent continuous variables\n\nContinuous variables measured at discrete levels (e.g. dose in an interventional experiment)\nContinuous variables divided into categories (often a bad idea)\n\nDummy variables will fit groups exactly\n\nIf no other predictors in the model, parameter estimates correspond exactly to descriptive statistics\n\nWith continuous variables, dummy variable coding assumes a step-function is true\nModeling with dummy variables ignores order of predictor of interest\n\n\n\nCode\nm.dummy &lt;- lm(height ~ factor(age), data=fev)\nnewdata &lt;- data.frame(age=3:19)\nnewdata$ht.dummy &lt;- predict(m.dummy, newdata=newdata)\nggplot(fev, aes(x=age, y=height)) + geom_point() + theme_bw() +\n  geom_point(data=newdata, aes(x=age, y=ht.dummy), col=\"red\", size=3)\n\n\n\n\n\nRed points are means at each level of age\n\n\n\n\n\nThe red points in the plot represents the mean height by subgroups defined by years of age\n\nDummy variables are used for each age (measured in years)\nStep function (e.g. when age changes from 8.999 to 9.000, expected height increases)\nSeveral ages have few subjects, which makes the estimates unreliable\n\nDo we really believe that average height of 18 year olds is less than the average height of 14, 15, 16, 17, and 19 year old?\n\nProbably not (the model is overfit)\nBy ignoring the ordering of age, we are not borrowing strength across adjacent age groups\nWhen building model, we can decide how much information we want to borrow from nearby groups\n\nPredicted heights for observed ages from the dummy variable and two polynomial models\n\n\n\nCode\nkable(newdata[12:17,], format='html', digits=2)\n\n\n\n\n\n\nage\nht.dummy\n\n\n\n\n12\n14\n67.52\n\n\n13\n15\n66.37\n\n\n14\n16\n67.38\n\n\n15\n17\n68.94\n\n\n16\n18\n66.00\n\n\n17\n19\n67.83"
  },
  {
    "objectID": "notes/Lec11.DoseResponse.html#flexible-methods",
    "href": "notes/Lec11.DoseResponse.html#flexible-methods",
    "title": "Flexible Modeling of Continuous Predictors",
    "section": "1.4 Flexible Methods",
    "text": "1.4 Flexible Methods\n\nWe have methods that can fit a wide variety of curve shapes\n\nDummy variables: Step function with tiny steps\nPolynomials: If high degree, allows many patterns of curvature\nSplines: Piecewise linear or piecewise polynomial\n\n\n\n1.4.1 Polyhomial functions\n\nWe can fit high order polynomials to flexibly model a continuous covariate\nA continuous covariate with \\(k\\) levels can be perfectly modeled using polynomials up to degree \\(k-1\\).\nIn the Height and Age dataset, there are 17 unique values of age (age is rounded to the nearest year), so we can perfectly model the means using polynomials up to degree 16\n\n\\(E[Height | Age = a] = \\beta_0 + \\beta_1 * a + \\beta_2 * a^2 + \\ldots + \\beta_{16} * a^{16}\\)\nEach \\(\\beta\\) is difficult to interpret, but the predicted value at the integer levels of age make sense.\nPredicted values of height at non-integer values of age (e.g. age of 18.5) may not make sense\nPolynomials become especially unstable near the upper and lower bounds of the age distribution. Stability near the upper and lower bounds is a general concern in many spline approaches.\n\nA polynomial of lesser degree could also be considered. I show a model that includes polynomials for age up to degree 3 in the following plot. It is also less stable near the upper and lower bounds of age, but not as obviously so as the 16 degree polynomial.\n\n\n\nCode\nggplot(fev, aes(x=age, y=height)) + geom_point() + theme_bw() + \n  geom_smooth(method=lm, formula=y ~ poly(x,16), se=FALSE) + \n  geom_smooth(method=lm, formula=y ~ poly(x,3), se=FALSE, col=\"Green\") +\n  geom_point(data=newdata, aes(x=age, y=ht.dummy), col=\"red\", size=3)\n\n\n\n\n\nSixteen degree and three degree polynomial model\n\n\n\n\n\n\nCode\nm.poly16 &lt;- lm(height ~ poly(age,16), data=fev)\nm.poly3 &lt;- lm(height ~ poly(age,3), data=fev)\nnewdata$ht.poly16 &lt;- predict(m.poly16, newdata)\nnewdata$ht.poly3 &lt;- predict(m.poly3, newdata)\nkable(newdata[12:17,], format='html', digits=2)\n\n\n\n\n\n\nage\nht.dummy\nht.poly16\nht.poly3\n\n\n\n\n12\n14\n67.52\n67.52\n67.29\n\n\n13\n15\n66.37\n66.37\n67.74\n\n\n14\n16\n67.38\n67.38\n67.83\n\n\n15\n17\n68.94\n68.94\n67.51\n\n\n16\n18\n66.00\n66.00\n66.77\n\n\n17\n19\n67.83\n67.83\n65.57\n\n\n\n\n\n\n\n\n\n1.4.2 Piecewise Linear Splines\n\nPiecewise linear curves\n\nJoined at “knots”\nStraight lines in between the knots\nConvenient parametrizations such that (1) coefficients are slopes of consecutive segments or (2) coefficients are slope changes at consecutive knots\nAdvantages\n\nInterpretability. Has the interpretation as the slope (or slope change) from the regression model, but within a range of \\(X\\)\nBest to specify know locations in advance for desired scientific interpretation\nNested within a restriced model with a single slope term (hierarchcical structure allow comparisons of complex to simpler models)\n\nDisadvantages\n\nAbrupt change in slope at each knot location\n\n\nStata: mkspline newvar0 knot1 newvar1 knot2 newvar2 ... knotp newvarp = oldvar\n\nWould then run regression using newvar0 ... newvarp\nAssumes straight lines between min and knot1; knot1 and knot2; etc.\n\nIn R, we can use the lspline package to make the linear splines\nThe following results will look at age and height using linear splines with 2 knots and 4 knots\n\n\n\nCode\nggplot(fev, aes(x=age, y=height)) + geom_point() + theme_bw() + \n  geom_smooth(method=lm, formula=y ~ lspline(x, knots=c(9.5,15.5)), se=FALSE) + \n  geom_smooth(method=lm, formula=y ~ lspline(x, knots=c(6.5,9.5,12.5,15.5)), se=FALSE, col=\"Green\") +\n  geom_point(data=newdata, aes(x=age, y=ht.dummy), col=\"red\", size=3)\n\n\n\n\n\nPiecewise linear splines with 2 or 4 knots. Red points are mean height at each subgroup of age.\n\n\n\n\n\n1.4.2.1 Piecewise linear spline interpretation (marginal is FALSE, the default)\n\nFocusing on the model with 2 knots for interpretation within age groups of [3 to 9], [10 to 15], and [16 to 19]\n\nKnots at 9.5 and 15.5 define three age groups\n\n\n\n\nCode\nm.linspline.nonmar &lt;- lm(height ~ lspline(age, knots=c(9.5,15.5)), data=fev)\ncoeftest(m.linspline.nonmar, varcov=sandwich)\n\n\n\nt test of coefficients:\n\n                                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                         38.491073   0.813179 47.3341   &lt;2e-16 ***\nlspline(age, knots = c(9.5, 15.5))1  2.488554   0.099940 24.9005   &lt;2e-16 ***\nlspline(age, knots = c(9.5, 15.5))2  1.085214   0.088609 12.2472   &lt;2e-16 ***\nlspline(age, knots = c(9.5, 15.5))3 -0.553084   0.371351 -1.4894   0.1369    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\ncoefci(m.linspline.nonmar, varcov=sandwich)\n\n\n                                         2.5 %    97.5 %\n(Intercept)                         36.8942979 40.087848\nlspline(age, knots = c(9.5, 15.5))1  2.2923105  2.684798\nlspline(age, knots = c(9.5, 15.5))2  0.9112196  1.259209\nlspline(age, knots = c(9.5, 15.5))3 -1.2822761  0.176108\n\n\n\n(Intercept) is the expect height when age is 0. Since this is outside the range of our data, it has no scientific interpretation.\nThe first slope term (‘lspline(age, knots = c(9.5, 15.5))1’) is the expected change in height comparing two individuals who differ in age by one year when both subjects are 9 years old or less. Comparing two children who are both 3 to 9 years old and differ in age by one year, the older child on average will be 2.49 inches taller than the younger child. We are 95% confident the true difference in height is between 2.29 and 2.68 inches taller.\n\nThat is, the model assumes that the change in height comparing 3 to 4, 4 to 5, 5 to 6, 6 to 7, 7 to 8, 8 to 9 is the same (a similar growth rate over these age ranges)\nThe model borrow strength across the age ranges by assuming the same slope\nThe model borrows strength across the next age range by assuming the mean height at the knot (9.5 years) is the same at the upper end of first age group and the lower end of the second age group.\n\nThe second slope term (‘lspline(age, knots = c(9.5, 15.5))2’) is the expected change in height comparing two individuals who differ in age by one year when both subjects are between 10 and 15 years old\nThe third slope term (‘lspline(age, knots = c(9.5, 15.5))3’) is the expected change in height comparing two individuals who differ in age by one year when both subjects are 16 years old or older\nWe can test if the age-height slope significantly changes with age category\n\n\\(H_0: \\beta_1 = \\beta_2, \\beta_1 = \\beta_3\\)\n\\(H_A\\): At least one not equal\n\n\n\n\nCode\nlinearHypothesis(m.linspline.nonmar, \n                 c(\"lspline(age, knots = c(9.5, 15.5))1=lspline(age, knots = c(9.5, 15.5))2\",\n                   \"lspline(age, knots = c(9.5, 15.5))1=lspline(age, knots = c(9.5, 15.5))3\"),\n                  vcov=sandwich(m.linspline.nonmar))\n\n\nLinear hypothesis test\n\nHypothesis:\nlspline(age, knots = c(9.5, 15.5))1 - lspline(age, knots = c(9.5, 15.5))2 = 0\nlspline(age, knots = c(9.5, 15.5))1 - lspline(age, knots = c(9.5, 15.5))3 = 0\n\nModel 1: restricted model\nModel 2: height ~ lspline(age, knots = c(9.5, 15.5))\n\nNote: Coefficient covariance matrix supplied.\n\n  Res.Df Df      F    Pr(&gt;F)    \n1    652                        \n2    650  2 85.721 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nWe reject the null hypothesis (\\(P&lt;0.001\\)) and conclude the linear spline model is a better fit than the restricted model, \\(E[Height | Age = a] = \\beta_0 + \\beta_1 * a\\)\nThe following is the basis matrix for the linear spline (non-marginal) for ages 3 to 19. It is relatively easy to calculate this matrix\n\nFirst columns is age\nSecond columns is max of 0 and (age minus the first knot)\nThird columns is max of 0 and (age minus the second knot)\n\n\n\n\nCode\ncbind(3:19, lspline(3:19,  knots = c(9.5, 15.5)))\n\n\n           1   2   3\n [1,]  3 3.0 0.0 0.0\n [2,]  4 4.0 0.0 0.0\n [3,]  5 5.0 0.0 0.0\n [4,]  6 6.0 0.0 0.0\n [5,]  7 7.0 0.0 0.0\n [6,]  8 8.0 0.0 0.0\n [7,]  9 9.0 0.0 0.0\n [8,] 10 9.5 0.5 0.0\n [9,] 11 9.5 1.5 0.0\n[10,] 12 9.5 2.5 0.0\n[11,] 13 9.5 3.5 0.0\n[12,] 14 9.5 4.5 0.0\n[13,] 15 9.5 5.5 0.0\n[14,] 16 9.5 6.0 0.5\n[15,] 17 9.5 6.0 1.5\n[16,] 18 9.5 6.0 2.5\n[17,] 19 9.5 6.0 3.5\n\n\n\nWe have transformed one measured predictor, age, into three predictors\n\n\n\n1.4.2.2 Piecewise linear spline interpretation (marginal is TRUE, the default)\n\nFocusing on the model with 3 knots for interpretation with age groups of [3 to 9], [10 to 15], and [16 to 19]\n\nKnots at 9.5 and 15.5 define these age groups\n\n\n\n\nCode\nm.linspline.mar &lt;- lm(height ~ lspline(age, knots=c(9.5,15.5), marginal=TRUE), data=fev)\ncoeftest(m.linspline.mar, varcov=sandwich)\n\n\n\nt test of coefficients:\n\n                                                     Estimate Std. Error\n(Intercept)                                          38.49107    0.81318\nlspline(age, knots = c(9.5, 15.5), marginal = TRUE)1  2.48855    0.09994\nlspline(age, knots = c(9.5, 15.5), marginal = TRUE)2 -1.40334    0.16335\nlspline(age, knots = c(9.5, 15.5), marginal = TRUE)3 -1.63830    0.42028\n                                                     t value  Pr(&gt;|t|)    \n(Intercept)                                          47.3341 &lt; 2.2e-16 ***\nlspline(age, knots = c(9.5, 15.5), marginal = TRUE)1 24.9005 &lt; 2.2e-16 ***\nlspline(age, knots = c(9.5, 15.5), marginal = TRUE)2 -8.5911 &lt; 2.2e-16 ***\nlspline(age, knots = c(9.5, 15.5), marginal = TRUE)3 -3.8981  0.000107 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\ncoefci(m.linspline.mar, varcov=sandwich)\n\n\n                                                         2.5 %     97.5 %\n(Intercept)                                          36.894298 40.0878479\nlspline(age, knots = c(9.5, 15.5), marginal = TRUE)1  2.292311  2.6847985\nlspline(age, knots = c(9.5, 15.5), marginal = TRUE)2 -1.724095 -1.0825855\nlspline(age, knots = c(9.5, 15.5), marginal = TRUE)3 -2.463570 -0.8130261\n\n\n\n(Intercept) is the expect height when age is 0. Since this is outside the range of our data, it has no scientific interpretation.\nThe first slope term (‘lspline(age, knots = c(9.5, 15.5))1’) is the expected change in height comparing two individuals who differ in age by one year when both subjects are 9 years old or less.\nThe second slope term (‘lspline(age, knots = c(9.5, 15.5))2’) is the change in the age-height slope for 10 to 15 year olds versus 3 to 9 year olds. This output is useful for estimating and testing the slope changes over age.\nThe third slope term (‘lspline(age, knots = c(9.5, 15.5))3’) is the change in the age-height slope for 16 to 19 year olds versus 3 to 9 year olds. This output is useful for estimating and testing the slope changes over age.\nUsing this model, we can test if the age-height slope significantly changes with age category\n\n\\(H_0: \\beta_2 = 0, \\beta_3 = 0\\)\n\\(H_A\\): At least one not equal\n\n\n\n\nCode\nlinearHypothesis(m.linspline.mar, \n                 c(\"lspline(age, knots = c(9.5, 15.5), marginal = TRUE)2\",\n                   \"lspline(age, knots = c(9.5, 15.5), marginal = TRUE)3\"),\n                 vcov=sandwich(m.linspline.mar))\n\n\nLinear hypothesis test\n\nHypothesis:\nlspline(age, knots = c(9.5, 15.5), marginal = TRUE)2 = 0\nlspline(age, knots = c(9.5, 15.5), marginal = TRUE)3 = 0\n\nModel 1: restricted model\nModel 2: height ~ lspline(age, knots = c(9.5, 15.5), marginal = TRUE)\n\nNote: Coefficient covariance matrix supplied.\n\n  Res.Df Df      F    Pr(&gt;F)    \n1    652                        \n2    650  2 85.721 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nThis is the same scientific test as for the non-marginal model, just a different parameteriziation. So, we obtain the same inference (F-stat and p-values are identical).\n\n\n\n\n1.4.3 Polynomial splines\n\nPolynomial splines introduces squared or cubic terms to increase the smoothness of the estimate. In particular, compare to linear splines, they are no abrupt changes in the slope at each knot point.\nThere are various types of polynomial splines, each with different formulas applied to generate the basis matrix for the regression model\n\nB-spline: ‘bs()’\n\nRepresenting the family of piecewise polynomials with the specified interior knots and degree\n\nNatural Cubic spline ‘ns()’\n\nEnforce the constraint that the function is linear beyond the boundary knots, which can either be supplied or default to the extremes of the data.\n\nRestricted Cubic spline ‘rcs()’\n\nrcs is a also a linear tail-restricted cubic spline function\n\n\nWe can choose to modify several parameters, depending on the function\n\nNumber of knots\nWhere to place the knots (or use defaults based on quantiles and number of knots)\nDegree of the polynomial. In practice, rarely is a degree over 3 considered\nIncreasing the number of knots and the degree of the polynomial will increase the flexibility of the model\n\nSeveral different comparisons are shown in the following plots\n\n\nB-spline versus polynomial function\n\n\nCode\nggplot(fev, aes(x=age, y=height)) + geom_point() + theme_bw() + \n  geom_smooth(method=lm, formula=y ~ poly(x, 3), se=FALSE, aes(color=\"Blue\")) + \n  geom_smooth(method=lm, formula=y ~ bs(x,3), se=FALSE, aes(col=\"Green\")) +\n  geom_point(data=newdata, aes(x=age, y=ht.dummy), col=\"Red\", size=3) + \n  scale_colour_manual(name=\"Model\", values=c(\"Blue\", \"Green\"), labels=c(\"poly(x,3)\",\"bs(x,3)\")) + theme(legend.position = c(0.8, 0.2))\n\n\n\n\n\nB-Spline with 3 degrees of freedom is the same as a 3 degree of polynomial. Two lines are not visible because the blue and green line are on top of each other.\n\n\n\n\n\n\nB-spline versus piecewise linear spline, function\n\n\nCode\nggplot(fev, aes(x=age, y=height)) + geom_point() + theme_bw() + \n  geom_smooth(method=lm, formula=y ~ lspline(x,knots=c(9.5, 15.5)), se=FALSE, aes(color=\"Blue\")) + \n  geom_smooth(method=lm, formula=y ~ bs(x,knots=c(9.5, 15.5), degree=3), se=FALSE, aes(color=\"Green\")) +\n  geom_point(data=newdata, aes(x=age, y=ht.dummy), col=\"red\", size=3) + \n  scale_colour_manual(name=\"Model\", values=c(\"Blue\", \"Green\"), labels=c(\"lspline(x,knots=c(9.5, 15.5))\",\"bs(x,knots=c(9.5, 15.5)\")) + theme(legend.position = c(0.8, 0.2))\n\n\n\n\n\nB-Spline with 3 degrees of freedom and knots at 9.5 and 15.5 compared to piecewise linear spline with knots at 9.5 and 15.5\n\n\n\n\n\n\nB-spline versus natural spline\n\n\nCode\nggplot(fev, aes(x=age, y=height)) + geom_point() + theme_bw() + \n  geom_smooth(method=lm, formula=y ~ ns(x, df=3), se=FALSE, aes(color=\"Blue\")) + \n  geom_smooth(method=lm, formula=y ~ bs(x,3, knots=c(9.5, 15.5)), se=FALSE, aes(color=\"Green\")) +\n  geom_point(data=newdata, aes(x=age, y=ht.dummy), col=\"red\", size=3)+  \n  scale_colour_manual(name=\"Model\", values=c(\"Blue\", \"Green\"), labels=c(\"ns(x, df=3)\",\"bs(x,3, knots=c(9.5, 15.5)\")) + theme(legend.position = c(0.8, 0.2))\n\n\n\n\n\nB-Spline with 3 degrees of freedom and knots at 9.5 and 15 compared to ns (3 df). The enforcement of the linearity constraint beyond the boundary knots is particular apparent for the natural spline at the low end of age.\n\n\n\n\n\n\nNautral spline versus restricted cubic spline\n\nNatural splines and restricted cubic spline are both cubic splines with restrictions of linearity beyond the boundary knot locations\nThey have different inputs which can result in different model fits\n\n\n\n\nCode\nggplot(fev, aes(x=age, y=height)) + geom_point() + theme_bw() + \n  geom_smooth(method=lm, formula=y ~ ns(x, knots=c(9.5, 15.5)), se=FALSE, aes(color=\"Blue\")) + \n  geom_smooth(method=lm, formula=y ~ rcs(x, parms=c(0, 9.5, 15.5, 19)), se=FALSE, aes(color=\"Green\")) +\n  geom_point(data=newdata, aes(x=age, y=ht.dummy), col=\"red\", size=3) +  \n  scale_colour_manual(name=\"Model\", values=c(\"Blue\", \"Green\"), labels=c(\"ns(x, knots=c(9.5, 15.5)\",\"rcs(x, parms=c(0, 9.5, 15.5, 19)\")) + theme(legend.position = c(0.8, 0.2))\n\n\n\n\n\nNatrual spline (with knots at 3.5 and 9.5) compared with restricted cubic spline with parms set to give the same fit. Both spline approaches impose a linearity constraint at the tails. Fits are same using these inputs.\n\n\n\n\n\n\nCode\nggplot(fev, aes(x=age, y=height)) + geom_point() + theme_bw() + \n  geom_smooth(method=lm, formula=y ~ ns(x, df=3), se=FALSE, aes(color=\"Blue\")) + \n  geom_smooth(method=lm, formula=y ~ rcs(x, nk=5), se=FALSE, aes(color=\"Green\")) +\n  geom_point(data=newdata, aes(x=age, y=ht.dummy), col=\"red\", size=3) +  \n  scale_colour_manual(name=\"Model\", values=c(\"Blue\", \"Green\"), labels=c(\"ns(x, df=3)\",\"rcs(x, nk=5)\")) + theme(legend.position = c(0.8, 0.2))\n\n\n\n\n\nNatrual cubic spline (3 d.f) compared with restricted cubic spline using default inputs. Both spline approaches impose a linearity constraint at the tails, but with different default knot locations the fits are different.\n\n\n\n\n\n\nNautral spline versus piecewise lienar spline\n\n\nCode\nggplot(fev, aes(x=age, y=height)) + geom_point() + theme_bw() + \n  geom_smooth(method=lm, formula=y ~ ns(x, df=3), se=FALSE, aes(color=\"Blue\")) + \n  geom_smooth(method=lm, formula=y ~ lspline(x, knots=c(9.5,15.5)), se=FALSE, aes(color=\"Green\")) +\n  geom_point(data=newdata, aes(x=age, y=ht.dummy), col=\"red\", size=3) +  \n  scale_colour_manual(name=\"Model\", values=c(\"Blue\", \"Green\"), labels=c(\"ns(x, df=3)\",\"lspline(x, knots=c(9.5,15.5)\")) + theme(legend.position = c(0.8, 0.2))\n\n\n\n\n\nNatrual spline (3 d.f) compared with restricted cubic spline with knots at 9.5 and 15.5. Both spline approaches impose a linearity constraint at the tails."
  },
  {
    "objectID": "notes/Lec11.DoseResponse.html#summary-comments",
    "href": "notes/Lec11.DoseResponse.html#summary-comments",
    "title": "Flexible Modeling of Continuous Predictors",
    "section": "1.5 Summary comments",
    "text": "1.5 Summary comments\n\nThere are a variety of methods for flexibly modeling continuous covariates\nApproach you use should be pre-specified and based on scientific considerations\nSplines for predictor of interest in an association study\n\nIf you want to be able to write numerical summaries of your results, consider piecewise linear splines\n\nMay provide an adequate summary of your results, particularly for comparison between two knots\nComparison between subgroups separated by a knot location more challenging as you have a change in slope\nFormal test can be conducted to determine if the slope is significantly different on ranges of \\(X\\) (hierarchical models)\n\nBest to pre-specify your knot locations based on relevant scientific meaning\n\nLacking scientific reasons, it is OK to use quantiles as the quantiles are not related to the outcome (unsupervised learning)\nShould not pick knot locations after looking at the outcome unless you have a plan for how to adjust your analysis for making data-driven decision (this type of adjustment is hard to do, so I recommend avoiding it)\n\n\nSpline for estimating means or prediction of new observations\n\nFlexible methods most appropriate in this approach\nBe careful of behavior of your splines at the extremes of your continuous covariates\n\n(Also, don’t extrapolate beyond the range of the date)\n\nDegrees of freedom, or allowed model complexity needs to be considered in terms of the effective sample size for the type of outcome you are studying\n\nEffective sample sizes\n\nLinear regression: Number of subjects\nLogistic regression: Lesser of number of events and number of non-events\nCox regression: Number of events\nPoisson regression: If counts are relatively continuous, more like linear regression. If counts are all small, more like Cox regression\nMultinomial regression: Number of events in smallest categorical outcome level\nOrdinal regression: If few severity events, more like logistic regression. If a wider range of severity, more like linear regression\n\nAllowed model complexity is, roughly, gain one available degree of freedom per every 10-15 of effective sample size\n\nDegrees of freedom can be spent on dummy variable, knot locations, and degree of polynomial\nDegrees of freedom can be saved by constraints, such as the linearity constraint at the tails for natural splines and restricted cubic spline. These are popular splines because the constraint often saves degrees of freedom while improving the generalizability of the fit at the tails of the continuous distribution\nThis is an approximations, and the allowed model complexity can depend on other characteristics of the data too\n\nSplines for effect modification (continuous effect modifier and/or continuous predictor of interest)\n\nChoice of model should be dictated by scientific considerations first\nStatistically, we have limited power to detect effect modification\nLinear terms for modeling effect modification may suffice\n\nFlexible models for a precision variable\n\nOften most of the association of the precision variable with the outcome can be capture in a linear term\nHowever, if justified, can consider a flexible model too\nGoal of a precision variable is to explain residual variability in the outcome after accounting for the predictor of interest (and, possibly, confounders that are included in the model)\n\nFlexible models for confounders\n\nOften it is important to adjust for confounders adequately, so a spline or other flexible function can help\nIf we are not interested in interpreting the effect of the confounder, then we are free to model it a variety of different ways\n\nArgues for flexibly modeling continuous confounder (limited by the effective sample size)\nNote that if the confounder effect really is a straight line, then a cubic spline with linearity constraints on the tails should also fit adequately\n\nThere is not much to be lost and potentially much to be gained by pre-specifying a flexible model for a continuous confounder\n\nIf it is a straight line, a flexibly spline or a linear tearm can capture the association (either would adjust for confounding)\nIf it is not a straight line, a flexible spline can capture the assocation while a linear term cannot (spline would more compoletly adjust for confouding)\n\n\n\n\n\nCode\nggplot(fev, aes(x=height, y=logfev)) + geom_point() + geom_smooth(method=lm, se=FALSE) +\n  geom_smooth(method=lm, formula=y ~ ns(x, df=3), se=FALSE, color=\"Green\")\n\n\n\n\n\nLinear (sraight line) and natural spline (3 d.f.) fits to log of FEV and height. The relationship is approximately a straight line, which can be capture by either assuming a straight line association or allowing for a flexible fit.\n\n\n\n\n\nExample interpretation for linear model with FEV as outcome, smoking the predictor of interest controlling for age and height. There are 654 observations in the dataset, so allowed model complexity is at least 43 degrees of freedom.\n\n\n\nCode\nnrow(fev) / 15\n\n\n[1] 43.6\n\n\nCode\nfev$smoker &lt;- (fev$smoke==\"current smoker\")+0\nfev$male &lt;- (fev$sex==\"male\")+0\nm1 &lt;- lm(fev ~ smoker + ns(age,3) + ns(height,3) + male, data= fev)\ncoeftest(m1, vcov=sandwich(m1))\n\n\n\nt test of coefficients:\n\n                Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept)     1.090121   0.071522 15.2418 &lt; 2.2e-16 ***\nsmoker         -0.147092   0.076797 -1.9153  0.055892 .  \nns(age, 3)1     0.494765   0.102371  4.8330 1.682e-06 ***\nns(age, 3)2     0.506067   0.205409  2.4637  0.014011 *  \nns(age, 3)3     0.807844   0.147608  5.4729 6.343e-08 ***\nns(height, 3)1  1.430324   0.088439 16.1731 &lt; 2.2e-16 ***\nns(height, 3)2  3.262242   0.215641 15.1281 &lt; 2.2e-16 ***\nns(height, 3)3  2.709726   0.158847 17.0588 &lt; 2.2e-16 ***\nmale            0.096504   0.033492  2.8814  0.004091 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nThe betas for the naturally-splined terms (age, height) are difficult to interpret directly. We could test for age and or height effect if that was of scientific interest using the Wald approach.\nFor the predictor of interest, smoker, we have the usual interpretation in terms of holding other covariates constant. “Among subjects of the same age, height, and sex but differing in smoking status…” However, we may be controlling for the confounding effects of age and height effects more completely by using flexible spline functions."
  },
  {
    "objectID": "notes/Lec04.Logistic.html",
    "href": "notes/Lec04.Logistic.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "Types of variables\n\nBinary data: e.g. sex, death\nNominal (unordered categorical) data: e.g. race, martial status\nOrdinal (ordered categorical data): e.g. cancer stage, asthma severity\nQuantitative data: e.g. age, blood pressure\nRight censored data: e.g. time to death\n\nThe measures used to summarize and compare distributions vary according to the type of variable\n\nMeans: Binary, quantitative\nMedians: Ordered, quantitative, censored\nProportions: Binary, nominal, ordinal\nOdds: Binary, nominal, ordinal\nHazards: Censored\n\nWhich regression model you choose to use is based on the parameter being compared across groups\n\n\n\n\n\nParameter\nApproach\n\n\n\n\nMeans\nLinear regression\n\n\nGeometric means\nLinear regression on log scale\n\n\nOdds\nLogistic regression\n\n\nRates\nPoisson regression\n\n\nHazards\nProportional Hazards (Cox) regression\n\n\n\n\n\nGeneral notation for variables and parameters\n\n\n\n\n\n\\(Y_i\\)\nResponse measured on the \\(i\\)th subject\n\n\n\\(X_i\\)\nValue of the predictor measured on the \\(i\\)th subject\n\n\n\\(\\theta_i\\)\nParameter summarizing distribution of \\(Y_i | X_i\\)\n\n\n\n\n\nThe parameter (\\(\\theta_i\\)) might be the mean, geometric mean, odds, rate, instantaneous risk of an event (hazard), etc.\nIn linear regression on means, \\(\\theta_i = E[Y_i | X_i]\\)\nChoice of correct \\(\\theta_i\\) should be based on scientific understanding of problem\nGeneral notation for simple regression model\n\n\\[g(\\theta_i) = \\beta_0 + \\beta_1 \\times X_i\\]\n\nGeneral notation for regression model with one predictor\n\n\n\n\\(g( )\\)\nLink function used for modeling\n\n\n\\(\\beta_0\\)\nIntercept\n\n\n\\(\\beta_1\\)\nSlope for predictor \\(X\\)\n\n\n\n\n\nThe link function is often either the identity function (for modeling means) or log (for modeling geometric means, odds, hazards)\n\nIdentity function: \\(f(x) = x\\)\n\n\n\n\n\nBorrowing information\n\nUse other groups to make estimates in groups with sparse data\nIntuitively, 67 and 69 year olds would provide some relevant information about 68 year olds\nAssuming a straight line relationship tells us about other, even more distant, individuals\nIf we do not want to assume a straight line, we may only want to borrow information from nearby groups\n\nDefining “Contrasts”\n\nDefine a comparison across groups to use when answering scientific questions\nIf the straight line relationship holds, the slope is the difference in parameter between groups differing by 1 unit in \\(X\\)\nIf a non-linear relationship in parameter, the slope is still the average difference in parameter between groups differing by 1 unit in \\(X\\)\nSlope is a (first order or linear) test for trend in the parameter\nStatistical jargon: “a contrast” across groups\n\nThe major difference between different regression models is the interpretation of the parameters\n\nHow do I want to summarize the outcome?\nMean, geometric mean, odds, hazard\n\nHow do I want to compare groups?\n\nDifference, ratio\n\nAnswering these two simple questions provides a starting road-map as to which regression model to choose\nIssues related to the inclusion of covariates remains the same\n\nAddress the scientific question: Predictor of interest, effect modification\nAddress confounding\nIncrease precision"
  },
  {
    "objectID": "notes/Lec04.Logistic.html#general-regression-setting",
    "href": "notes/Lec04.Logistic.html#general-regression-setting",
    "title": "Logistic Regression",
    "section": "",
    "text": "Types of variables\n\nBinary data: e.g. sex, death\nNominal (unordered categorical) data: e.g. race, martial status\nOrdinal (ordered categorical data): e.g. cancer stage, asthma severity\nQuantitative data: e.g. age, blood pressure\nRight censored data: e.g. time to death\n\nThe measures used to summarize and compare distributions vary according to the type of variable\n\nMeans: Binary, quantitative\nMedians: Ordered, quantitative, censored\nProportions: Binary, nominal, ordinal\nOdds: Binary, nominal, ordinal\nHazards: Censored\n\nWhich regression model you choose to use is based on the parameter being compared across groups\n\n\n\n\n\nParameter\nApproach\n\n\n\n\nMeans\nLinear regression\n\n\nGeometric means\nLinear regression on log scale\n\n\nOdds\nLogistic regression\n\n\nRates\nPoisson regression\n\n\nHazards\nProportional Hazards (Cox) regression\n\n\n\n\n\nGeneral notation for variables and parameters\n\n\n\n\n\n\\(Y_i\\)\nResponse measured on the \\(i\\)th subject\n\n\n\\(X_i\\)\nValue of the predictor measured on the \\(i\\)th subject\n\n\n\\(\\theta_i\\)\nParameter summarizing distribution of \\(Y_i | X_i\\)\n\n\n\n\n\nThe parameter (\\(\\theta_i\\)) might be the mean, geometric mean, odds, rate, instantaneous risk of an event (hazard), etc.\nIn linear regression on means, \\(\\theta_i = E[Y_i | X_i]\\)\nChoice of correct \\(\\theta_i\\) should be based on scientific understanding of problem\nGeneral notation for simple regression model\n\n\\[g(\\theta_i) = \\beta_0 + \\beta_1 \\times X_i\\]\n\nGeneral notation for regression model with one predictor\n\n\n\n\\(g( )\\)\nLink function used for modeling\n\n\n\\(\\beta_0\\)\nIntercept\n\n\n\\(\\beta_1\\)\nSlope for predictor \\(X\\)\n\n\n\n\n\nThe link function is often either the identity function (for modeling means) or log (for modeling geometric means, odds, hazards)\n\nIdentity function: \\(f(x) = x\\)\n\n\n\n\n\nBorrowing information\n\nUse other groups to make estimates in groups with sparse data\nIntuitively, 67 and 69 year olds would provide some relevant information about 68 year olds\nAssuming a straight line relationship tells us about other, even more distant, individuals\nIf we do not want to assume a straight line, we may only want to borrow information from nearby groups\n\nDefining “Contrasts”\n\nDefine a comparison across groups to use when answering scientific questions\nIf the straight line relationship holds, the slope is the difference in parameter between groups differing by 1 unit in \\(X\\)\nIf a non-linear relationship in parameter, the slope is still the average difference in parameter between groups differing by 1 unit in \\(X\\)\nSlope is a (first order or linear) test for trend in the parameter\nStatistical jargon: “a contrast” across groups\n\nThe major difference between different regression models is the interpretation of the parameters\n\nHow do I want to summarize the outcome?\nMean, geometric mean, odds, hazard\n\nHow do I want to compare groups?\n\nDifference, ratio\n\nAnswering these two simple questions provides a starting road-map as to which regression model to choose\nIssues related to the inclusion of covariates remains the same\n\nAddress the scientific question: Predictor of interest, effect modification\nAddress confounding\nIncrease precision"
  },
  {
    "objectID": "notes/Lec04.Logistic.html#simple-logistic-regression",
    "href": "notes/Lec04.Logistic.html#simple-logistic-regression",
    "title": "Logistic Regression",
    "section": "2 Simple Logistic Regression",
    "text": "2 Simple Logistic Regression\n\n2.1 Uses of logistic regression\n\nUse logistic regression when you want to make inference about the odds\n\nAllows for continuous (or multiple) grouping variables\nIs OK with binary grouping variables too\nCompares odds of responses across groups using ratios\n\n“Odds ratio”\n\n\nBinary response variable\nWhen using regression with binary response variables, we typically model the (log) odds using logistic regression\n\nConceptually there should be no problem modeling the proportion (which is the mean of the distribution)\nHowever, there are several technical reasons why we do not use linear regression very often with binary responses\nWhy not use linear regression for binary responses?\n\nMany misconceptions about the advantages and disadvantages of analyzing the odds\n\nReasons I consider valid: Scientific basis\n\nUses of odds ratios in case control studies\nPlausibility of linear trends and no effect modifiers\n\nReasons I consider valid: Statistical basis\n\nThere is a mean variance relationship (if not using robust SE) that can be incorporate in the logistic regression model\n\n\n\n\n\n2.2 Reasons to use logistic regression\n\nFirst (scientific) reason: Case-Control Studies\n\nStudying a rare disease, so we do study in reverse\n\ne.g. find subjects with cancer (and suitable controls) and then ascertain exposure of interest\nEstimate distribution of the “effect” across groups defined by “cause”\nProportion (or odds) of smokers among people with or without lung cancer\n\n\n\nCase-Control or Cohort 2x2 Table. In the Case-Control design, the total number of subjects with Cancer \\((a+c)\\) and without cancer \\((b+d)\\) are fixed by design.\n\n\n\nLung Cancer +\nLung Cancer -\n\n\nSmoker\na\nb\n\n\nNon-Smoker\nc\nd\n\n\n\n\nIn contrast, a cohort study samples by exposure (smoking) and then estimates the distribution of the effect in exposure groups\nIn a case-control study, we cannot estimate prevalence (without knowing selection probabilities)\n\ne.g. if doing a 1:1 case-control study, \\((a+c) = b+d\\) so it would look like \\(50\\%\\) of the subjects have cancer\n\nOdds ratios are estimable in either case-control or cohort sampling scheme\n\nCohort study: Odds of cancer among smoker compared to odds of cancer among nonsmokers\nCase-control study: Odds of smoking among cancer compared to odds of smoking among non-cancer\n\nMathematically, these two odds ratios are the same\n\n\n\nOdds ratios are easy to interpret when investigating rare events\n\nOdds = prob / (1 - prob)\nFor rare events, (1 - prob) is approximately 1\n\nOdds is approximately the probability\nOdds ratios are approximately risk ratios\n\nCase-control studies usually used when events are rare\n\n\nSecond (scientific) reason: Linearity\n\nProportions are bounded by 0 and 1\nIt is thus unlikely that a straight line relationship would exists between a proportion and a predictor\n\nUnless the predictor itself is bounded\nOtherwise, there eventually must be a threshold above which the probability does not increase (or only increases a little)\n\n\n\n\n\nCode\nexpit &lt;- function(x) {exp(x)/(1+exp(x))}\nplot(function(x) expit(x), -4,4, ylab=\"Probabilty\", xlab=\"Predictor\")\n\n\n\n\n\nLogistic function will bound probabilities between 0 and 1\n\n\n\n\n\nThird (scientific) reason: Effect modification\n\nThe restriction on ranges for probabilities makes it likely that effect modification must be present with proportions\nExample: Is the association between 2-year relapse rates and having a positive scan modified by gender?\n\nWomen relapse 40% of the time when the scan is negative, and 95% of the time when the scan is positive (an increase of 55%)\n\nIf men relapse 75% of the time when the scan is negative, then a positive scan can increase the relapse rate to at most 100%, which is only a 25% increase\n\n\n\nProportions\n\n\n\n\n\nWomen\nMen\n\n\nNegative Scan\n40%\n75%\n\n\nPositive Scan\n95%\n(up to 100%)\n\n\n\n\n\n\n\nDifference\n55%\nUp to 25%\n\n\nRatio\n1.64\n\\(\\leq 1.33\\)\n\n\n\nWith the odds, the association can hold without effect modification\n\n\n\nOdds\n\n\n\n\n\nWomen\nMen\n\n\nNegative Scan\n0.67\n3\n\n\nPositive Scan\n19\n(up to \\(\\infty\\))\n\n\n\n\n\n\n\nRatio\n28.5\n\\(&lt; \\infty\\)\n\n\n\n\n\n\nIf the o dds of positive scan in men was 85.5, then the odds ratio would be exactly 28.5 (no effect modification)\nFourth (statistics) reason:\n\nClassical linear regression requires equal variances across each predictor group\nBut, with binary data, the variance within a group depends on the mean\nFor binary \\(Y\\), \\(E(Y) = p\\) and \\(Var(Y) = p(1-p)\\)\nWith robust standard errors, the mean-variance relationship is not a major problem. However, a logistic model that correctly models the mean-variance relationship will be more efficient.\n\n\n\n\n2.3 The simple logistic regression model\n\nModeling the odds of binary response variable \\(Y\\) on predictor \\(X\\)\n\nDistribution: \\(\\textrm{Pr}(Y_i = 1) = p_i\\)\nModel: \\(\\textrm{logit}(p_i) = \\textrm{log}\\left(\\frac{p_i}{1-p_i}\\right) = \\beta_0 + \\beta_1 \\times X_i\\)\nWhen \\(X_i = 0\\): log odds = \\(\\beta_0\\)\nWhen \\(X_i = x\\): log odds = \\(\\beta_0 + \\beta_1 \\times x\\)\nWhen \\(X_i = x+1\\): log odds = \\(\\beta_0 + \\beta_1 \\times x + \\beta_1\\)\n\nTo interpret as odds, exponentiate the regression parameters\n\nDistribution: \\(\\textrm{Pr}(Y_i = 1) = p_i\\)\nModel: \\(\\frac{p_i}{1-p_i} = \\exp(\\beta_0 + \\beta_1 \\times X_i) = e^{\\beta_0} \\times e^{\\beta_1 \\times X_i}\\)\nWhen \\(X_i = 0\\): odds = \\(e^{\\beta_0}\\)\nWhen \\(X_i = x\\): odds = \\(e^{\\beta_0} \\times e^{\\beta_1 \\times x}\\)\nWhen \\(X_i = x+1\\): odds = \\(e^{\\beta_0} \\times e^{\\beta_1 \\times x} \\times e^{\\beta_1}\\)\n\nTo interpret as proportions (remember proportion = odds / (1 + odds))\n\nDistribution: \\(\\textrm{Pr}(Y_i = 1) = p_i\\)\nModel: \\(p_i = \\frac{e^{\\beta_0} e^{\\beta_1 \\times X_i}}{1 + e^{\\beta_0} e^{\\beta_1 \\times X_i}}\\)\nWhen \\(X_i = 0\\): \\(p_i = \\frac{e^{\\beta_0}}{1 + e^{\\beta_0}}\\)\n\nWhen \\(X_i = x\\): \\(p_i = \\frac{e^{\\beta_0} e^{\\beta_1 \\times x}}{1 + e^{\\beta_0} e^{\\beta_1 \\times x}}\\)\nWhen \\(X_i = x+1\\): \\(p_i = \\frac{e^{\\beta_0} e^{\\beta_1 \\times x} e^{\\beta_1}}{1 + e^{\\beta_0} e^{\\beta_1 \\times x}e^{\\beta_1}}\\)\n\nMost common interpretations found by exponentiating the coefficients\n\nOdds when predictor is 0 found by exponentiating the intercept: \\(\\exp(\\beta_0)\\)\nOdds ratio between groups differing in the values of the predictor by 1 unit found by exponentiating the slope: \\(\\exp(\\beta_1)\\)\n\nStata commands\nlogit respvar predvar, [robust]\n\nProvides regression parameter estimates an inference on the log odds scale (both coefficients with CIs, SEs, p-values)\n\nlogistic respvar predvar, [robust]\n\nProvides regression parameter estimates and inference on the odds ratio scale (only slope with CIs, SEs, p-values)\n\nR Commands\n\nWith rms package, lrm(respvar ~ predvar, ...)\nIn general, glm(respvar ~ predvar, family=“binomial”)"
  },
  {
    "objectID": "notes/Lec04.Logistic.html#example-survival-on-the-titanic-and-age",
    "href": "notes/Lec04.Logistic.html#example-survival-on-the-titanic-and-age",
    "title": "Logistic Regression",
    "section": "3 Example: Survival on the Titanic and Age",
    "text": "3 Example: Survival on the Titanic and Age\n\nDataset at https://biostat.app.vumc.org/DataSets\nDescribes the survival status of individual passengers on the Titanic\nData on age available for many, but not all, subjects (data continually being updated)\nResponse variables is Survival\n\nBinary variable: 1=Survived, 0=Died\n\nPredictor variable is Age\n\nContinuous grouping variable\n\nPossibly different age effects by sex (effect modification by sex)\n\n\n3.1 Descriptive Plots\n\n\nCode\n# | fig-cap: Missing data patterns in the Tianic dataset\nlibrary(rms)\ntitanic &lt;- read.csv(file=\"data/titanic3.csv\")\nplot(naclus(titanic)) # study patterns of missing values\n\n\n\n\n\n\n\nCode\n# | fig-cap: Scatterplot of age versus survival in the Titanic data with lowess smooth.  This simple plot is not very useful because survival is either 0 or 1, making it hard to visualize any trends.\nggplot(titanic, aes(x=age, y=survived)) + geom_jitter(width=0, height=.02, alpha=.5) + geom_smooth()\n\n\n\n\n\n\n\nCode\n# | fig-cap: Age versus survival by sex in the Titanic data by age using a super smoother.  The trends are clearer with this smoothing approach.\nwith(titanic, \n     plsmo(age, survived, group=sex, datadensity=T, ylab=\"Survived (1=Yes, 0=No)\", xlab=\"Age (years)\")\n)\n\n\n\n\n\n\n\n\n\nComments on the plots\n\nAge is missing for many subjects, which we will not worry about in the following analysis\nThe simple scatterplot, even with superimposes lowess smooth, is worthless. I have jittered the point and altered their opacity to help visualize overlapping point.\nMore advanced plotting available in R (in this case, the plsmo() function) can help to visualize the data\n\n\n\n\n3.2 Regression Model\n\nRegression model for survival on age (ignoring possible effect modification for now)\nAnswer question by assessing linear trends in log odds of survival by age\nEstimate the best fitting line to log odds of survival within age groups\n\n\\[\\textrm{logodds}(\\textrm{Survival} | \\textrm{Age}) = \\beta_0 + \\beta_1 \\times \\textrm{Age}\\]\n\nAn association will exist if the slope \\(\\beta_1\\) is nonzero\nIn that case, the odds (and probability) of survival will be different across different age groups\n\n\n\nCode\nm.titanic &lt;- glm(survived ~ age, data=titanic, family = \"binomial\")\nsummary(m.titanic)\n\n\n\nCall:\nglm(formula = survived ~ age, family = \"binomial\", data = titanic)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept) -0.136534   0.144715  -0.943   0.3454  \nage         -0.007899   0.004407  -1.792   0.0731 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1414.6  on 1045  degrees of freedom\nResidual deviance: 1411.4  on 1044  degrees of freedom\n  (263 observations deleted due to missingness)\nAIC: 1415.4\n\nNumber of Fisher Scoring iterations: 4\n\n\n\\(\\textrm{logodds}(\\textrm{Survival} | \\textrm{Age}) = -0.1365 - 0.007899 \\times \\textrm{Age}\\)\n\nGeneral interpretation\n\nIntercept is labeled “(Intercept)”\nSlope for age is labeled “age”\n\nInterpretation of intercept\n*Estimated log odds for newborns (age=0) is \\(-0.136534\\)\n\nOdds of survival for newborns is \\(e^{-0.136534} = 0.8724\\)\nProbability of survival\n\nProb = odds / (1 + odds)\n\\(0.8724 / (1 + .8724) = 0.4659\\)\n\n\n\n\n\nCode\npredict(m.titanic, newdata=data.frame(age=0), type='response')\n\n\n        1 \n0.4659194 \n\n\n\nInterpretation of slope\n\nEstimate difference in the log odds of survival for two groups differing by one year in age is \\(-0.0078985\\)\nThis estimate averages over males and females\nOlder groups tend to have lower log odds\nOdds Ratio: \\(e^{-0.0078985} = 0.9921\\)\nFor five year difference in age: \\(e^{-0.0078985 \\times 5} = 0.9612\\)\nIn Stata use “lincom age, or” or “lincom 5*age, or”\n\n\nNote that if the straight line relationship does not hold true, we interpret the slope as an average difference in the log odds of survival per one year difference in age\nThere are several ways to get the odds ratio and confidence interval in R\n\n\nCode\n# The coefficient and confidence interval (on the log-odds scale)\ncoef(m.titanic)[\"age\"]\n\n\n         age \n-0.007898504 \n\n\nCode\nconfint.default(m.titanic, \"age\")\n\n\n          2.5 %       97.5 %\nage -0.01653509 0.0007380868\n\n\nCode\n# Odds ratio for age and confidence interval for age (1 year increase)\nexp(coef(m.titanic)[\"age\"])\n\n\n      age \n0.9921326 \n\n\nCode\nexp(confint.default(m.titanic, \"age\"))\n\n\n        2.5 %   97.5 %\nage 0.9836009 1.000738\n\n\nCode\n# Odds ratio for age and confidence interval for age (5 year increase)\nexp(5*coef(m.titanic)[\"age\"])\n\n\n      age \n0.9612771 \n\n\nCode\nexp(5*confint.default(m.titanic, \"age\"))\n\n\n        2.5 %   97.5 %\nage 0.9206499 1.003697\n\n\n\nUsing finalfit to create a nicer output table of the coefficients and confidence intervals\n\nFor finalfit to use a logistic regression model by default, survived must be defined as a factor variable with two levels\n\n\n\n\nCode\nmykable = function(x){\n  knitr::kable(x, row.names = FALSE, align = c(\"l\", \"l\", \"r\", \"r\", \"r\", \"r\", \"r\", \"r\", \"r\"),\n               booktabs=TRUE)\n}\n\n\nlibrary(finalfit)\nlibrary(dplyr)\nlibrary(rms)\nexplanatory = c(\"age\")\n\ntitanic$survived.factor &lt;- factor(titanic$survived, levels=0:1, labels=c(\"Died\",\"Survived\"))\ndependent = 'survived.factor'\n\nlabel(titanic$age) &lt;- \"Age (years)\"\ntitanic %&gt;% \n  finalfit(dependent, explanatory) %&gt;% mykable()\n\n\n\n\n\n\n\n\n\n\n\n\n\nDependent: survived.factor\n\nDied\nSurvived\nOR (univariable)\nOR (multivariable)\n\n\n\n\nAge (years)\nMean (SD)\n30.5 (13.9)\n28.9 (15.1)\n0.99 (0.98-1.00, p=0.073)\n0.99 (0.98-1.00, p=0.073)\n\n\n\n\n\n\n\n3.3 Comments on Interpretation\n\nThe slope for age is expressed as a difference in group means, not the difference due to aging. We did not do a longitudinal study in which repeated measurements were taken on the same subject.\nIf the group log odds are truly linear, then the slope has an exact interpretation as the change in survival due to a one year change in (any) age\nOtherwise, the slope estimates the first order trend of the sample data and we should not treat the estimates of group odds or probabilities as accurate\nIt is difficult to see in the above example, but the CIs around the odds ratios are not symmetric\n\n(Symmetric) CIs are calculated on the log odds scale, and then transformed to the odds scale by expoenentiating the lower and upper limits of the CI\n\n“From logistic regression analysis, we estimate that for each 5 year difference in age, the odds of survival on the Titanic decreased by 3.9%, though this estimate is not statistically significant (\\(p = 0.07\\)). A 95% CI suggests that this observation is not unusual if a group that is five years older might have an odds of survival that was anywhere between 7.9% lower and 0.4% higher than the younger group.”\nThe confidence interval and statistical test given in the output is called a Wald test. Other tests (Score, Likelihood Ratio) are also possible.\n\nAll tests are asymptotically equivalent\nThe Wald test is easiest to obtain, but generally performs the poorest in small sample sizes\nThe Likelihood Ratio test performs the best in small samples. We will discuss it later, including how to obtain the test using post-estimation commands.\nThe Score test is not bad in small samples, but is often hard to obtain from software. It is exactly equal to the Chi-squared test for binary outcomes and categorical predictors.\n\n\n\n3.3.1 Bayesian Estimates and Interpretation\n\nBayesian approach to the logistic model requires specifying the model, prior distributions, and the likelihood\nThe model\n\ne.g. a model for the log odds of survival that is linear in the parameters with an intercept and slope for age\n\nPrior distributions on parameters\n\nFor the simple logistic regression model, we have parameters \\(\\beta_0\\), and \\(\\beta_1\\).\nFor now, we will use default prior distributions that are are intended to be weakly informative in that they provide moderate regularization and help stabilize computation. See the STAN documentation for more details\nAppropriate priors can be based on scientific considerations\nSensitivity analyses can evaluate the the robustness of finding to different prior assumptions\n\nThe likelihood\n\nFor a binomial GLM the likelihood for one observation \\(y\\) can be written as a conditionally binomial probability mass function\n\n\n\\[\\binom{n}{y} \\pi^{y} (1 - \\pi)^{n - y},\\]\n\n\\(n\\) is the known number of trials, \\(\\pi = g^{-1}(\\eta)\\) is the probability of success and \\(\\eta = \\alpha + \\mathbf{x}^\\top \\boldsymbol{\\beta}\\) is a linear predictor\n\n\n\nFor a sample of size \\(N\\), the likelihood of the entire sample is the product of \\(N\\) individual likelihood contributions.\n\n\n\nBecause \\(\\pi\\) is a probability, for a binomial model the link function \\(g\\) maps between the unit interval (the support of \\(\\pi\\)) and the set of all real numbers \\(\\mathbb{R}\\). When applied to a linear predictor \\(\\eta\\) with values in \\(\\mathbb{R}\\), the inverse link function \\(g^{-1}(\\eta)\\) therefore returns a valid probability between 0 and 1.\nThe two most common link functions used for binomial GLMs are the\n\nlogit and\nprobit\n\nWith the logit (or log-odds) link function \\(g(x) = \\ln{\\left(\\frac{x}{1-x}\\right)}\\), the likelihood for a single observation becomes\n\n\\[\\binom{n}{y}\\left(\\text{logit}^{-1}(\\eta)\\right)^y\n\\left(1 - \\text{logit}^{-1}(\\eta)\\right)^{n-y} =\n\\binom{n}{y} \\left(\\frac{e^{\\eta}}{1 + e^{\\eta}}\\right)^{y}\n\\left(\\frac{1}{1 + e^{\\eta}}\\right)^{n - y}\\]\n\nWith the probit link function \\(g(x) = \\Phi^{-1}(x)\\) yields the likelihood\n\n\\[\\binom{n}{y} \\left(\\Phi(\\eta)\\right)^{y}\n\\left(1 - \\Phi(\\eta)\\right)^{n - y},\\]\nwhere \\(\\Phi\\) is the CDF of the standard normal distribution.\n\nOutput from Bayesian logistic regression using logit link function\n\n\n\nCode\nlibrary(rstanarm)\nlibrary(bayesplot)\n\nfit2 &lt;- stan_glm(survived ~ age,\n                 data=titanic, family=binomial(),\n                 seed=1234,\n                 refresh=0)\nsummary(fit2, digits=4, prob=c(.025, .5, .975))\n\n\n\nModel Info:\n function:     stan_glm\n family:       binomial [logit]\n formula:      survived ~ age\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 1046\n predictors:   2\n\nEstimates:\n              mean    sd      2.5%    50%     97.5%\n(Intercept) -0.1343  0.1459 -0.4135 -0.1336  0.1560\nage         -0.0079  0.0044 -0.0169 -0.0078  0.0004\n\nFit Diagnostics:\n           mean   sd     2.5%   50%    97.5%\nmean_PPD 0.4089 0.0217 0.3652 0.4092 0.4512 \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse   Rhat   n_eff\n(Intercept)   0.0029 0.9994 2533 \nage           0.0001 0.9996 2570 \nmean_PPD      0.0004 1.0004 3073 \nlog-posterior 0.0284 1.0047 1340 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\n\n\nAnd a summary of the prior distributions used\n\n\n\nCode\nprior_summary(fit2, digits = 2)\n\n\nPriors for model 'fit2' \n------\nIntercept (after predictors centered)\n ~ normal(location = 0, scale = 2.5)\n\nCoefficients\n  Specified prior:\n    ~ normal(location = 0, scale = 2.5)\n  Adjusted prior:\n    ~ normal(location = 0, scale = 0.17)\n------\nSee help('prior_summary.stanreg') for more details\n\n\n\nInterpretation\n\nSlope for age is of primary scientific importance\nA priori we assume that no association between age and survival. Specifically, we assumed a Normal prior with location (mean) of 0 and scale (standard devation) of 0.17 for \\(\\beta_1\\).\nConditional on the data, we estimate that for every 1 year increase in age, the log odds of decreases by -0.0079 (95% credible interval -0.0169 to 0.0004).\nTo obtain the posterior odds ratio and 95% credible intervals, some additional commands are needed\n\n\n\n\nCode\n# 1 year change in age\nexp(coef(fit2)[\"age\"]) # Posterior Odds Ratio\n\n\n      age \n0.9922461 \n\n\nCode\nexp(posterior_interval(fit2, prob = 0.95)) # 95% credible interval\n\n\n                 2.5%    97.5%\n(Intercept) 0.6613115 1.168821\nage         0.9832330 1.000353\n\n\nCode\n# 5 year increase in age\nexp(5*coef(fit2)[\"age\"]) # Posterior Odds Ratio\n\n\n      age \n0.9618272 \n\n\nCode\nexp(5*posterior_interval(fit2, prob = 0.95))[2,] # 95% credible interval\n\n\n     2.5%     97.5% \n0.9189294 1.0017684 \n\n\nCode\n# Could also flip (invert) these odds ratios to interpret as decrease in age being associated with increased survival\n# 5 year decrease in age\n1/exp(5*coef(fit2)[\"age\"]) # Posterior Odds Ratio\n\n\n     age \n1.039688 \n\n\nCode\n1/exp(5*posterior_interval(fit2, prob = 0.95))[2,] # 95% credible interval\n\n\n     2.5%     97.5% \n1.0882228 0.9982347 \n\n\n\nThe association between age and probability of survival was estimated using a Bayesian logistic regression model. The model did not adjust for other covariates and assumed a logit link function and Binomial likelihood. We assumed a weakly informative prior distribution for the log odds of survival given age (Normal prior with mean 0 and scale 0.17 for \\(\\beta_1\\)). Conditional on the data, the posterior mean estimate indicates that comparing two subjects who differ in age by 5 years, the younger subject has a 1.04 fold increased odds of survival compared to the older subject. A 95% credible for this posterior odds ratio is from 1.00 to 1.09.\nNote that this model does not consider gender, so it is averaging over the males and females. We will revisits this analysis where the age effect is analyzed separately in males and females."
  },
  {
    "objectID": "notes/Lec04.Logistic.html#inference-with-logistic-regression",
    "href": "notes/Lec04.Logistic.html#inference-with-logistic-regression",
    "title": "Logistic Regression",
    "section": "4 Inference with Logistic Regression",
    "text": "4 Inference with Logistic Regression\n\nThe ideas of Signal and Noise found in simple linear regression do not translate well to logistic regression\nWe do not tend to quantify an error distribution with logistic regression\nValid statistical inference (CIs, p-values) about associations requires three general assumptions\nAssumption 1: Approximately Normal distributions for the parameter estimates\n\nLarge N\nNeed for either robust standard errors or classical logistic regression\nDefinition of large depends on the underlying probabilities (odds)\nRecall the rule of thumb for chi-squared tests based on the expected number of events\n\nAssumption 2: Assumptions about the independence of observations\n\nClassical regression: Independence of all observation\nRobust standard errors: Correlated observations within identified clusters\n\nAssumption 3: Assumptions about variance of observations within groups\n\nClassical regression: Mean-variance relationship for binary data\n\nClassical logistic regression estimates SE using model based estimates\nHence in order to satisfy this requirement, linearity of log odds across groups must hold\n\nRobust standard errors\n\nAllows unequal variance across groups\nHence, do not need linearity of log odds across groups to hold\nValid statistical inference (CIs, p-values) about odds of response in specific groups requires a further assumption\n\n\nAssumption 4: Adequacy of the linear model\n\nIf we are trying to borrow information about the log odds from neighboring groups, and we are assuming a straight line relationship, the straight line needs to be true\n\nNeeded for either classical or robust standard errors\nNote that we can model transformations of the measured predictor if we feel a straight line is not appropriate\n\nInference about individual observations (prediction intervals, P-values) in specific groups requires no further assumptions because we have binary data\n\nFor binary data, if we know the mean (proportion), we know everything about the distribution including the variance\nThis differs from linear regression where we can have a correct model for the mean, but the assumption about the error distribution (Normality, homoskedasticity) can be incorrect\n\n\n\n\n4.1 Interpreting “Positive” Results\n\nSlope is statistically different from 0 using robust standard errors\nObserved data is atypical of a setting with no linear trend in odds of response across groups\nData suggests evidence of a trend toward larger (or smaller) odds in groups having larger values of the predictor\n(To the extent the data appears linear, estimates of the group odds or probabilities will be reliable)\n\n\n\n4.2 Interpreting “Negative” Results\n\nMany possible reasons why the slope is not statistically different from 0 using robust standard errors\n\nThere may be no association between the response and predictor\nThere may be an association in the parameter considered, but the best fitting line has zero slope\nThere may be a first order trend in the log odds, but we lacked the precision to be confident that it truly exists (a type II error)"
  },
  {
    "objectID": "notes/Lec04.Logistic.html#example-analysis-revisited-effect-modification",
    "href": "notes/Lec04.Logistic.html#example-analysis-revisited-effect-modification",
    "title": "Logistic Regression",
    "section": "5 Example analysis revisited: Effect Modification",
    "text": "5 Example analysis revisited: Effect Modification\n\nRecall in our Titanic example that the effect of age appeared to differ by sex\n\nWe ignored this difference earlier, so our estimated age effect was a (weighted) average of the age effect in males and the age effect in female\nHere is the plot again describing the trends we see in survival by age and sex (using plsmo).\n\n\n\n\nCode\n# | fig-cap: Age versus survival by gender in the Titanic data by age using a super smoother.\nwith(titanic, \n     plsmo(age, survived, group=sex, datadensity=T, ylab=\"Survived (1=Yes, 0=No)\", xlab=\"Age (years)\")\n)\n\n\n\n\n\n\nWe could describe the observed differences in two way, both being correct\n\nGender modifies the age effect\n\nIn males, the probability of survival worsened with age while in female the probability of survival improved with age\nEmphasizes that the female age slope is positive while the male age slope is negative\n\nAge modifies the gender effect\n\nThe survival rates of male and females were more similar at younger ages than older ages\nCould specify the odds ratio of survival comparing females to males at specific ages\n\n\n\n\n5.1 Stratified analysis by sex\n\nThe log odds of survival in females\n\n\n\nCode\nfit.titanic.female &lt;- glm(survived ~ age, data=titanic, subset=sex==\"female\")\nfit.titanic.female\n\n\n\nCall:  glm(formula = survived ~ age, data = titanic, subset = sex == \n    \"female\")\n\nCoefficients:\n(Intercept)          age  \n   0.637644     0.004006  \n\nDegrees of Freedom: 387 Total (i.e. Null);  386 Residual\n  (78 observations deleted due to missingness)\nNull Deviance:      72.25 \nResidual Deviance: 70.93    AIC: 447.7\n\n\n\nThe log odds of survival in males\n\n\n\nCode\nfit.titanic.male &lt;- glm(survived ~ age, data=titanic, subset=sex==\"male\")\nfit.titanic.male\n\n\n\nCall:  glm(formula = survived ~ age, data = titanic, subset = sex == \n    \"male\")\n\nCoefficients:\n(Intercept)          age  \n   0.316337    -0.003635  \n\nDegrees of Freedom: 657 Total (i.e. Null);  656 Residual\n  (185 observations deleted due to missingness)\nNull Deviance:      107.3 \nResidual Deviance: 105.5    AIC: 669.1\n\n\n\n5.1.1 Odds ratios and confidence intervals for age effect by sex\n\nConsider a 5 year change in age\n\n\n\nCode\n# Females\nexp(5*coef(fit.titanic.female)[\"age\"])\n\n\n     age \n1.020234 \n\n\nCode\nexp(5*confint.default(fit.titanic.female,\"age\"))\n\n\n       2.5 %  97.5 %\nage 1.005398 1.03529\n\n\nCode\n# Males\nexp(5*coef(fit.titanic.male)[\"age\"])\n\n\n      age \n0.9819903 \n\n\nCode\nexp(5*confint.default(fit.titanic.male,\"age\"))\n\n\n        2.5 %    97.5 %\nage 0.9715019 0.9925919\n\n\n\n\n\n5.2 Effect modification using interaction terms\n\nInstead of fitting two separate models for male and females, we could estimate all parameters in a single regression model\n\nLet \\(p_i\\) be the probability of survival for passenger \\(i\\) and \\(\\textrm{logit}(p)= \\textrm{log}\\left(\\frac{p}{1-p}\\right)\\)\nLet \\(X_{1i}\\) be the age of subject \\(i\\)\nLet \\(X_{2i}\\) be an indicator variable for female sex. \\(X_{2i}=1\\) if a subject is female and \\(X_{2i} = 0\\) if a subject is male\n\n\n\\[\\textrm{logit}(p_i | X_{1i},X_{2i}) = \\beta_0 + \\beta_1 * X_{1i} + \\beta_{2i} * X_{2i} + \\beta_3*X_{1i}*X_{2i}\\]\n\nIn males, \\(X_{2i} = 0\\), this model reduces to\n\n\\[\\textrm{logit}(p_i | X_{1i}, X_{2i}=0) = \\beta_0 + \\beta_1 * X_{1i}\\]\n\nIn females, \\(X_{2i} = 1\\), this model can be expressed as\n\n\\[\\textrm{logit}(p | X_{1i}, X_{2i}=1) = (\\beta_0+\\beta_2) + (\\beta_1+\\beta_3) * X_{1i}\\]\n\n\\(\\hat{\\beta_1}\\) is the estimate age effect in males\n\\(\\hat{\\beta_1} + \\hat{\\beta_3}\\) is the estimated age effect in females\n\\(\\hat{\\beta_3}\\) is the estimated difference between the age effect in male and the age effect in females\n\n\n\nCode\n# female has already been defined in the dataset, but if I wanted to create this variable I could do so\ntitanic$female &lt;- (titanic$sex==\"female\")+0\n\nfit.titanic.interact &lt;- glm(survived ~ age + female + age*female, data=titanic)\nsummary(fit.titanic.interact)\n\n\n\nCall:\nglm(formula = survived ~ age + female + age * female, data = titanic)\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.316337   0.037944   8.337 2.39e-16 ***\nage         -0.003635   0.001124  -3.233  0.00126 ** \nfemale       0.321307   0.059757   5.377 9.35e-08 ***\nage:female   0.007641   0.001823   4.192 3.01e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.1693472)\n\n    Null deviance: 252.69  on 1045  degrees of freedom\nResidual deviance: 176.46  on 1042  degrees of freedom\n  (263 observations deleted due to missingness)\nAIC: 1116.9\n\nNumber of Fisher Scoring iterations: 2\n\n\n\nWe can see that the parameter estimates from the interaction model are the same as the estimates from the two stratified models\n\n\n\nCode\n# Interaction model\ncoef(fit.titanic.interact)\n\n\n (Intercept)          age       female   age:female \n 0.316337448 -0.003634770  0.321306890  0.007641206 \n\n\nCode\n# Model fit just on male subjects\ncoef(fit.titanic.male)\n\n\n(Intercept)         age \n 0.31633745 -0.00363477 \n\n\nCode\n# Model fit just on female subjects\ncoef(fit.titanic.female)\n\n\n(Intercept)         age \n0.637644338 0.004006436 \n\n\nCode\n# Linear combinations from the interaction model give the female intercept and age slope\ncoef(fit.titanic.interact)[1] + coef(fit.titanic.interact)[3]\n\n\n(Intercept) \n  0.6376443 \n\n\nCode\ncoef(fit.titanic.interact)[2] + coef(fit.titanic.interact)[4]\n\n\n        age \n0.004006436"
  },
  {
    "objectID": "notes/Lec03.SimpleLinearRegression.html",
    "href": "notes/Lec03.SimpleLinearRegression.html",
    "title": "Simple Linear Regression",
    "section": "",
    "text": "Code\nlibrary(rstanarm)\nlibrary(bayesplot)\nlibrary(finalfit)\nlibrary(dplyr)\nlibrary(rms)\nlibrary(ggplot2)\nlibrary(gglm)"
  },
  {
    "objectID": "notes/Lec03.SimpleLinearRegression.html#general-regression-setting",
    "href": "notes/Lec03.SimpleLinearRegression.html#general-regression-setting",
    "title": "Simple Linear Regression",
    "section": "1 General Regression Setting",
    "text": "1 General Regression Setting\n\n1.1 Two variable setting\n\nMany statistical problems examine the association between two variables\n\nOutcome variable (response variable, dependent variable)\nGrouping variable (covariate, predictor variable, independent variable)\n\nGeneral goal is to compare distribution of the outcome variable across levels of the grouping variable\nGroups are defined by the grouping variable\n\nWithin each group, the grouping variable is constant\nTreatment and control groups\nAge: Continuous (days), broadly categorized (years), or wide categories (e.g. 0-18, 18-65, 65+)\n\nIn intro course, statistical analysis is characterized by two factors\n\nNumber of groups (samples)\nIf subjects in groups are independent\n\nIn the two variable setting, statistical analysis is more generally characterized by the grouping variable. If the grouping variable is\n\nConstant: One sample problem\nBinary: Two sample problem\nCategorical: \\(k\\) sample problem (e.g. ANOVA)\nContinuous: Infinite sample problem (analyzed with regression)\n\nRegression thus extends the one- and two-sample problems up to infinite sample problems\n\nOf course, in reality we never have infinite samples, but models that can handle this case are the ultimate generalization\nContinuous predictors of interest\nContinuous adjustment variables\n\n\n\n\nCode\n  ### Make a cholesterol and age dataframe.  Set the random number seed so everything is reproducible\nset.seed(19)\n\nplotdata &lt;- data.frame(age=c(63, 63, rep(65:80,15), rep(81:85,10), 86,86,86, 87,89, 90, 93, 95, 100),\n                       chol=NA)\nplotdata$chol &lt;- 190 + .5*plotdata$age + rnorm(length(plotdata$age), 0, 15)\n\nggplot(plotdata, aes(x=age, y=chol)) + geom_point() + ylab(\"Cholesterol\") + xlab(\"Age (years)\") + theme_bw()\n\n\n\n\n\nFigure 1: Scatterplot of cholesterol and age. Age is a continuous grouping variable.\n\n\n\n\n\n\n1.2 Regression versus two sample approaches\n\nWith a binary grouping variable, regression models reduce to the corresponding two variable methods\nLinear regression with a binary predictor\n\nt-test, equal variance: Classic linear regression\nt-test, unequal variance: Linear regression with robust standard errors (approximately)\n\nLogistic regression with a binary predictor\n\n(Pearson) Chi-squared test: Score test from logistic regression\n\nProportional odds regression with a binary predictor\n\nWilcoxon Rank Sum Test\n\nCox (proportional hazards) regression with a binary predictor\n\nLog-rank test: Score test from Cox regression\n\n\n\n\n1.3 Guiding principle\n\nEverything can be recast as a regression problem."
  },
  {
    "objectID": "notes/Lec03.SimpleLinearRegression.html#motivating-problem-cholesterol-and-age",
    "href": "notes/Lec03.SimpleLinearRegression.html#motivating-problem-cholesterol-and-age",
    "title": "Simple Linear Regression",
    "section": "2 Motivating Problem: Cholesterol and Age",
    "text": "2 Motivating Problem: Cholesterol and Age\n\n2.1 Definitions\n\nIs there an association between cholesterol and age?\nScientific question: Does aging effect cholesterol?\nStatistical question: Does the distribution of cholesterol differ across age groups?\n\nAcknowledges variability in the response (cholesterol)\nAcknowledges cause-effect relationship is uncertain\nAssociation does not imply causation\n\nAny observed differences could be due to other factors. For example, the association could be with year of birth rather than age.\n\n\nContinuous response variable: Cholesterol\nContinuous grouping variable (predictor of interest): Age\n\nAn infinite number of ages are possible\nWe will not sample every possible age, and may round for simplicity\n\n\n\n\n2.2 Simple Regression Model\n\nAttempt to answer scientific question by assessing linear trends in average cholesterol\nEstimate the best fitting line to average cholesterol within age groups\n\n\\[\nE[\\textrm{Chol} | \\textrm{Age}] = \\beta_0 + \\beta_1 \\times \\textrm{Age}\n\\]\n\nThe expected value of cholesterol given age is modeled using an intercept (\\(\\beta_0\\)) and slope (\\(\\beta_1\\))\n\nAn association exists if the slope is nonzero\nA non-zero slope indicates that the average cholesterol will be different across different age groups\n\n\n\n2.2.1 Descriptive plots of cholesterol and age\n\n\nCode\nggplot(plotdata, aes(x=age, y=chol)) + geom_point() + ylab(\"Cholesterol\") + xlab(\"Age (years)\") + theme_bw() + geom_smooth()\n\n\n\n\n\nFigure 2: Scatterplot of cholesterol and age. Association is described using a lowess smooth line.\n\n\n\n\n\n\nCode\nggplot(plotdata, aes(x=age, y=chol)) + geom_point() + ylab(\"Cholesterol\") + xlab(\"Age (years)\") + theme_bw() + geom_smooth(method=\"lm\")\n\n\n\n\n\nFigure 3: Scatterplot of cholesterol and age. Association is described using a linear trend line.\n\n\n\n\n\n\n2.2.2 Simple linear regression model\n\nThe simple regression model produces an easy to remember (but approximate) rule of thumb.\n\n“Normal cholesterol is 190 plus half your age”\n\\(E[\\textrm{Chol} | \\textrm{Age}] = 190 + 0.50 \\times \\textrm{Age}\\)\nNote that data were generated using this model. Estimates, below, will be different.\n\n\n\n\n2.2.3 Frequentist Estimates and Interpretation\n\nRaw, unformatted estimates, model summary, and confidence intervals for age\n\n\n\nCode\nm.chol &lt;- lm(chol ~ age, data=plotdata)\nsummary(m.chol)\n\n\n\nCall:\nlm(formula = chol ~ age, data = plotdata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-39.591 -10.524  -0.234  11.154  41.819 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 203.2259    10.3138  19.704   &lt;2e-16 ***\nage           0.3209     0.1375   2.333   0.0203 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15.34 on 299 degrees of freedom\nMultiple R-squared:  0.01788,   Adjusted R-squared:  0.0146 \nF-statistic: 5.444 on 1 and 299 DF,  p-value: 0.0203\n\n\nCode\nconfint.default(m.chol, \"age\") # Based on asymptotic Normality\n\n\n       2.5 %    97.5 %\nage 0.051334 0.5904842\n\n\n\\(E[\\textrm{Chol} | \\textrm{Age}] = 203.2 + 0.32 \\times \\textrm{Age}\\)\n\nUsing finalfit to create a nicer output table of the slope coefficient for age and confidence intervals\n\n\n\nCode\nmykable = function(x){\n  knitr::kable(x, row.names = FALSE, align = c(\"l\", \"l\", \"r\", \"r\", \"r\", \"r\", \"r\", \"r\", \"r\"),\n               booktabs=TRUE)\n}\n\n\nexplanatory = c(\"age\")\ndependent = 'chol'\n\nlabel(plotdata$chol) &lt;- \"Cholesterol\"\nlabel(plotdata$age) &lt;- \"Age (years)\"\nplotdata %&gt;% \n  finalfit(dependent, explanatory) %&gt;% mykable()\n\n\n\n\n\n\n\n\n\n\n\n\n\nDependent: Cholesterol\n\nunit\nvalue\nCoefficient (univariable)\nCoefficient (multivariable)\n\n\n\n\nAge (years)\n[63.0,100.0]\nMean (sd)\n227.2 (15.5)\n0.32 (0.05 to 0.59, p=0.020)\n0.32 (0.05 to 0.59, p=0.020)\n\n\n\n\n\n\n\n2.2.4 Bayesian Estimates and Interpretation\n\nBayesian approach to the linear model requires specifying\n\nThe model, e.g. a linear model with intercept and slope for age, Normally distributed errors and constant variance\nPrior distributions on parameters\n\nFor the simple linear regression model, we have parameters \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\sigma\\).\nFor now, we will use default prior distributions that are are intended to be weakly informative in that they provide moderate regularization and help stabilize computation. See the STAN documentation for more details\nAppropriate priors can be based on scientific considerations\nSensitivity analyses can evaluate the the robustness of finding to different prior assumptions\n\n\nOutput from Bayesian linear regression\n\n\n\nCode\nfit1 &lt;- stan_glm(chol ~ age,\n                 data=plotdata, family=gaussian(),\n                 seed=1234,\n                 refresh=0)\nsummary(fit1, digits=2, prob=c(.025, .5, .975))\n\n\n\nModel Info:\n function:     stan_glm\n family:       gaussian [identity]\n formula:      chol ~ age\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 301\n predictors:   2\n\nEstimates:\n              mean   sd     2.5%   50%    97.5%\n(Intercept) 203.28  10.69 182.56 203.18 224.80 \nage           0.32   0.14   0.04   0.32   0.60 \nsigma        15.39   0.63  14.25  15.37  16.74 \n\nFit Diagnostics:\n           mean   sd     2.5%   50%    97.5%\nmean_PPD 227.19   1.24 224.79 227.19 229.63 \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.18 1.00 3721 \nage           0.00 1.00 3735 \nsigma         0.01 1.00 3469 \nmean_PPD      0.02 1.00 3911 \nlog-posterior 0.03 1.00 1638 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\n\n\nAnd a summary of the prior distributions used\n\n\n\nCode\nprior_summary(fit1, digits = 2)\n\n\nPriors for model 'fit1' \n------\nIntercept (after predictors centered)\n  Specified prior:\n    ~ normal(location = 227, scale = 2.5)\n  Adjusted prior:\n    ~ normal(location = 227, scale = 39)\n\nCoefficients\n  Specified prior:\n    ~ normal(location = 0, scale = 2.5)\n  Adjusted prior:\n    ~ normal(location = 0, scale = 6)\n\nAuxiliary (sigma)\n  Specified prior:\n    ~ exponential(rate = 1)\n  Adjusted prior:\n    ~ exponential(rate = 0.065)\n------\nSee help('prior_summary.stanreg') for more details\n\n\n\nInterpretation\n\nSlope for age is of primary scientific importance\nA priori we assume that no association between age and cholesterol. Specifically, we assumed a Normal prior with location (mean) of 0 and scale (standard devation) of 6 for \\(\\beta_1\\).\nConditional on the data, we estimate that for every 1 year increase in age, cholesterol increases by 0.32 mg/dl (95% credible interval 0.06 to 0.58).\n\n\n\n\n2.2.5 Comparison of Frequentist and Bayesian interpretation of the slope\n\nFollowing was given by gpt-4.0 1\n\n1 Query “Explain how a slope from simple linear regression is interpreted using language a college graduate could understand. Provide a Bayesian and Frequentist interpretation.” VUMC aiChat tool, GPT-4. Retrieved 12/28/2023.\nIn simple linear regression, the slope (often denoted as “b” or “beta”) is interpreted as the expected change in the dependent (or response) variable for a one-unit increase in the independent (or predictor) variable, assuming all other variables are held constant.\nFrom a Frequentist perspective, the slope is considered a fixed but unknown value which is estimated from the sample data. If the slope is found to be 2, for instance, it would suggest that for every one-unit increase in our predictor variable, we expect, on average, a 2-unit increase in the response variable. This is based on the data at hand and is subject to sampling variability. Confidence intervals can be constructed around this estimate to provide a range of plausible values for this unknown fixed parameter. If a 95% confidence interval for the slope is (1.5, 2.5), it would suggest that we are 95% confident that the true slope falls within this range.\nIn contrast, a Bayesian approach treats the slope as a random variable. Rather than a single fixed value, the slope has a distribution of possible values. This distribution combines our prior beliefs about what the slope might be (prior distribution) and the observed data (likelihood). The resulting distribution (posterior distribution) reflects our updated belief about the slope after seeing the data. If the mean of the posterior distribution is 2, it suggests that, given the prior and the data, our best guess is that a one-unit increase in the predictor variable corresponds to a 2-unit increase in the response variable on average. A credible interval, say a 95% credible interval of (1.5, 2.5), suggests that there is a 95% probability that the true value of the slope falls within this range, given our prior and the observed data.\n\n\nWhen to use Bayesian or frequentist approaches to estimation2\n\nOn many occasions, if one is careful in execution, both approaches to analysis will yield essentially equivalent inference\nFor small sample sizes, Bayesian approaches with carefully considered priors are often the only way to go because it is difficult to obtain well-calibrated frequentist intervals\nFor medium to large samples, unless there is strong prior information that one wants to incorporate, robust frequentist estimation using sandwich estimation is very appealing because its consistency is guaranteed under mild conditions\nFor highly complex models, a Bayesian approach is often the most convenient way to formulate the model, and computation under the Bayesian approach is the most straightforward\nIn summary, in many instances carefully considered Bayesian and frequentist approach will lead to similar scientific conclusions. My goal is describe the advantages and shortcoming of each approach, but a strong recommendation of one over the other is not given as there is often no reason for stating a preference.\n\n\n2 Wakefield, Jon. Bayesian and Frequentist Regression Methods. Section 1.6, Executive Summary\n\n\n2.3 Uses of Regression\n\nBorrowing information\n\nUse other groups to make estimates in groups with sparse data\nIntuitively, 67 and 69 year olds would provide some relevant information about 68 year olds\nAssuming a straight line relationship tells us about other, even more distant, individuals\nIf we do not want to assume a straight line, we may only want to borrow information from nearby groups\nLocally weighted scatterplot smooth line (lowess) added to the previous figures\nSplines discussed in future lectures\nMay not want to borrow too much information\n\nLinear relationship is an assumption, with often low power to detect departures from linearity\nAlways avoid extrapolating beyond the range of the data (e.g. ages under 65 or over 100)\nDefining “Contrasts”\n\nDefine a comparison across groups to use when answering scientific questions\nIf the straight line relationship holds, the slope is the difference in mean cholesterol levels between groups differing by 1 year in age\nDo we want to assume that comparisons of 65 to 66 year old subjects are the same as comparisons of 95 to 96 year old subjects?\nIf a non-linear relationship, the slope is still the average difference in mean cholesterol levels between groups differing by 1 year in age\nSlope is a (first order or linear) test for trend\n\n\n\n\n2.4 Linear Regression Inference\n\nRegression output provides\n\nEstimates\n\nIntercept: Estimated mean cholesterol when age is 0\nSlope: Estimated average difference in average cholesterol for two groups differing by 1 year in age\nStandard errors\n\nConfidence intervals\nP-values for testing\n\nIntercept is zero (usually unimportant)\nSlope is zero (test for linear trend in means)\n\n\n(Frequentist) Interpretation\n\nFrom linear regression analysis, we estimate that for each year difference in age, the difference in mean cholesterol is 0.32 mg/dL. A 95% confidence interval (CI) suggests that this observation is not unusual if the true difference in mean cholesterol per year difference in age were between 0.05 and 0.59 mg/dL. Because \\(p = 0.02\\), we reject the null hypothesis that there is no linear trend in the average cholesterol across age groups using a significance level, \\(\\alpha\\), of \\(0.05\\)."
  },
  {
    "objectID": "notes/Lec03.SimpleLinearRegression.html#simple-linear-regression",
    "href": "notes/Lec03.SimpleLinearRegression.html#simple-linear-regression",
    "title": "Simple Linear Regression",
    "section": "3 Simple Linear Regression",
    "text": "3 Simple Linear Regression\n\n3.1 Ingredients\n\nResponse\n\nThe distribution of this variable will be compared across groups\nLinear regression models the mean of the response variable\nLog transformation of the response corresponds to modeling the geometric mean\nNotation: It is extremely common to use \\(Y\\) to denote the response variable when discussing general methods\n\nPredictor\n\nGroup membership is measured by this variable\nNotation\n\nWhen not using mnemonics, will be referred to as the \\(X\\) variable in simple linear regression (linear regression with one predictor)\nLater, when we discuss multiple regression, will refer to \\(X_1, X_2, \\ldots, X_p\\) when there are up to \\(p\\) predictors\n\n\nRegression Model\n\nWe typically consider a “linear predictor function” that is linear in the modeled predictors\nExpected value (i.e. mean) of \\(Y\\) for a particular value of \\(X\\)\n\\(E[Y | X] = \\beta_0 + \\beta_1 \\times X\\)\nIn a deterministic world, a line is of the form \\(y = mx + b\\)\n\nWith no variation in the data, each value of \\(y\\) would like exactly on a straight line\nIntercept \\(b\\) is values of \\(y\\) when \\(x = 0\\)\nSlope \\(m\\) is the difference in \\(y\\) for a one unit difference in \\(x\\)\n\nStatistics in not completely deterministic. The real world has variability\n\nResponse within groups is variable (people born on the same day will have different cholesterol levels!)\n\nRandomness due to other variables impacting cholesterol\nInherent randomness\n\n\n\nThe regression line thus describes the central tendency of the data in a scatterplot of the response versus the predictor\nInterpretation of regression parameters\n\nIntercept \\(\\beta_0\\): Mean \\(Y\\) for a group with \\(X=0\\)\n\nOften \\(\\beta_0\\) is not of scientific interest\nMay be out of the range of data, or even impossible to observe \\(X=0\\)\n\nSlope \\(\\beta_1\\): Difference in mean \\(Y\\) across groups differing in \\(X\\) by 1 unit\n\nUsually measures association between \\(Y\\) and \\(X\\)\n\\(E[Y | X] = \\beta_0 + \\beta_1 \\times X\\)\n\n\nDerivation of interpretation\n\nSimple linear regression of response \\(Y\\) on predictor \\(X\\)\nMean of any arbitrary group can be derived from the equation \\[\nY_i = \\beta_0 + \\beta_1 X_i\n\\]\nInterpretation determined by considering possible values of \\(X\\)\nModel: \\(E[Y_i | X_i] = \\beta_0 + \\beta_1 \\times X_i\\)\n\nWhen \\(X_i = 0\\), \\(E[Y_i | X_i = 0 ] = \\beta_0\\)\nWhen \\(X_i = x\\), \\(E[Y_i | X_i = x ] = \\beta_0 + \\beta_1 x\\)\nWhen \\(X_i = x + 1\\), \\(E[Y_i | X_i = x + 1 ] = \\beta_0 + \\beta_1 x + \\beta_1\\)\nWe can use the above to get an equation for \\(\\beta_1\\)\n\n\\[\nE[Y_i | X_i = x + 1 ] - E[Y_i | X_i = x ] = \\\\\n(\\beta_0 + \\beta_1 x + \\beta_1) - (\\beta_0 + \\beta_1 x) \\\\\n= \\beta_1\n\\]\n\nThus, \\(\\beta_1\\) is the expected change in \\(Y\\) per one unit increase in \\(x\\)\n\n\n\n\n\n3.2 Matrix Notation\n\nUsing scalars, the simple linear regression model can be written as\n\n\\(Y_i = \\beta_0 + \\beta_1 \\times X_i + \\epsilon_i\\)\n\\(i = 1, \\ldots, n\\)\n\\(i\\) indexes the independent sampling units (e.g. subjects)\n\\(n\\) is the total number of independent sampling units\nThis formulauation drops the expected value notation, add in \\(\\epsilon_i\\)\n\n\\(\\epsilon_i\\) are the “Residuals” or “Errors”\n\\(E[\\epsilon_i] = 0\\)\n\\(V[\\epsilon_i] = \\sigma^2\\) (constant variance assumption)\n\n\n\n\nMatrix notation: \\({\\boldsymbol{Y}}= {\\boldsymbol{X}}{\\boldsymbol{\\beta}}+ {\\boldsymbol{\\epsilon}}\\)\n\n\\({\\boldsymbol{Y}}= \\left( \\begin{array}{c} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{array} \\right)_{n\\times1}\\) \\({\\boldsymbol{X}}= \\left( \\begin{array}{cc} 1 & x_1 \\\\ 1 & x_2 \\\\ \\vdots & \\vdots \\\\ 1 & x_n \\end{array} \\right)_{n\\times2}\\) \\({\\boldsymbol{\\beta}}= \\left( \\begin{array}{c} \\beta_0 \\\\ \\beta_1 \\\\ \\end{array} \\right)_{2\\times1}\\) \\({\\boldsymbol{\\epsilon}}= \\left( \\begin{array}{c} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{array} \\right)_{n\\times1}\\)\n\\(E[{\\boldsymbol{\\epsilon}}] = {\\boldsymbol{0}}\\), where \\({\\boldsymbol{0}}= \\left( \\begin{array}{c} 0 \\\\ 0 \\\\ \\vdots \\\\0 \\end{array} \\right)_{n\\times1}\\)\n\\(V[{\\boldsymbol{\\epsilon}}] = \\sigma^2 {\\boldsymbol{I}}\\), where \\({\\boldsymbol{I}}= \\left( \\begin{array}{cccc} 1 & 0 & \\ldots & 0 \\\\ 0 & 1 & \\ldots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\ldots & 1 \\end{array} \\right)_{n\\times n}\\)\n\\(E[{\\boldsymbol{Y}}] = {\\boldsymbol{X}}{\\boldsymbol{\\beta}}\\)\n\nI am using standard notation to indicate matrices/vectors and scalars\nBoldface indicates a vector or matrix (\\({\\boldsymbol{Y}}\\), \\({\\boldsymbol{X}}\\), \\({\\boldsymbol{\\beta}}\\), \\({\\boldsymbol{\\epsilon}}\\), \\({\\boldsymbol{0}}\\), \\({\\boldsymbol{I}}\\))\nNormal typeface indicates a scalar (\\(Y_i\\), \\(x_i\\), \\(\\beta_0\\), \\(\\beta_1\\), \\(\\epsilon_i\\), \\(0\\), \\(1\\))"
  },
  {
    "objectID": "notes/Lec03.SimpleLinearRegression.html#example-analysis-of-bmi-part-1",
    "href": "notes/Lec03.SimpleLinearRegression.html#example-analysis-of-bmi-part-1",
    "title": "Simple Linear Regression",
    "section": "4 Example Analysis of BMI (Part 1)",
    "text": "4 Example Analysis of BMI (Part 1)\n\nExample analysis conducted in class involving BMI (response) with gender (predictor 1) and age (predictor 2)\nThis is “Lab 1” and will serve as an example of how future labs will proceed"
  },
  {
    "objectID": "notes/Lec03.SimpleLinearRegression.html#residuals",
    "href": "notes/Lec03.SimpleLinearRegression.html#residuals",
    "title": "Simple Linear Regression",
    "section": "5 Residuals",
    "text": "5 Residuals\n\nOften linear regression models are specified in terms of the response instead of the mean response\n\nInclude an error term in the model, \\(\\epsilon_i\\)\nModel \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\)\n\nThe linear regression model is divided into two parts\n\nThe mean, or systematic, part (the “signal”)\nThe error, or random, part (the “noise”)\n\nResiduals\n\n\\(\\hat{\\epsilon}_i = Y_i - \\left(\\hat{\\beta_0} + \\hat{\\beta_1} X_i\\right)\\)\n\\(\\hat{{\\boldsymbol{\\epsilon}}} = {\\boldsymbol{Y}}- {\\boldsymbol{X}}\\hat{{\\boldsymbol{\\beta}}}\\)\n\\(\\hat{{\\boldsymbol{\\beta}}} = \\left({\\boldsymbol{X}}'{\\boldsymbol{X}}\\right)^{-1} \\left({\\boldsymbol{X}}'{\\boldsymbol{Y}}\\right)\\)\n\nThe mean of the residuals is \\(0\\)\nThe standard deviation of the residuals is the “Root Mean Square Error”\n\nIn our example analysis of BMI and gender, the RMSE is exactly equal to the pooled estimate of the standard deviation from a two-sample, equal variance t-test\nIn our example analysis of BMI and age, the RMSE is the square root of the average variances across the age groups\n\nIn many textbooks, \\(\\epsilon_i \\sim N(0, \\sigma^2)\\)\n\nA common \\(\\sigma^2\\) implies constant variance across all levels of the grouping variable, “homoscedasticity”\nNormality of the residuals is a nice property, but it is not necessary (and rarely observed in practice)\nWe will discuss how lack of Normality and heteroscedasticity impact statistical inference"
  },
  {
    "objectID": "notes/Lec03.SimpleLinearRegression.html#assumptions-and-inference-with-linear-regression",
    "href": "notes/Lec03.SimpleLinearRegression.html#assumptions-and-inference-with-linear-regression",
    "title": "Simple Linear Regression",
    "section": "6 Assumptions and Inference with Linear Regression",
    "text": "6 Assumptions and Inference with Linear Regression\n\n6.1 The Big Three Assumptions\n\nMost common uses of regression\n\nPrediction: Estimating what a future value of \\(Y\\) will be based on observed \\(X\\)\nComparisons within groups: Describing the distribution of \\(Y\\) across levels of the grouping variable \\(X\\) by estimating the mean \\(E[Y | X]\\)\nComparisons across groups: Differences appear across groups if the regression parameter slope estimate \\(\\beta_1\\) is non-zero\n\nValid statistical inference (CIs, p-values) about associations requires three general assumptions\nAssumption 1: Approximately Normal distributions for the parameter estimates\n\nNormal data or “large” N\nIt is often surprising how small “large” can be\n\nDefinition of large depends on the error distribution and relative sample sizes within each group\nWith exactly Normally distributed errors, only need one observation (or two to estimate a slope)\nWith very heavy tails, “large” can be very large\nSee Lumley, et al., Ann Rev Pub Hlth, 2002\n\n\nAssumption 2: Independence of observations\n\nClassic regression: Independence of all observation (now)\nRobust standard errors: Correlated observations within identified clusters (later)\n\nAssumption 3: Assumption about the variance of observations within groups\n\nClassic regression: Homoscedasticity (equal variance across groups)\nRobust standard errors: Allows for unequal variance across groups\n\n\n\n\n6.2 Additional Assumptions for Additional Inference\n\nNote that some textbooks will claim there are more than three assumptions. In truth, additional assumptions are not needed to make the aforementioned statistical inference about associations. However ...\nValid statistical inference (CIs, p-values) about mean responses in specific groups requires a further assumption\nAssumption 4: Adequacy of the linear model\n\nIf we are trying to borrow information about the mean from neighboring groups, and we are assuming a straight line relationship, the straight line needs to be true\nNo longer saying there is just a linear trend in the means, but now need to believe that all the means lie on a straight line\nNote that we can model transformations of the measured predictor\n\nFor inference about individual observations (prediction intervals, P-values) in specific groups requires another assumption\nAssumption 5: Assumptions about the distribution of the errors within each group (a very strong assumption)\n\nClassically: Errors have the same Normal distribution within each grouping variable\nRobust standard error will not help\nPrediction intervals assume a common error distribution across groups (homoscedasticity)\n\nPossible extension: Errors have the same distribution, but not necessarily Normal (rarely implemented in frequentist software)\n\nBootstrapping\nBayesian analysis\nOther flexible approaches\n\n\n\n6.2.1 Scenario 1: All model assumptions hold\n\n\nCode\nset.seed(1234)\nn &lt;- 200\nregassumptions &lt;- data.frame(x=seq(from=0, to=100, length=n))\n# Linear model correct, Normal errors\nregassumptions$y1 &lt;- 5 + 0.2*regassumptions$x + rnorm(n,0,5)\n# Linear model correct, Skewed errors\nregassumptions$y2 &lt;- 5 + 0.2*regassumptions$x + rlnorm(n,0,1)\n# Linear model correct, errors increasing with predictor (so increasing with Y too)\nregassumptions$y3 &lt;- 5 + 0.2*regassumptions$x + rnorm(n,0,1+regassumptions$x*.1)\n# Linear model incorrect, Normal error\nregassumptions$y4&lt;- 5 + 0.2*regassumptions$x + 0.02*(regassumptions$x)^2 + rnorm(n,0,5)\nggplot(regassumptions, aes(x=x, y=y1)) + geom_point() + geom_smooth(method=\"lm\")\n\n\n\n\n\nFigure 4: Scatterplot with best fit line where assumptions of Normality, linearity, homoskedasticity all hold\n\n\n\n\n\n\nCode\nmodel.y1 &lt;- lm(y1 ~ x, data = regassumptions)\ngglm(model.y1)\n\n\n\n\n\nFigure 5: Diagnostic plots where assumptions of Normality, linearity, homoskedasticity all hold\n\n\n\n\n\nCan estimate the mean of Y at a given value of X\nCan make inference about the association between X and Y (based on the slope)\nCan predict new values of Y given X\n\n\n\n6.2.2 Scenario 2: Error distribution is not Normally distributed\n\n\nCode\nggplot(regassumptions, aes(x=x, y=y2)) + geom_point() + geom_smooth(method=\"lm\")\n\n\n\n\n\nFigure 6: Scatterplot with best fit line where the error distribution is skewed to the right\n\n\n\n\n\n\nCode\nmodel.y2 &lt;- lm(y2 ~ x, data = regassumptions)\ngglm(model.y2)\n\n\n\n\n\nFigure 7: Diagnostic plots where the error distribution is skewed to the right\n\n\n\n\n\nIf the sample size is large enough, we can estimate the mean of Y at a given value of X (Central Limit Theorem)\nIf we use “robust” standard errors, we can make inference about the association between X and Y (based on the slope)\n\nRobust standard error relax the assumption of homoskedasticity/common variance\n\nWe cannot predict new values of Y given X. This requires the correct error distribution.\n\n\n\n6.2.3 Scenario 3: Variance increasing with predictor and expected value of Y (heteroskedasticity)\n\n\nCode\nggplot(regassumptions, aes(x=x, y=y3)) + geom_point() + geom_smooth(method=\"lm\")\n\n\n\n\n\nFigure 8: Scatterplot with best fit line where the error distribution variance is increasing\n\n\n\n\n\n\nCode\nmodel.y3 &lt;- lm(y3 ~ x, data = regassumptions)\ngglm(model.y3)\n\n\n\n\n\nFigure 9: Diagnostic plot where the error distribution variance is increasing\n\n\n\n\n\nIf the sample size is large enough, we can estimate the mean of Y at a given value of X (Central Limit Theorem)\nIf we use “robust” standard errors, we can make inference about the association between X and Y (based on the slope)\n\nRobust standard error relax the assumption of homoskedasticity/common variance\n\nWe cannot predict new values of Y given X. This requires the correct error distribution (and mean model, but that is correct)\n\n\n\n6.2.4 Scenario 4: Mean model incorrectly specified\n\n\nCode\nggplot(regassumptions, aes(x=x, y=y4)) + geom_point() + geom_smooth(method=\"lm\")\n\n\n\n\n\nFigure 10: Scatterplot with best fit line where the mean model is not correctly specified\n\n\n\n\n\n\nCode\nmodel.y4 &lt;- lm(y4 ~ x, data = regassumptions)\ngglm(model.y4)\n\n\n\n\n\nFigure 11: Diagnostic plots where the mean model is not correctly specified\n\n\n\n\n\nBecause our model is wrong, we cannot estimate the mean of Y at a given value of X. It will be biased, even if the sample size is large, the model will still be incorrect\n\nThe good news is that we can fix our mean model to more flexibly model the association.\n\nHere, we have just included a single slope term to model the association with \\(X\\), \\(E[Y|X] = \\beta_0 + \\beta_1*X\\).\nThe correct model in this simple example is \\(E[Y|X] = \\beta_0 + \\beta_1*X + \\beta_2*X^2\\). Note that this model is still a “linear model”. A “linear model” is linear in the parameters (\\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\)). We can included the quadratic predictor \\(X^2\\) in the model and still have a model that is linear in the parameters.\n\n\nIf we use “robust” standard errors, we can make inference about the association between X and Y (based on the slope, \\(\\beta_1\\))\n\nRobust standard error relax the assumption of homoskedasticity\nWhile the model is not a great fit, \\(\\beta_1\\) may summarize a first-order trend\n\nWe cannot predict new values of Y given X. This requires the correct mean model and error distribution.\n\n\n\n6.2.5 Summary\n\nRegression based inference about associations is far more trustworthy than estimation of group means or individual predictions.\nThere is much to be gained by using robust variance estimates\n\n\n\nTable 1: Summary of what is required for valid estimates of \\(\\beta\\) and the variance of beta using the classical, non-robust (NR) and robust (R) approaches. The classic (non-robust) variance is \\(\\hat{\\textrm{Var}}_{NR}(\\hat{\\beta})\\) and the robust variance is \\(\\hat{\\textrm{Var}}_R(\\hat{\\beta})\\)\n\n\n\n\n(a) Assumption Holds (Yes/No)\n\n\nLinearity\nHomoskedasticity\nNormality\n\n\n\n\nY\nY\nY\n\n\nY\nY\nN\n\n\nY\nN\nY\n\n\nY\nN\nN\n\n\nN\nY\nY\n\n\nN\nY\nN\n\n\nN\nN\nY\n\n\nN\nN\nN\n\n\n\n\n\n\n(b) Valid Inference (Yes/No/Maybe)\n\n\n\n\n\n\n\n\\(\\hat{\\beta}\\)\n\\(\\hat{\\textrm {Var}}_{NR}(\\hat{\\beta})\\)\n\\(\\hat{\\textrm{ Var}}_R(\\hat{\\beta})\\)\n\n\n\n\nY\nY\nY\n\n\nY\nM2\nM2\n\n\nY\nN\nY\n\n\nY\nN\nM2\n\n\nM1\nN\nM3\n\n\nM1\nN\nM2,3\n\n\nM1\nN\nM2\n\n\nM1\nN\nM2,3\n\n\n\n\n\n\n\n1 First-order trend, if appropriate\n2 Approximately valid under large sample sizes\n3 If \\(X\\) is random, but not if \\(X\\) is fixed\n\n\n\n\n6.3 Interpreting “Positive” Results\n\nSlope is statistically different from 0 using robust standard errors\nObserved data is atypical of a setting with no linear trend in mean response across groups\nData suggests evidence of a trend toward larger (or smaller) means in groups having larger values of the predictor\n(To the extent the data appears linear, estimates of the group means will be reliable)\n\n\n\n6.4 Interpreting “Negative” Results\n\nMany possible reasons why the slope is not statistically different from 0 using robust standard errors\nThere may be no association between the response and predictor\nThere may be an association, but not in the parameter considered (the mean response)\nThere may be an association in the parameter considered, but the best fitting line has zero slope\nThere may be a first order trend in the parameter considered, but we lacked the precision to be confident that it truly exists (a type II error)"
  },
  {
    "objectID": "notes/Lec03.SimpleLinearRegression.html#model-checking",
    "href": "notes/Lec03.SimpleLinearRegression.html#model-checking",
    "title": "Simple Linear Regression",
    "section": "7 Model Checking",
    "text": "7 Model Checking\n\nMuch statistical literature has been devoted to methods for checking the assumptions for regression models\nMy philosophy: Model checking is generally fraught with peril as it necessarily involves multiple comparisons\n\nWe cannot reliably use the sampled data to assess whether it accurately portrays the population\nWe are more worried about the data from the population that we might not have sampled\nIt is not so much the abnormal points that we see, but the ones that are hiding in the population that will make our model perform badly\nBut, do tend to worry more when we see a tendency to extreme outliers in the sample or clear departures from model assumptions\nIf we over-check our model and make adjustments to the model based on our observed data, we will inflate the type I error rate (i.e. will be more likely to claim statistical significance when it doesn’t really exist). We run the risk of creating a model that fits our data well but does not generalize.\n\nEstimates are biased away from the null\nReported standard errors are too small\nIf we fish through the data, we will always find significant results\nIn clinical trials, often Phase II results are not able to be replicated in Phase III trials\n\n\nInstead of extensive model checking, go back to our choices of inference when planning our analysis\n\nBest to plan for unusual data\nThere is often little to be lost and much to be gained by using the robust standard error estimates\nBy using robust errors, avoids much of the need for model checking\nModel checking is almost entirely data driven\nRobust standard errors is a more logical scientific approach\n\nMinimize the need to presume more knowledge than the question we are trying to answer\nE.g., if we don’t know how the means might differ, why should we presume to know how the variances or the shape of the distribution might behave?\n\n\n\n\n7.1 Residuals versus fitted values\n\nPlot of \\(\\hat{\\epsilon_i}\\) versus \\(\\hat{y_i}\\)\nIf assumptions hold, should be a random pattern about zero. See Figure 5 for example.\nIf you have a priori concerns about non-constant variance, this is one potential check\n\nE.g., it is fairly common for uncertainty to increase with outcome. See Figure 9 for the resulting “fan” pattern that would appear\n\n\n\n\n7.2 Scale-location plot\n\nThe scale-location plot is very similar to residuals vs fitted values, and is used to evaluate the homoskedasticity assumption\nIt uses the square root of the absolute value of standardized residuals instead of plotting the residuals themselves\nWe want to check two things\n\nThat the best fit line is approximately horizontal. If it is, then the average magnitude of the standardized residuals isn’t changing much as a function of the fitted values.\nThat the spread around the fit line doesn’t vary with the fitted values. If so, then the variability of magnitudes doesn’t vary much as a function of the fitted values.\n\n\n\n\n7.3 QQ plot\n\nUsed to evaluate Normality of the residuals\n\nIf assumptions hold, points should lie on a 1:1 line\n\nPlot of standardized residuals versus theoretical quantities from a N(0,1) distribution\n\nIf assumptions hold, the standardized residuals should be a random sample from a N(0,1) distribution\nWe can use quantiles of the Normal distribution to check how closely the observed matches the expected\n\nThe 50th percentile of the residuals should be 0\nThe 75th percentile of the residuals should be 0.67\nThe 90th percentile of the residuals should be 1.28\nThe 95th percentile of the residuals should be 1.64\nThe 97.5th percentil of the residuals should be 1.96\n\n\nSimilar idea would be a histogram or density plot of the (standardized) residuals that could be visually evaluated for Normality\n\n\n\n7.4 Residuals versus leverage\n\nUsed to check for outliers\nPlot can help us to identify influential observations, if there are any\nInfluence differs from leverage. Not all outliers influence the regression coefficient estimates"
  },
  {
    "objectID": "notes/Lec03.SimpleLinearRegression.html#linear-regression-with-robust-standard-error-estimates",
    "href": "notes/Lec03.SimpleLinearRegression.html#linear-regression-with-robust-standard-error-estimates",
    "title": "Simple Linear Regression",
    "section": "8 Linear Regression with Robust Standard Error estimates",
    "text": "8 Linear Regression with Robust Standard Error estimates\n\nThere are several libraries available in R for fitting models with robust error estimates\nThere are also several different flavors of robust estimators\n\nFor now, we are going to consider the default “Huber-White sandwich estimator”\nIn Stata, the Huber-White robust estimate of the standard error can be obtained using the ‘robust’ option\nregress chol age, robust\n\nIn the following examples I provide estimates using classical linear regression and linear regression estimate using robust standard errors. For each of these example compare\n\nEstimates of the intercept, \\(\\hat{\\beta_0}\\)\nEstimates of the slope, \\(\\hat{\\beta_1}\\)\nEstimates of the standard errors, \\(\\hat{\\textrm{se}}(\\hat{\\beta_0})\\) and \\(\\hat{\\textrm{se}}(\\hat{\\beta_1})\\)\n\n\n\n8.1 Comparison when Normality, constant variance holds\n\n8.1.1 Age and Cholesterol\n\n\nCode\nfit.ols &lt;- ols(chol ~ age, data=plotdata, x=TRUE)\nfit.ols\n\n\nLinear Regression Model\n \n ols(formula = chol ~ age, data = plotdata, x = TRUE)\n \n                  Model Likelihood    Discrimination    \n                        Ratio Test           Indexes    \n Obs      301    LR chi2      5.43    R2       0.018    \n sigma15.3405    d.f.            1    R2 adj   0.015    \n d.f.     299    Pr(&gt; chi2) 0.0198    g        2.344    \n \n Residuals\n \n     Min      1Q  Median      3Q     Max \n -39.591 -10.524  -0.234  11.154  41.819 \n \n \n           Coef     S.E.    t     Pr(&gt;|t|)\n Intercept 203.2259 10.3138 19.70 &lt;0.0001 \n age         0.3209  0.1375  2.33 0.0203  \n \n\n\n\n\nCode\nrobcov(fit.ols)\n\n\nLinear Regression Model\n \n ols(formula = chol ~ age, data = plotdata, x = TRUE)\n \n                  Model Likelihood    Discrimination    \n                        Ratio Test           Indexes    \n Obs      301    LR chi2      5.43    R2       0.018    \n sigma15.3405    d.f.            1    R2 adj   0.015    \n d.f.     299    Pr(&gt; chi2) 0.0198    g        2.344    \n \n Residuals\n \n     Min      1Q  Median      3Q     Max \n -39.591 -10.524  -0.234  11.154  41.819 \n \n \n           Coef     S.E.    t     Pr(&gt;|t|)\n Intercept 203.2259 10.3790 19.58 &lt;0.0001 \n age         0.3209  0.1378  2.33 0.0205  \n \n\n\n\n\n8.1.2 Scenario 1 dataset\n\n\nCode\nols(y1 ~ x, data=regassumptions, x=TRUE)\n\n\nLinear Regression Model\n \n ols(formula = y1 ~ x, data = regassumptions, x = TRUE)\n \n                 Model Likelihood    Discrimination    \n                       Ratio Test           Indexes    \n Obs     200    LR chi2    200.34    R2       0.633    \n sigma5.0531    d.f.            1    R2 adj   0.631    \n d.f.    198    Pr(&gt; chi2) 0.0000    g        7.659    \n \n Residuals\n \n      Min       1Q   Median       3Q      Max \n -15.2533  -3.2042  -0.4963   3.0693  14.4377 \n \n \n           Coef   S.E.   t     Pr(&gt;|t|)\n Intercept 3.3375 0.7119  4.69 &lt;0.0001 \n x         0.2275 0.0123 18.47 &lt;0.0001 \n \n\n\nCode\nrobcov(ols(y1 ~ x, data=regassumptions, x=TRUE))\n\n\nLinear Regression Model\n \n ols(formula = y1 ~ x, data = regassumptions, x = TRUE)\n \n                 Model Likelihood    Discrimination    \n                       Ratio Test           Indexes    \n Obs     200    LR chi2    200.34    R2       0.633    \n sigma5.0531    d.f.            1    R2 adj   0.631    \n d.f.    198    Pr(&gt; chi2) 0.0000    g        7.659    \n \n Residuals\n \n      Min       1Q   Median       3Q      Max \n -15.2533  -3.2042  -0.4963   3.0693  14.4377 \n \n \n           Coef   S.E.   t     Pr(&gt;|t|)\n Intercept 3.3375 0.7002  4.77 &lt;0.0001 \n x         0.2275 0.0132 17.20 &lt;0.0001 \n \n\n\n\n\n\n8.2 Comparison when variance is increasing with predictor (Scenario 3)\n\n\nCode\nols(y3 ~ x, data=regassumptions, x=TRUE)\n\n\nLinear Regression Model\n \n ols(formula = y3 ~ x, data = regassumptions, x = TRUE)\n \n                 Model Likelihood    Discrimination    \n                       Ratio Test           Indexes    \n Obs     200    LR chi2     95.93    R2       0.381    \n sigma6.5232    d.f.            1    R2 adj   0.378    \n d.f.    198    Pr(&gt; chi2) 0.0000    g        5.909    \n \n Residuals\n \n      Min       1Q   Median       3Q      Max \n -24.1947  -3.7861  -0.1145   4.1816  17.2286 \n \n \n           Coef   S.E.   t     Pr(&gt;|t|)\n Intercept 5.6640 0.9191  6.16 &lt;0.0001 \n x         0.1755 0.0159 11.04 &lt;0.0001 \n \n\n\nCode\nrobcov(ols(y3 ~ x, data=regassumptions, x=TRUE))\n\n\nLinear Regression Model\n \n ols(formula = y3 ~ x, data = regassumptions, x = TRUE)\n \n                 Model Likelihood    Discrimination    \n                       Ratio Test           Indexes    \n Obs     200    LR chi2     95.93    R2       0.381    \n sigma6.5232    d.f.            1    R2 adj   0.378    \n d.f.    198    Pr(&gt; chi2) 0.0000    g        5.909    \n \n Residuals\n \n      Min       1Q   Median       3Q      Max \n -24.1947  -3.7861  -0.1145   4.1816  17.2286 \n \n \n           Coef   S.E.   t     Pr(&gt;|t|)\n Intercept 5.6640 0.5955  9.51 &lt;0.0001 \n x         0.1755 0.0153 11.49 &lt;0.0001 \n \n\n\n\n\n8.3 Comparison when error variance is skewed to the right (Scenario 2)\n\n\nCode\nols(y2 ~ x, data=regassumptions, x=TRUE)\n\n\nLinear Regression Model\n \n ols(formula = y2 ~ x, data = regassumptions, x = TRUE)\n \n                 Model Likelihood    Discrimination    \n                       Ratio Test           Indexes    \n Obs     200    LR chi2    431.83    R2       0.885    \n sigma2.0635    d.f.            1    R2 adj   0.884    \n d.f.    198    Pr(&gt; chi2) 0.0000    g        6.596    \n \n Residuals\n \n     Min      1Q  Median      3Q     Max \n -1.8206 -1.1341 -0.6103  0.1731 16.6448 \n \n \n           Coef   S.E.   t     Pr(&gt;|t|)\n Intercept 6.9339 0.2907 23.85 &lt;0.0001 \n x         0.1959 0.0050 38.95 &lt;0.0001 \n \n\n\nCode\nrobcov(ols(y2 ~ x, data=regassumptions, x=TRUE))\n\n\nLinear Regression Model\n \n ols(formula = y2 ~ x, data = regassumptions, x = TRUE)\n \n                 Model Likelihood    Discrimination    \n                       Ratio Test           Indexes    \n Obs     200    LR chi2    431.83    R2       0.885    \n sigma2.0635    d.f.            1    R2 adj   0.884    \n d.f.    198    Pr(&gt; chi2) 0.0000    g        6.596    \n \n Residuals\n \n     Min      1Q  Median      3Q     Max \n -1.8206 -1.1341 -0.6103  0.1731 16.6448 \n \n \n           Coef   S.E.   t     Pr(&gt;|t|)\n Intercept 6.9339 0.3526 19.67 &lt;0.0001 \n x         0.1959 0.0056 35.18 &lt;0.0001 \n \n\n\n\n\n8.4 Comments\n\nComparison of standard error estimates\n\n\n\n\n\n\n\n\n\nModel\nClassical Intercept\nRobust Intercept\nClassical Slope\nRobust Slope\n\n\n\n\nAge-chol All Assump. hold\n\n\n\n\n\n\nScenario 1 Assump hold\n\n\n\n\n\n\nScenario 3 Increase Variance\n\n\n\n\n\n\nScenario 2 Skewed Variance\n\n\n\n\n\n\n\n\nPoint estimates \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) were the same for robust and classical standard error\nPoint estimates \\(\\hat{\\textrm{se}}(\\hat{\\beta_0})\\) and \\(\\hat{\\textrm{se}}(\\hat{\\beta_1})\\) were different for robust and classical standard error\n\nIn general, it is hard to predict the effect of using robust standard errors\nThese were just a a couple of examples and do not constitute a thorough examination of the impact of using robust standard errors\n\nWhen the model assumptions hold, for these two scenarios, robust standard error estimates were larger than classical standard error estimates\n\nWhile this is not unexpected, to formally evaluate the amount of increase would require a statistical simulation\n\nSimulate many datasets where the Normality and constant variance assumption holds\nFor each dataset, estimate \\(\\hat{\\textrm{se}}(\\hat{\\beta_0})\\) and \\(\\hat{\\textrm{se}}(\\hat{\\beta_1})\\)\nSummarize the results\nApproach will be covered in a future lab"
  },
  {
    "objectID": "notes/Lec03.SimpleLinearRegression.html#linear-regression-and-correlation",
    "href": "notes/Lec03.SimpleLinearRegression.html#linear-regression-and-correlation",
    "title": "Simple Linear Regression",
    "section": "9 Linear Regression and Correlation",
    "text": "9 Linear Regression and Correlation\n\n9.1 Pearson’s Correlation\n\nNotation\n\n\\(\\rho\\) signifies the population value\n\\(r\\) (or \\(\\hat{\\rho}\\)) is the estimated correlation from data\n\nFormula\n\n\\(r = \\frac{\\Sigma(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\Sigma(x_i - \\bar{x})^2\\Sigma(y_i - \\bar{y})^2}}\\)\nRange: \\(-1 \\leq r \\leq 1\\)\n\nInterpretation\n\nMeasures the linear relationship between \\(X\\) and \\(Y\\)\nCorrelation coefficient is a unitless index of strength of association between two variables (+ = positive association, - = negative, 0 = no association)\nCan test for significant association by testing whether the population correlation is zero t = which is identical to the \\(t\\)-test used to test whether the population \\(r\\) is zero; \\(\\textrm{d.f.} = n-2\\)\nUse probability calculator for \\(t\\) distribution to get a 2-tailed \\(P\\)-value\nConfidence intervals for population \\(r\\) calculated using Fisher’s \\(Z\\) transformation\n\n\\[Z = \\frac{1}{2} \\textrm{log}_\\textrm{e} \\left( \\frac{1+r}{1-r} \\right)\\]\n\nFor large \\(n\\), \\(Z\\) follows a Normal distribution with standard error \\(\\frac{1}{\\sqrt{n-3}}\\)\nTo calculate a confidence interval for \\(r\\), first find the confidence interval for \\(Z\\) then transform back to the \\(r\\) scale\n\n\n\\[\\begin{aligned}\nZ & = & \\frac{1}{2} \\textrm{log}_\\textrm{e} \\left( \\frac{1+r}{1-r} \\right) \\\\\n2*Z & = & \\textrm{log}_\\textrm{e} \\left( \\frac{1+r}{1-r} \\right) \\\\\n\\textrm{exp}(2*Z) & = & \\left( \\frac{1+r}{1-r} \\right) \\\\\n\\textrm{exp}(2*Z) * (1-r) & = & 1 + r \\\\\n\\textrm{exp}(2*Z) - r * \\textrm{exp}(2*Z) & = & 1 + r \\\\\n\\textrm{exp}(2*Z) - 1 & = & r * \\textrm{exp}(2*Z) + r \\\\\n\\textrm{exp}(2*Z) - 1 & = & r \\left(\\textrm{exp}(2*Z) + 1\\right) \\\\\n\\frac{\\textrm{exp}(2*Z) - 1}{\\textrm{exp}(2*Z) + 1} & = & r \\\\\n\\end{aligned}\\]\n\nExample (Altman 89-90): Pearson’s \\(r\\) for a study investigating the association of basal metabolic rate with total energy expenditure was calculated to be \\(0.7283\\) in a study of \\(13\\) women. Derive a 95% confidence interval for \\(r\\)\n\\[Z = \\frac{1}{2} \\textrm{log}_\\textrm{e} \\left( \\frac{1+r}{1-r} \\right) = 0.9251\\]\nThe lower limit of a 95% CI for \\(Z\\) is given by \\(0.9251 + 1.96*\\frac{1}{\\sqrt{n-3}} = 0.3053\\)\nThe upper limit is \\(0.9251 + 1.96*\\frac{1}{\\sqrt{n-3}} = 1.545\\)\nA 95% CI for the population correlation coefficient is given by transforming these limits from the \\(Z\\) scale back to the \\(r\\) scale.\n\n\\[\\frac{\\textrm{exp}(2*0.3053) - 1}{\\textrm{exp}(2*0.3053) + 1} \\hspace{.5cm} \\textrm{to} \\hspace{.5cm}  \\frac{\\textrm{exp}(2*1.545) - 1}{\\textrm{exp}(2*1.545) + 1}\\]\n\nWhich gives a 95% CI from 0.30 to 0.91 for the population correlation\n\n\n\nCode\nfisher.z &lt;- function(r) {.5 * log((1+r)/(1-r))}\nfisher.z.inv &lt;- function(z) {(exp(2*z)-1) / (exp(2*z)+1)}\n\nfisher.z.se &lt;- function(n) {1/sqrt(n-3)}\nfisher.z.inv(fisher.z(0.7283)) #Should be original value\n\n\n[1] 0.7283\n\n\nCode\nfisher.z(0.7283)\n\n\n[1] 0.9250975\n\n\nCode\nfisher.z(0.7283)-1.96*fisher.z.se(13)\n\n\n[1] 0.3052911\n\n\nCode\nfisher.z(0.7283)+1.96*fisher.z.se(13)\n\n\n[1] 1.544904\n\n\nCode\nfisher.z.inv(fisher.z(0.7283)+c(-1.96, 1.96)*fisher.z.se(13))\n\n\n[1] 0.2961472 0.9129407\n\n\n\n\n9.2 Relationship to regression\n\nPearson’s correlation (\\(\\rho\\)) is directly related to linear regression\nCorrelation treats \\(Y\\) and \\(X\\) symmetrically, but we can relate\n\n\\(E[Y | X]\\) as a function of \\(X\\)\n\\(E[Y | X] = \\beta_0 + \\beta_1 X\\)\n\\(\\beta_1 = \\rho \\frac{\\sigma_Y}{\\sigma_X}\\)\n\\(E[Y | X]\\): mean \\(Y\\) withing groups having equal \\(X\\)\n\\(\\beta_1\\): difference in mean \\(Y\\) per 1 unit difference in \\(X\\)\n\\(\\rho\\): true correlation between \\(Y\\) and \\(X\\)\n\\(\\sigma_Y\\): standard deviation of \\(Y\\)\n\\(\\sigma_X\\): standard deviation of \\(X\\)\n\nMore interpretable formulation of \\(\\rho\\)\n\n\\(\\rho \\approx \\beta \\sqrt{\\frac{\\textrm{Var}(X)}{\\beta^2\\textrm{Var}(X) + \\textrm{Var}(Y | X = x)}}\\)\n\\(\\beta\\): slope between \\(Y\\) and \\(X\\)\n\\(\\textrm{Var}(X)\\): variance of \\(X\\) in the sample\n\\(\\textrm{Var}(Y | X = x)\\): variance of \\(Y\\) in groups having the same value of \\(X\\) (the vertical spread of data)\n\nCorrelation tends to increase in absolute value as\n\nThe absolute value of the slope of the line increases\nThe variance of data decreases within groups that share a common value of \\(X\\)\nThe variance of \\(X\\) increases\n\nScientific uses of correlation\n\nThe slope between \\(X\\) and \\(Y\\) is of scientific interest\nThe variance of \\(Y\\) given \\(X=x\\) is partly of scientific interest, but can also be impacted by restricting sampling to certain values of another variable\n\nThe variance of height given age is less if sampling just males than if sampling both sexes\nThe variance of \\(X\\) is often set by study design (which is often not of scientific interest)\n\nHypothesis tests for a nonzero correlation are exactly the same as a test for a nonzero slope in classical linear regression\nThe statistical significance of a given value of \\(\\rho\\) depends only on the sample size\n\nCorrelation is more of a statistical measure than a scientific measure"
  },
  {
    "objectID": "notes/Lec03.SimpleLinearRegression.html#regression-on-geometric-means-log-transformations",
    "href": "notes/Lec03.SimpleLinearRegression.html#regression-on-geometric-means-log-transformations",
    "title": "Simple Linear Regression",
    "section": "10 Regression on Geometric Means: Log transformations",
    "text": "10 Regression on Geometric Means: Log transformations\n\nGeometric means of distributions are typically analyzed by using linear regression on the log transformed outcome\nUsed for inference under the following circumstances\n\nWe are (scientifically) interested in multiplicative rather than additive models\nWe want to down-weight the impact of outliers\nThe standard deviation of the response in a groups is proportional to the mean\n\n“Error is \\(\\pm 10\\%\\)” instead of “Error is \\(\\pm 10\\)”\n\n\n\n\n10.1 Interpretation of Parameters\n\nLinear regression on the log transformed \\(Y\\)\nNote that in statistics \\(\\textrm{log}\\) almost always refers the the natural log (\\(\\textrm{log}_e\\))\nModel: \\(E[\\textrm{log} Y_i | X_i] = \\beta_0 + \\beta_1 \\times X_i\\)\n\nWhen \\(X_i = 0\\): \\(E[\\textrm{log} Y_i | X_i = 0] = \\beta_0\\)\nWhen \\(X_i = x\\): \\(E[\\textrm{log} Y_i | X_i = x] = \\beta_0 + \\beta_1 \\times x\\)\nWhen \\(X_i = x+1\\): \\(E[\\textrm{log} Y_i | X_i = x + 1] = \\beta_0 + \\beta_1 \\times x + \\beta_1\\)\nRestate the model as log link for geometric mean\n\nModel: \\(\\textrm{log GM}[Y_i | X_i] = \\beta_0 + \\beta_1 \\times X_i\\)\n\nWhen \\(X_i = 0\\): \\(\\textrm{log GM}[Y_i | X_i = 0] = \\beta_0\\)\nWhen \\(X_i = x\\): \\(\\textrm{log GM}[Y_i | X_i = x] = \\beta_0 + \\beta_1 \\times x\\)\nWhen \\(X_i = x+1\\): \\(\\textrm{log GM}[Y_i | X_i = x + 1] = \\beta_0 + \\beta_1 \\times x + \\beta_1\\)\n\nInterpretation of parameters by back-transforming model\n\nModel: \\(\\textrm{GM}[Y_i | X_i] = e^{\\beta_0} \\times e^{\\beta_1 \\times X_i}\\)\nWhen \\(X_i = 0\\): \\(\\textrm{GM}[Y_i | X_i = 0] = e^{\\beta_0}\\)\nWhen \\(X_i = x\\): \\(\\textrm{GM}[Y_i | X_i = x] = e^{\\beta_0} \\times e^{\\beta_1 \\times x}\\)\nWhen \\(X_i = x+1\\): \\(\\textrm{GM}[Y_i | X_i = x + 1] = e^{\\beta_0} \\times e^{\\beta_1 \\times x} \\times e^{\\beta_1}\\)\n\n\n\n\n10.2 Example: FEV with Height\n\nQuestion: How does FEV differ across height groups?\nScientific justification for summarizing with geometric mean\n\nFEV is a volume\nHeight is a linear dimension\nEach dimension of lung size is proportional to height\nStandard deviation is likely proportional to height\nScience: \\(FEV \\propto Height^3\\) or \\(\\sqrt[3]{FEV} \\propto Height\\)\n\nStatistics: \\(\\textrm{log}(FEV) \\propto 3 \\textrm{ log}(Height)\\)\n\nScience dictates model choice\n\nStatistical preference for (any) transformations of response\n\nMay transform to equal variance across groups\nHomoscedasticity allows for easier inference\n\nStatistical preference for a log transformation\n\nEasier interpretation: multiplicative model\nCompare groups using ratios\n\n\n\n\n10.3 Log Transformed Predictors\n\nInterpretation of log predictor and log link for response\nLog link used to model the geometric mean of \\(Y\\)\nExponentiated slope estimates ratio of geometric means across groups\nLog transformation of predictor compares groups with a \\(k\\)-fold difference in there measured predictors\nEstimated ratio of geometric means\n\n\\(\\exp (\\textrm{log}(k) \\times \\beta_1) = k^{\\beta_1}\\)\n\nInterpretation of model\n\n\\(\\textrm{log GM}[FEV_i | log (height_i)] = -11.9 + 3.12 \\times log(height_i)\\)\n\nEstimate ratio of geometric mean FEV for two groups differing by \\(10\\%\\) in height (a 1.1 fold difference in height)\nTo find: Exponentiate \\(1.1\\) to the slope: \\(1.1^{3.12} = 1.35\\)\n\n“Group that is 10% taller is estimated to have a geometric mean FEV that is 1.35 times higher (35% higher)”\n\nWhy transform the predictor?\n\nTypically chosen according to whether the data are likely to follow a straight line relationship\nLinearity necessary to predict the values of the parameter in individual groups\nLinearity is not necessary to estimate existence of an association\nLinearity is not necessary to estimate a first order trend in the parameter across groups having the sampled distribution of the predictor\n(Inference about these two questions will tend to be conservative if linearity does not hold)\nRarely do we know which transformation of the predictor provides the best linear fit\nAs always, there is significant danger in using the data to estimate the best transformation to use\nIf there is no association of any kind between the response and the predictor, a linear fit (with a zero slope) is correct\nTrying to detect a transformation is thus an informal test for an association (multiple testing, which inflates type I error rate)\nIt is best to choose transformations of the predictor on scientific grounds\nIn practice, it is often true that mean functions are well approximated by a straight line over a small range of the data\n\nExample: In the FEV dataset, the logarithm of height is approximately linear over the range of heights sampled (look at the plot)\nWe can often choose untransformed predictors when science would suggest a non-linear association\nCan have major advantages in interpreting the results of the analysis\nIn the FEV example, it is far more natural to compare heights by difference than by ratios\n\nWe would rather characterize two children as differing by 4 inches in height rather than as the 44 inch child being 10% taller than the 40 inch child\n\n\n\n\n10.4 Looking Ahead to Multivariable Models\n\nThe relative importance of having the “true” transformation for a predictor depends on the statistical role\n\nPredictor of Interest\nEffect Modifiers\nConfounders\nPrecision variables\n\nWe treat these variables different when planning our analysis\n\nPredictor of Interest (should be pre-specified)\n\nIn general, don’t worry about modeling the exact association until you have even established an association (binary search)\nNeed to have a prespecified model for the POI\nSearching for the best fit can inflate the type I error\nMake most accurate, precise inference about the presence of the association first\nExploratory models can suggest models for future analyses\n\nEffect Modifiers (difficult to do right anyway)\n\nModeling of effect modifiers is invariably just a test for the existence of an interaction\nWe rarely have a lot of precision to answer questions in subgroups of the data\nPatterns of interaction can be so complex that it is unlikely that we will really capture the interactions across all subgroups in a single model\nTypically, we would restrict future studies to analyses treating subgroups separately\n\nConfounders (matters the most)\n\nIt is important to have an appropriate model of the association between the confounder and the response\nFailure to accurately model the confounder means that some residual confounding will exist\nHowever, searching for the best model may inflate the type I error for inference about the predictor of interest by overstating the precision of the study\nLuckily, we rarely care about inference for the confounder, so we are free to use inefficient means of adjustment (e.g. a stratified analysis)\n\nPrecision Variables\n\nWhen modeling precision variables, it is rarely worth the effort to use the “best” transformation\nWe usually capture the largest part of the added precision using crude models\nWe generally do not care about estimating association between the response and the precision variable\n\nMost often, precision variables represent known effects on the response"
  },
  {
    "objectID": "notes/Lec10.Modeling.Effect.Modification.html",
    "href": "notes/Lec10.Modeling.Effect.Modification.html",
    "title": "Modeling Effect Modification",
    "section": "",
    "text": "Code\nlibrary(rms)\nlibrary(ggplot2)\nlibrary(biostat3)\nlibrary(car)"
  },
  {
    "objectID": "notes/Lec10.Modeling.Effect.Modification.html#overview",
    "href": "notes/Lec10.Modeling.Effect.Modification.html#overview",
    "title": "Modeling Effect Modification",
    "section": "1 Overview",
    "text": "1 Overview\n\nScientific questions\n\nMost often scientific questions are translated into comparing the distribution of some response variable across groups of interest\nGroups are defined by the predictor of interest (POI)\nEffect modification evaluates if the association between the outcome and POI is modified by strata defined by a third, modifying variable\nExamples of effect modification\n\nBinary effect modification: If we stratify by gender, we may get different answers to our scientific question in men and women\nContinuous effect modification: If we stratify by age, we way get different answers to our scientific question in young and old\n\n\nThe association between the Response and the Predictor of Interest differs in strata defined by the effect modifier\n\nWill get different answers to the scientific question within different strata defined by the effect modifier\nStatistical term: “Interaction” between the effect modifier and the POI\nEffect modification depends on the measure of effect that you choose\n\nChoice of summary measure: mean, median, geometric mean, odds, hazard\nChoice of comparisons across groups: differences, ratios\n\n\nWe will also do some model diagnostics through examples\n\nWhen looking at effect modification, I am also worried that one point may be driving the interaction\nWill go over case diagnostics in an example analysis of somatosensory evoked potential (SEP)\n\nGraphical displays for effect modification\n\nWhen analyzing difference of means of continuous data, stratified smooth curves of the data are non-parallel\nGraphical techniques more difficult for odds and hazards\n\n\n\n\nCode\nset.seed(1231)\nn &lt;- 200\ngrp &lt;- rep(c(0,1),each=n/2)\nX &lt;- runif(n)\nemplot &lt;- data.frame(grp=grp,\n                     X=X,\n                     Y=.1*grp + 2*X -4*grp*X + rnorm(n)\n)\nemplot$group &lt;- factor(emplot$grp, levels=0:1, labels=c(\"Trt\",\"Ctrl\"))\nggplot(emplot, aes(x=X, y=Y)) + geom_point() + geom_smooth(se=FALSE) + theme_bw()\n\n\n\n\n\nUnadjusted association between outcome and predictor shows a flat slope.\n\n\n\n\n\n\nCode\nggplot(emplot, aes(x=X, y=Y, color=group, grp=group)) + geom_point() + geom_smooth(se=FALSE,) + theme_bw()\n\n\n\n\n\nAdjusted association between outcome and predictor shows an increasing slope in one group and a decreasing slope in the other. This demonstrates effect modification of group."
  },
  {
    "objectID": "notes/Lec10.Modeling.Effect.Modification.html#analysis-of-effect-modification",
    "href": "notes/Lec10.Modeling.Effect.Modification.html#analysis-of-effect-modification",
    "title": "Modeling Effect Modification",
    "section": "2 Analysis of Effect Modification",
    "text": "2 Analysis of Effect Modification\n\nWhen the scientific question involves effect modification, analyses must be performed within each stratum separately\nIf we want to test the degree of effect modification or test statistically, the regression model will typically include\n\nPredictor of interest\nEffect modifier\nA covariate modeling the interaction (usually a product)\n\n\n\n2.1 Example: Is blood pressure by gender modified by smoking?\n\n\n\n\nNon-Smoke\nSmoke\nNon-smoke\nSmoke\n\n\nWomen\n107.4\n135.2\n105.6\n134.7\n\n\nMen\n122.7\n174.7\n121.1\n174.1\n\n\n\n\n\n\n\n\n\nDiff\n15.3\n39.5\n15.5\n39.4\n\n\nRatio\n1.14\n1.28\n1.15\n1.29\n\n\n\n\n2.1.1 Model for the Mean\n\nModel: \\(Y = \\beta_0 + \\beta_M * \\textrm{Male} + \\beta_S * \\textrm{Smoke} + \\beta_{MS} * \\textrm{Male} * \\textrm{Smoke} + \\epsilon\\)\n\nMale and Smoke are modeled using indicator variables\n\nMale is 1 for males, 0 for females\nSmoke is 1 for smokers, 0 for non-smokers\n\nEstimates of the mean from the regression model are given in the following table\n\n\n\n\n\n\n\n\n\n\nNon-smoker (smoke=0)\nSmoker (smoke=1)\n\n\nWomen (male=0)\n\\(\\beta_0\\)\n\\(\\beta_0 + \\beta_S\\)\n\n\nMen (male=1)\n\\(\\beta_0 + \\beta_M\\)\n\\(\\beta_0 + \\beta_S + \\beta_M + \\beta_{MS}\\)\n\n\nDifference\n\\(\\beta_M\\)\n\\(\\beta_M + \\beta_{MS}\\)\n\n\n\nWhat is the scientific interpretation of \\(\\beta_{MS} = 0\\)?\n\nIf true, the Male to Female difference in smokers \\(= \\beta_M\\)\nAnd, the Male to Female difference in non-smokers \\(= \\beta_M\\)\nIn other words, the effect of gender is the same in smoker and non-smokers\n\nA test of \\(H_0\\): \\(\\beta_{MS} = 0\\) is a test of effect modification (for means, differences)\n\n\n\n\n\n2.1.2 Model for the Geometric Mean\n\nModel: \\(\\textrm{log}(Y) = \\beta_0 + \\beta_M * \\textrm{Male} + \\beta_S * \\textrm{Smoke} + \\beta_{MS} * \\textrm{Male} * \\textrm{Smoke} + \\epsilon\\)\n\nMale and Smoke are modeled using indicator variables\n\nMale is 1 for males, 0 for females\nSmoke is 1 for smokers, 0 for non-smokers\n\nEstimates of the geometric mean from the regression model are given in the following table\n\n\n\n\n\n\n\n\n\n\nNon-smoker (smoke=0)\nSmoker (smoke=1)\n\n\nWomen (male=0)\n\\(e^{\\beta_0}\\)\n\\(e^{\\beta_0 + \\beta_S}\\)\n\n\nMen (male=1)\n\\(e^{\\beta_0 + \\beta_M}\\)\n\\(e^{\\beta_0 + \\beta_S + \\beta_M + \\beta_{MS}}\\)\n\n\nRatio\n\\(e^{\\beta_M}\\)\n\\(e^{\\beta_M + \\beta_{MS}}\\)\n\n\n\nWhat is the scientific interpretation of \\(\\beta_{MS} = 0\\)?\n\nIf true, the Male to Female ratio in smokers \\(= e^{\\beta_M}\\)\nAnd, the Male to Female ratio in non-smokers \\(= e^{\\beta_M}\\)\nIn other words, the effect of gender is the same in smoker and non-smokers\n\nA test of \\(H_0\\): \\(\\beta_{MS} = 0\\) is a test of effect modification (for geometric means, ratios)\n\n\n\n\n\n2.1.3 R Output\n\nMeans\nWe can obtain the means by male and smoking using summary statistics or from regression ouput\n\n\nCode\nbp &lt;- read.csv(\"data/bp.csv\")\nbp$logbp &lt;- log(bp$bp)\nbp$malesmoke &lt;- bp$male * bp$smoke\n\nlibrary(dplyr)\nbp %&gt;% group_by(smoke, male) %&gt;% summarize(meanbp=mean(bp))\n\n\n# A tibble: 4 × 3\n# Groups:   smoke [2]\n  smoke  male meanbp\n  &lt;int&gt; &lt;int&gt;  &lt;dbl&gt;\n1     0     0   107.\n2     0     1   123.\n3     1     0   135.\n4     1     1   175.\n\n\n\n\nCode\nm1 &lt;- lm(bp ~ male + smoke + male*smoke, data=bp)\n\nnewdata &lt;- expand.grid(male=0:1, smoke=0:1)\n\n# Predicted means from the regression model.  Matches above table\ncbind(newdata, predict(m1, newdata))\n\n\n  male smoke predict(m1, newdata)\n1    0     0                107.4\n2    1     0                122.7\n3    0     1                135.2\n4    1     1                174.7\n\n\nCode\n# Four linear combinations using biostat3 function lincom and robust standard error\n#  to obtain the 4 means\nlincom(m1, c(\"(Intercept)\",\n             \"(Intercept) + male\",\n             \"(Intercept) + smoke\",\n             \"(Intercept) + male + smoke + male:smoke\"),\n       vcov=hccm(m1, type = \"hc3\"))\n\n\n                                        Estimate 2.5 %    97.5 %   F       \n(Intercept)                             107.4    94.53401 120.266  267.6815\n(Intercept) + male                      122.7    109.3391 136.0609 323.9761\n(Intercept) + smoke                     135.2    126.4956 143.9044 926.7665\n(Intercept) + male + smoke + male:smoke 174.7    164.3974 185.0026 1104.565\n                                        Pr(&gt;F)      \n(Intercept)                             2.996149e-18\n(Intercept) + male                      1.389668e-19\n(Intercept) + smoke                     2.745825e-27\n(Intercept) + male + smoke + male:smoke 1.296132e-28\n\n\n\nDifference in mean bp, males versus females, in non-smokers\n\n\nCode\n# Male effect in non-smokers\nlincom(m1, \"male\", vcov=hccm(m1, type = \"hc3\"))\n\n\n     Estimate     2.5 %   97.5 %        F    Pr(&gt;F)\nmale     15.3 -3.248523 33.84852 2.613728 0.1146731\n\n\n\n\nDifference in mean bp, males versus females, in smokers\n\n\nCode\n# Male effect in smokers\nlincom(m1, \"male + male:smoke\", vcov=hccm(m1, type = \"hc3\"))\n\n\n                  Estimate    2.5 %   97.5 %        F       Pr(&gt;F)\nmale + male:smoke     39.5 26.01261 52.98739 32.94842 1.546644e-06\n\n\n\n\nDifference in mean bp, smokers versus non-smokers, in females\n\n\nCode\n# Smoking effect in females\nlincom(m1, \"smoke\", vcov=hccm(m1, type = \"hc3\"))\n\n\n      Estimate    2.5 %   97.5 %        F      Pr(&gt;F)\nsmoke     27.8 12.26615 43.33385 12.30347 0.001232434\n\n\n\n\nDifference in mean bp, smokers versus non-smokers, in males\n\n\nCode\n# Smoking effect in males\nlincom(m1, \"smoke + male:smoke\", vcov=hccm(m1, type = \"hc3\"))\n\n\n                   Estimate    2.5 %   97.5 %        F       Pr(&gt;F)\nsmoke + male:smoke       52 35.12822 68.87178 36.49062 6.131127e-07\n\n\n\n\nTest for effect modification\nIs the effect of smoking on mean bp modified by male? Is the effect of male on mean bp modified by smoking? Effect modification is symmetric. The answer to these two questions is answered by testing for the significance of the interaction term between male and smoking. The following answers both questions.\n\n\nCode\n# Effect modification\nlincom(m1, \"male:smoke\", vcov=hccm(m1, type = \"hc3\"))\n\n\n           Estimate   2.5 %   97.5 %        F     Pr(&gt;F)\nmale:smoke     24.2 1.26624 47.13376 4.277366 0.04586541\n\n\n\n\n\nGeometric Means\nWe can obtain the geometric means by male and smoking using summary statistics or from regression ouput\n\n\nCode\nlibrary(dplyr)\nbp %&gt;% group_by(smoke, male) %&gt;% summarize(geomeanbp=exp(mean(log(bp))))\n\n\n# A tibble: 4 × 3\n# Groups:   smoke [2]\n  smoke  male geomeanbp\n  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;\n1     0     0      106.\n2     0     1      121.\n3     1     0      135.\n4     1     1      174.\n\n\n\n\nCode\nm2 &lt;- lm(logbp ~ male + smoke + male*smoke, data=bp)\n\nnewdata &lt;- expand.grid(male=0:1, smoke=0:1)\n\n# Predicted means from the regression model.  Matches above table\ncbind(newdata, exp(predict(m2, newdata)))\n\n\n  male smoke exp(predict(m2, newdata))\n1    0     0                  105.5572\n2    1     0                  121.0690\n3    0     1                  134.6539\n4    1     1                  174.0794\n\n\nCode\n# Four linear combinations using biostat3 function lincom and robust standard error\n#  to obtain the 4 means\nlincom(m2, c(\"(Intercept)\",\n             \"(Intercept) + male\",\n             \"(Intercept) + smoke\",\n             \"(Intercept) + male + smoke + male:smoke\"),\n       vcov=hccm(m2, type = \"hc3\"),\n       eform=TRUE)\n\n\n                                        Estimate 2.5 %    97.5 %   F       \n(Intercept)                             105.5572 92.42845 120.5509 4727.281\n(Intercept) + male                      121.069  107.9158 135.8254 6681.152\n(Intercept) + smoke                     134.6539 126.706  143.1003 24946.97\n(Intercept) + male + smoke + male:smoke 174.0794 164.3235 184.4145 30743.14\n                                        Pr(&gt;F)      \n(Intercept)                             8.57959e-40 \n(Intercept) + male                      1.761937e-42\n(Intercept) + smoke                     9.482449e-53\n(Intercept) + male + smoke + male:smoke 2.217396e-54\n\n\n\nRatio of geometric mean bp, males versus females, in non-smokers\n\n\nCode\n# Male effect in non-smokers\nlincom(m2, \"male\", vcov=hccm(m2, type = \"hc3\"), eform=TRUE)\n\n\n     Estimate     2.5 %   97.5 %        F   Pr(&gt;F)\nmale 1.146951 0.9621492 1.367248 2.339419 0.134877\n\n\n\n\nRatio of geometric mean bp, males versus females, in smokers\n\n\nCode\n# Male effect in smokers\nlincom(m2, \"male + male:smoke\", vcov=hccm(m2, type = \"hc3\"), eform=TRUE)\n\n\n                  Estimate    2.5 %  97.5 %        F       Pr(&gt;F)\nmale + male:smoke 1.292792 1.188834 1.40584 36.04897 6.862878e-07\n\n\n\n\nRatio of geometric mean bp, smokers versus non-smokers, in females\n\n\nCode\n# Smoking effect in females\nlincom(m2, \"smoke\", vcov=hccm(m2, type = \"hc3\"), eform=TRUE)\n\n\n      Estimate    2.5 %   97.5 %       F      Pr(&gt;F)\nsmoke 1.275648 1.102263 1.476307 10.6683 0.002396985\n\n\n\n\nRatio of geometric mean bp, smokers versus non-smokers, in males\n\n\nCode\n# Smoking effect in males\nlincom(m2, \"smoke + male:smoke\", vcov=hccm(m2, type = \"hc3\"), eform=TRUE)\n\n\n                   Estimate    2.5 %   97.5 %        F       Pr(&gt;F)\nsmoke + male:smoke 1.437853 1.264265 1.635276 30.60412 2.934134e-06\n\n\n\n\nTest for effect modification\nIs the effect of smoking on geometric mean bp modified by male? Is the effect of male on geometric mean bp modified by smoking? Effect modification is symmetric. The answer to these two questions is answered by testing for the significance of the interaction term between male and smoking. The following answers both questions.\n\n\nCode\n# Effect modification\nlincom(m2, \"male:smoke\", vcov=hccm(m2, type = \"hc3\"), eform=TRUE)\n\n\n           Estimate     2.5 %   97.5 %        F    Pr(&gt;F)\nmale:smoke 1.127155 0.9277704 1.369389 1.452356 0.2360127\n\n\n\n\n\n\n2.1.4 Stata Output\nSiimilar Stata output to obtain the results.\n.  insheet using \"/home/slaughjc/docs/teaching/b312/doc/effectmod/bp.csv\", clear\n(3 vars, 40 obs)\n. gen logbp = log(bp)\n. tabulate male smoke, summarize(bp) nostandard nofreq noobs\n\n                                Means of bp\n\n           |        smoke\n      male |         0          1 |     Total\n-----------+----------------------+----------\n         0 |     107.4      135.2 |     121.3\n         1 |     122.7      174.7 |     148.7\n-----------+----------------------+----------\n     Total |    115.05     154.95 |       135\n\n\n. di 122.7 - 107.4\n15.3\n\n. di 122.7 / 107.4\n1.1424581\n. \n. di 174.7 - 135.2\n39.5\n\n. di 174.7 / 135.2\n1.2921598\n\n. gen malesmoke = male*smoke\n. \n. ************************************************\n. * Linear Regression: Inference about means *\n. ************************************************\n. regress bp male smoke malesmoke, robust\n\nLinear regression                                      Number of obs =      40\n                                                       F(  3,    36) =   28.05\n                                                       Prob &gt; F      =  0.0000\n                                                       R-squared     =  0.6918\n                                                       Root MSE      =  17.552\n\n------------------------------------------------------------------------------\n             |               Robust\n          bp |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n        male |       15.3    8.97806     1.70   0.097    -2.908349    33.50835\n       smoke |       27.8   7.518865     3.70   0.001     12.55103    43.04897\n   malesmoke |       24.2   11.10065     2.18   0.036     1.686837    46.71316\n       _cons |      107.4   6.227537    17.25   0.000     94.76997      120.03\n------------------------------------------------------------------------------\n\n. \n. * Get the means presented in the Table \n. *   Method 1: The adjust command         \n. *   Method 2: Four linear combos            \n. \n. adjust, by(male smoke)\n\n---------------------------------------------------------------------------------------\n     Dependent variable: bp     Command: regress\n    Variable left as is: malesmoke\n---------------------------------------------------------------------------------------\n\n------------------------\n          |    smoke    \n     male |     0      1\n----------+-------------\n        0 | 107.4  135.2\n        1 | 122.7  174.7\n------------------------\n     Key:  Linear Prediction\n\n. lincom _cons\n\n ( 1)  _cons = 0\n\n------------------------------------------------------------------------------\n          bp |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n         (1) |      107.4   6.227537    17.25   0.000     94.76997      120.03\n------------------------------------------------------------------------------\n\n. lincom _cons + male\n\n ( 1)  male + _cons = 0\n\n------------------------------------------------------------------------------\n          bp |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n         (1) |      122.7   6.467096    18.97   0.000     109.5841    135.8159\n------------------------------------------------------------------------------\n\n. lincom _cons + smoke\n\n ( 1)  smoke + _cons = 0\n\n------------------------------------------------------------------------------\n          bp |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n         (1) |      135.2   4.213207    32.09   0.000     126.6552    143.7448\n------------------------------------------------------------------------------\n\n. lincom _cons + male + smoke + malesmoke\n\n ( 1)  male + smoke + malesmoke + _cons = 0\n\n------------------------------------------------------------------------------\n          bp |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n         (1) |      174.7    4.98676    35.03   0.000     164.5864    184.8136\n------------------------------------------------------------------------------\n\n. \n. * Now, look at the important differences\n. \n. lincom male\n\n ( 1)  male = 0\n\n------------------------------------------------------------------------------\n          bp |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n         (1) |       15.3    8.97806     1.70   0.097    -2.908349    33.50835\n------------------------------------------------------------------------------\n\n. lincom male + malesmoke\n\n ( 1)  male + malesmoke = 0\n\n------------------------------------------------------------------------------\n          bp |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n         (1) |       39.5   6.528314     6.05   0.000     26.25996    52.74004\n------------------------------------------------------------------------------\n\n\n. lincom smoke\n\n ( 1)  smoke = 0\n\n------------------------------------------------------------------------------\n          bp |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n         (1) |       27.8   7.518865     3.70   0.001     12.55103    43.04897\n------------------------------------------------------------------------------\n\n\n. lincom smoke + malesmoke\n\n ( 1)  smoke + malesmoke = 0\n\n------------------------------------------------------------------------------\n          bp |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n         (1) |         52   8.166463     6.37   0.000     35.43765    68.56235\n------------------------------------------------------------------------------\n\n. lincom malesmoke\n\n ( 1)  malesmoke = 0\n\n------------------------------------------------------------------------------\n          bp |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n         (1) |       24.2   11.10065     2.18   0.036     1.686837    46.71316\n------------------------------------------------------------------------------\n\n. \n. . tabulate male smoke, summarize(logbp) nostandard nofreq noobs\n\n                              Means of logbp\n\n           |        smoke\n      male |         0          1 |     Total\n-----------+----------------------+----------\n         0 | 4.6592534  4.9027077 | 4.7809805\n         1 | 4.7963605  5.1595116 | 4.9779361\n-----------+----------------------+----------\n     Total | 4.7278069  5.0311096 | 4.8794583\n\n. \n. di exp(4.6592534)\n105.55724\n\n. di exp(4.7963605)\n121.06898\n\n. di exp(4.9027077)\n134.65389\n\n. di exp(5.1595116)\n174.07941\n\n. \n. di exp(4.7963605) - exp(4.6592534)\n15.51174\n\n. di exp(4.7963605) / exp(4.6592534)\n1.146951\n\n. \n. di exp(5.1595116) - exp(4.9027077)\n39.425526\n\n. di exp(5.1595116) / exp(4.9027077)\n1.2927916\n\n\n. \n. ***********************************************************\n. * Linear Regression: Inference about geometric means *\n. ***********************************************************\n. \n. \n. * Get the log geometric means presented in the Table \n. *   Method 1: The adjust command                                \n. *   Method 2: Four linear combos                                   \n. *   After either method, take exp(x) to get geometric mean\n. \n. regress logbp male smoke malesmoke, robust\n\nLinear regression                                      Number of obs =      40\n                                                       F(  3,    36) =   28.00\n                                                       Prob &gt; F      =  0.0000\n                                                       R-squared     =  0.6271\n                                                       Root MSE      =  .14898\n\n------------------------------------------------------------------------------\n             |               Robust\n       logbp |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n        male |   .1371071   .0850408     1.61   0.116    -.0353636    .3095778\n       smoke |   .2434543   .0707116     3.44   0.001     .1000444    .3868641\n   malesmoke |   .1196969   .0942253     1.27   0.212    -.0714009    .3107946\n       _cons |   4.659253   .0642883    72.47   0.000     4.528871    4.789636\n------------------------------------------------------------------------------\n\n. adjust, by(male smoke)\n\n---------------------------------------------------------------------------------------\n     Dependent variable: logbp     Command: regress\n    Variable left as is: malesmoke\n---------------------------------------------------------------------------------------\n\n----------------------------\n          |      smoke      \n     male |       0        1\n----------+-----------------\n        0 | 4.65925  4.90271\n        1 | 4.79636  5.15951\n----------------------------\n     Key:  Linear Prediction\n\n. lincom _cons\n\n ( 1)  _cons = 0\n\n------------------------------------------------------------------------------\n       logbp |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n         (1) |   4.659253   .0642883    72.47   0.000     4.528871    4.789636\n------------------------------------------------------------------------------\n\n. lincom _cons + male\n\n ( 1)  male + _cons = 0\n\n------------------------------------------------------------------------------\n       logbp |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n         (1) |    4.79636   .0556682    86.16   0.000      4.68346    4.909261\n------------------------------------------------------------------------------\n\n. lincom _cons + smoke\n\n ( 1)  smoke + _cons = 0\n\n------------------------------------------------------------------------------\n       logbp |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n         (1) |   4.902708   .0294475   166.49   0.000     4.842985     4.96243\n------------------------------------------------------------------------------\n\n. lincom _cons + male + smoke + malesmoke\n\n ( 1)  male + smoke + malesmoke + _cons = 0\n\n------------------------------------------------------------------------------\n       logbp |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n         (1) |   5.159512   .0279162   184.82   0.000     5.102895    5.216128\n------------------------------------------------------------------------------\n\n. \n. \n. * Now, look at the important ratios\n. \n. lincom male, eform\n\n ( 1)  male = 0\n\n------------------------------------------------------------------------------\n       logbp |     exp(b)   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n         (1) |   1.146951   .0975376     1.61   0.116     .9652544     1.36285\n------------------------------------------------------------------------------\n\n. lincom male + malesmoke, eform\n\n ( 1)  male + malesmoke = 0\n\n------------------------------------------------------------------------------\n       logbp |     exp(b)   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n         (1) |   1.292792   .0524572     6.33   0.000     1.190663     1.40368\n------------------------------------------------------------------------------\n\n. lincom smoke, eform\n\n ( 1)  smoke = 0\n\n------------------------------------------------------------------------------\n       logbp |     exp(b)   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n         (1) |   1.275648   .0902032     3.44   0.001      1.10522    1.472356\n------------------------------------------------------------------------------\n\n. lincom malesmoke, eform\n\n ( 1)  malesmoke = 0\n\n------------------------------------------------------------------------------\n       logbp |     exp(b)   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n         (1) |   1.127155   .1062065     1.27   0.212     .9310886    1.364509\n------------------------------------------------------------------------------\n\n\n\n2.2 Ignoring effect modification\n\nBy design or mistake, we sometimes do not model effect modification\nMight perform\n\nUnadjusted analysis: POI only\nAdjusted analysis: POI and third variable, but no interaction term\n\nIf effect modification exists, an unadjusted analysis will give different results according to the association between the POI and effect modifier in the sample\n\nIf the POI and the effect modifier are not associated\n\nUnadjusted analysis tends toward an (approximate) weighted average of the stratum specific effects\nWith means, exactly a weighted average\nWith odds and hazards, an approximate weighted average (because they are non-linear functions of the mean)\n\nIf the POI and the effect modifier are associated in the sample\n\nThe “average” effect is confounded and thus unreliable\n(variables can be both effect modifiers and confounders)\n\n\nIf effect modification exists, an analysis adjusting only for the third variable (but no interaction) will tend toward a weight average of the stratum specific effects\n\nHence, an association in one stratum and not the other will make an adjusted analysis look like an association (provide the sample size is large enough)"
  },
  {
    "objectID": "notes/Lec10.Modeling.Effect.Modification.html#general-model-for-effect-modification",
    "href": "notes/Lec10.Modeling.Effect.Modification.html#general-model-for-effect-modification",
    "title": "Modeling Effect Modification",
    "section": "3 General Model for Effect Modification",
    "text": "3 General Model for Effect Modification\n\nTypical model for effect modification will include\n\nMain effects\n\n\\(X\\), or predictors involving only \\(X\\)\n\\(W\\), or predictors involving only \\(W\\)\n\nInteractions\n\nPredictors derived from both \\(X\\) and \\(W\\)\n\n\n\\[\\begin{aligned}\ng(\\theta | X_i , W_i) & = & \\beta_0 + \\beta_1 \\times X_i + \\beta_2 \\times W_i + \\beta_3 \\times (XW)_i \\\\\ng(\\theta | X_i , W_i) & = & \\beta_0 + \\beta_1 \\times X_i + \\beta_2 \\times W_i + \\beta_3 \\times X_i \\times W_i\\end{aligned}\\]\nInterpretation of parameters more difficult\n\nCan try the usual approach of making comparisons of \\(\\theta\\) “across groups differing by 1 unit in corresponding predictor but agreeing in other modeled predictors.”\nHowever, terms involving two scientific variables makes this approach difficult\nIntercept\n\n\\(\\beta_0\\): corresponds to \\(X=0\\), \\(W=0\\)\nMay lack scientific meaning\n\nSlopes for main effects\n\n\\(\\beta_1\\): corresponds to 1 unit difference in \\(X\\), holding \\(W\\) and \\((X \\times W)\\) constant\n\nSo, a 1 unit difference in \\(X\\) when \\(W=0\\)\nMay lack scientific meaning\n\n\\(\\beta_2\\): corresponds to 1 unit difference in \\(W\\), holding \\(X\\) and \\((X \\times W)\\) constant\n\nSo, a 1 unit difference in \\(W\\) when \\(X=0\\)\nMay lack scientific meaning\n\n\nSlope for interaction (difficult)\n\n\\(\\beta_3\\): corresponds to 1 unit difference in \\((X \\times W)\\), holding \\(X\\) and \\(W\\) constant\n\nImpossible, so we need another way to interpret this slope parameter for the interaction\n\n\n\n\n\n3.1 Interpretation of slope parameter for the interaction\n\nConsider fixing stratum \\(W_i = w\\). Then, \\[\\begin{aligned}\n  g(\\theta | X_i , W_i = w) & = & \\beta_0 + \\beta_1 \\times X_i + \\beta_2 \\times w + \\beta_3 \\times X_i \\times w \\\\\n& = & (\\beta_0 + \\beta_2 \\times w) + (\\beta_1 + \\beta_3 \\times w) \\times X_i\\end{aligned}\\]\n\nIntercept: \\((\\beta_0 + \\beta_2 \\times w)\\) corresponds to \\(X_i = 0\\)\nSlope: \\((\\beta_1 + \\beta_3 \\times w)\\) compares groups differing by 1 unit in \\(X\\)\n\\(\\beta_3\\): Difference in \\(X\\) slope per 1 unit difference in \\(W\\)\nExample: \\(Y\\) is height, \\(X\\) is age, \\(W\\) is gender\n\n\\(\\beta_3\\): Difference in growth curves, males compared to females\n\n\nConsider fixing stratum \\(X_i = x\\). Then, \\[\\begin{aligned}\n  g(\\theta | X_i = x , W_i) & = & \\beta_0 + \\beta_1 \\times x + \\beta_2 \\times W_i + \\beta_3 \\times x \\times W_i \\\\\n& = & (\\beta_0 + \\beta_1 \\times x) + (\\beta_2 + \\beta_3 \\times x) \\times W_i\\end{aligned}\\]\n\nIntercept: \\((\\beta_0 + \\beta_1 \\times x)\\) corresponds to \\(W_i = 0\\)\nSlope: \\((\\beta_2 + \\beta_3 \\times x)\\) compares groups differing by 1 unit in \\(W\\)\n\\(\\beta_3\\): Difference in \\(W\\) slope per 1 unit difference in \\(X\\)\n\nNote the implied symmetry\n\nIf \\(W\\) modifies the association between \\(X\\) and \\(Y\\), then \\(X\\) modifies the association between \\(W\\) and \\(Y\\)\nStatistically, there is no distinction between which variable you call your “effect modifier” and your “POI”\n\nThere often is a scientific distinction that should be considered\n\n\nAside: Does confounding have to be symmetric?\n\nIf \\(W\\) confounds the association between \\(Y\\) and \\(X\\), must it also be true that \\(X\\) confounds the association between \\(W\\) and \\(Y\\)?\n\n\n\n\n3.2 Inference for Effect Modification\n\\(g(\\theta | X_i , W_i) = \\beta_0 + \\beta_1 \\times X_i + \\beta_2 \\times W_i + \\beta_3 \\times X_i \\times W_i\\)\n\nInference for effect modification\n\nNo effect modification if \\(\\beta_3 = 0\\)\nHence, to test for existence of effect modification we consider \\(H_0: \\beta_3 = 0\\)\n\nWe can perform such inference using standard regression output for the corresponding slope parameter\n\n\nInference for main effect slope\n\nInterpretation of \\(\\beta_1 = 0\\)\n\nSame intercept in strata defined by \\(W\\)\nGenerally a very uninteresting question\nWe rarely make inference on main effects slopes by themselves\n\n\nInference about effect of \\(X\\)\n\nResponse parameter not associated with \\(X\\) if \\(\\beta_1 = 0\\) AND \\(\\beta_3 = 0\\)\nWe will need to construct special tests that both parameters are simultaneously \\(0\\)\n\n\\(H_0: \\beta_1 = 0, \\beta_3 = 0\\)\nNote that the Wald tests given in regression output only consider one slope at a time\n\n\nTesting multiple slopes in Stata\n\nStata has an easy method for performing a test that multiple parameters are simultaneously \\(0\\)\n\nFirst, perform any regression command\nThen, use test var1 var2 ...\nProvides P value of the hypothesis test based on most recently executed regression command\nWill work on any type of regression\n\n\n\n\n\n3.3 Salary by sex and administration\n\nQuestion: Does sex modify the association between mean salary and administrative duties?\nWith two binary variables (sex, admin), modeling the interaction using a product is the obvious choice\n\\(E[Salary | Male, Adm] = \\beta_0 + \\beta_1 \\times Adm_i + \\beta_2 \\times Male_i + \\beta_3 \\times Adm_i \\times Male_i\\)\n\n\n3.3.1 Stata Regression Output\n. gen maleadmin = male*admin\n \n. regress salary admin male maleadmin, robust\n\nLinear regression                                      Number of obs =    1597\n                                                       F(  3,  1593) =  125.26\n                                                       Prob &gt; F      =  0.0000\n                                                       R-squared     =  0.1615\n                                                       Root MSE      =  1866.9\n\n------------------------------------------------------------------------------\n             |               Robust\n      salary |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n       admin |   1489.471    292.628     5.09   0.000     915.4946    2063.448\n        male |   1226.234   95.37051    12.86   0.000      1039.17    1413.299\n   maleadmin |   461.9072   341.6782     1.35   0.177    -208.2789    1132.093\n       _cons |   5280.373   72.61871    72.71   0.000     5137.934    5422.811\n------------------------------------------------------------------------------\n\n\n3.3.2 Descriptive statistics\n\nNote that with two binary variables, the regression parameters agree exactly with the corresponding group means\n\n------------------------------\n          |        male       \n    admin |        0         1\n----------+-------------------\n        0 | 5280.373  6506.607\n        1 | 6769.844  8457.985\n------------------------------\n\n\n3.3.3 Inference about effect modification\n\nDoes sex modify the association between mean salary and administrative duties?\nModel estimates that the administrative supplement is $462 per month more for men than women\n\n95% confident that the true difference is between $1132 more and $208 less\nNot statistically significant (\\(p = 0.177\\))\n\n\n------------------------------------------------------------------------------\n             |               Robust\n      salary |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n       admin |   1489.471    292.628     5.09   0.000     915.4946    2063.448\n        male |   1226.234   95.37051    12.86   0.000      1039.17    1413.299\n   maleadmin |   461.9072   341.6782     1.35   0.177    -208.2789    1132.093\n       _cons |   5280.373   72.61871    72.71   0.000     5137.934    5422.811\n------------------------------------------------------------------------------\n\n\n3.3.4 Inference about sex association\n\nIs sex associated with mean salary?\nNeed to test that the slope parameter for male and maleadmin are simultaneously \\(0\\)\n\n\\(H_0: \\beta_2 = 0, \\beta_3 = 0\\)\n\\(H_1:\\) At least one not equal\n\n\n. test male maleadmin\n\n ( 1)  male = 0\n ( 2)  maleadmin = 0\n\n       F(  2,  1593) =   95.90\n            Prob &gt; F =    0.0000\n\n\n3.3.5 Inference about admin association\n\nAre administrative duties associated with mean salary?\nNeed to test that the slope parameter for admin and maleadmin are simultaneously \\(0\\)\n\n\\(H_0: \\beta_1 = 0, \\beta_3 = 0\\)\n\\(H_1:\\) At least one not equal\n\n\n. test admin maleadmin\n\n ( 1)  admin = 0\n ( 2)  maleadmin = 0\n\n       F(  2,  1593) =   74.15\n            Prob &gt; F =    0.0000\n\n\n3.3.6 Inference about admin association using R\n\nSee the effect modification lab. We will go over this in detail in class."
  },
  {
    "objectID": "notes/Lec10.Modeling.Effect.Modification.html#continuous-effect-modification",
    "href": "notes/Lec10.Modeling.Effect.Modification.html#continuous-effect-modification",
    "title": "Modeling Effect Modification",
    "section": "4 Continuous Effect Modification",
    "text": "4 Continuous Effect Modification\n\nModeling interactions with continuous predictors is conceptually more complicated\nIs a multiplicative model at all a reasonable model for the data?\nNonetheless, this is the most common way we detect interactions\n\nBe cautious against using the model for prediction of means or individual observations\n\n\n\n4.1 Example: Normal ranges for SEP\n\nWe want to find normal ranges for somatosensory evoked potential (SEP)\n\np60: Average time (in milliseconds) to detection of the second positive SEP following stimulation of the posterior right and left tibial nerve\n\nAs a first step, we want to consider important predictors of nerve conduction times\n\nIf any variables such as sex, age, height, race, etc. are important predictors of nerve conduction times, then it would make most sense to obtain normal ranges within such groups\nScientifically, we might expect that height, age, and sex are related to the nerve conduction time\n\nNerve length should matter, and height is a surrogate for nerve length\nAge might affect nerve conduction times (people slow down with age)\nSex: Males have worse nervous systems\n\n\nPrior to looking at the data, we can also consider the possibility that interactions between these variables might be important\n\nHeight and age interaction? Do we expect...\n\nDifference in conduction times comparing 6 foot tall and 5 foot tall 20 year old, to be the same as ...\nDifference in conduction times comparing 6 foot tall and 5 foot tall 50 year old, to be the same as ...\nDifference in conduction times comparing 6 foot tall and 5 foot tall 80 year old?\n\nWe might suspect such an interaction due to the fact that height may not be as good a surrogate for nerve length in older people\n\nWith age, some people tend to shrink due to osteoporosis and compression of intervertebral discs. It is not clear that nerve length would be altered in such a process.\n\nThus, in young people, differences in height probably are a better measure of nerve length than in old people\n\nTall old people probably have been tall always\nShort old people will include some who were taller when they were young\n\n\nWe can also consider the possibility of three way interactions between height, age, and sex\n\nOsteoporosis affects women far more than men\nWe might expect the height-age interaction to be greatest in women and not so important in men\nA two-way interaction between height and age that is different between men and women defines a three way interaction between height, age, and sex\n\n\n\n\n4.2 SEP regression model\n\nTo define a regression model with interactions, we must create variables to model the three way interaction term\nFurthermore, it is a very good idea to include all main effects and lower order interactions in the model too\n\nMain effects: The individual variables which contribute to the interaction\nLower order terms: All interactions that involve some combination of the variables which contribute to the interaction\n\nMost often, we lack sufficient information to be able to guess what the true form of an interaction might be\n\nThe most popular approach is thus to consider multiplicative interactions\nCreate a new variable by merely multiplying the two (or more) interacting predictors\n\nFor the problem, we will create the variables\n\nHA = Height * Age\nHM = Height * Male\nAM = Age * Male\nHAM = Height * Age * Male\n\nInterpretation: In the presence of higher order terms (powers, interactions) interpretation of parameters is not easy\n\nWe can no longer use “the change associated with a 1-unit difference in predictor holding other variables constant”\nIt is generally impossible to hold other variables constant when changing a covariate involved in an interaction\nWhen it is not impossible, it is often uninteresting scientifically\n\n\n\\(E[p60 | Ht, Age, Male] = \\beta_0 + \\beta_H Ht + \\beta_A Age + \\beta_M Male + \\beta_{HA} HA + \\beta_{HM} HM + \\beta_{AM} AM + \\beta_{HAM} HAM\\)\n\np60 - Height relationship for Age = a:\n\n\n\n\n\n\n\n\nSex\nIntercept\nSlope\n\n\n\n\nF\n\\((\\beta_0 + \\beta_A a)\\)\n\\((\\beta_H + \\beta_{HA} a)\\)\n\n\nM\n\\((\\beta_0 + \\beta_M + (\\beta_A + \\beta_{AM}) a)\\)\n\\((\\beta_H + \\beta_{HM} + (\\beta_{HA} + \\beta_{HAM}) a)\\)\n\n\n\nFrom the above, we see the importance of including the main effects and lower order terms\n\nE.g., leaving out the height-sex interaction is tantamount that claiming the p60 - height relationship among newborns is the same for the two sexes\n\nIt might be true, but the chance that our lines would predict the truth is very slight– we are trying to approximate relationships in other age ranges\n\n\n\n\n\n4.3 Regression Output\n. regress p60 height age male ha hm am ham\n\n------------------------------------------------------------------------------\n         p60 |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n      height |   1.380275    .362647     3.81   0.000     .6659271    2.094622\n         age |   1.129423   .4249166     2.66   0.008     .2924161     1.96643\n        male |   74.95773   32.30708     2.32   0.021     11.31875    138.5967\n          ha |  -.0149985   .0066386    -2.26   0.025    -.0280754   -.0019217\n          hm |  -1.127006   .4825427    -2.34   0.020    -2.077526   -.1764858\n          am |  -1.162866   .5817348    -2.00   0.047    -2.308776   -.0169558\n         ham |   .0175005   .0087708     2.00   0.047     .0002236    .0347773\n       _cons |  -36.44286   23.48684    -1.55   0.122    -82.70758     9.82187\n------------------------------------------------------------------------------\n\nIf we restrict analysis to just females\n\nPoint estimates are the same as in the saturated model\nInference (CIs, p-values) can differ due to the estimate of the residual standard error\nNote that restricting by age or height would give different estimates because we are still borrowing information across groups\n\n\n. regress p60 height age ha if male==0\n\n------------------------------------------------------------------------------\n         p60 |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n      height |   1.380275   .3614558     3.82   0.000     .6653291     2.09522\n         age |   1.129423   .4235208     2.67   0.009     .2917155    1.967131\n          ha |  -.0149985   .0066168    -2.27   0.025    -.0280863   -.0019108\n       _cons |  -36.44286   23.40969    -1.56   0.122    -82.74631    9.860598\n------------------------------------------------------------------------------\nInterpreting all of the estimates can be difficult\n\nCan graph predicted values, which should be multiple linear for this model\nNote that age is a continuous variable, so to understand how age modified the association between p60 and height, we will fix age at some values\n\nUsing general notation, let \\(Age = a\\)\n\n\nEstimated association between p60 and height for Females\n\n\n\n\n\n\n\n\nAge\nIntercept\nSlope\n\n\n\n\na\n\\(\\hat{\\beta}_0 + \\hat{\\beta}_A \\times a\\)\n\\(\\hat{\\beta}_H + \\hat{\\beta}_{HA} \\times a\\)\n\n\na\n\\(-36.44 + 1.129 \\times a\\)\n\\(1.38 - 0.01499 \\times a\\)\n\n\n\nEstimated association between p60 and height in Males\n\n\n\n\n\n\n\n\nAge\nIntercept\nSlope\n\n\n\n\na\n\\(\\hat{\\beta}_0 + \\hat{\\beta}_M + (\\hat{\\beta}_A + \\hat{\\beta}_{AM}) a\\)\n\\(\\hat{\\beta}_H + \\hat{\\beta}_{HM} + (\\hat{\\beta}_{HA} + \\hat{\\beta}_{HAM}) a\\)\n\n\na\n\\(-36.44 + 74.96 + (1.13 - 1.16) a\\)\n\\(1.38 - 1.13 + (-0.015 + 0.0175) a\\)\n\n\na\n\\(38.51 - 0.033 \\times a\\)\n\\(0.25 + 0.0025 \\times a\\)\n\n\n\nWhich corresponds to a regression including just males\n. regress p60 height age ha if male==1\n\n------------------------------------------------------------------------------\n         p60 |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n      height |   .2532689   .3196022     0.79   0.430    -.3801724    .8867101\n         age |  -.0334425   .3989042    -0.08   0.933    -.8240577    .7571727\n          ha |   .0025019   .0057549     0.43   0.665    -.0089041    .0139079\n       _cons |   38.51487   22.27228     1.73   0.087    -5.628068    82.65781\n------------------------------------------------------------------------------\n\n\n4.3.1 R Code to generate plot\n\n\nCode\nlibrary(rms)\nd2 &lt;- stata.get(\"http://biostat.app.vumc.org/wiki/pub/Main/CourseBios312/sep.dta\")\nd2$p60 &lt;- (d2$p60R + d2$p60L) /2\nm.female &lt;- lm(p60 ~ height + age + height:age, data=d2, subset=sex==0)\nm.male &lt;- lm(p60 ~ height + age + height:age, data=d2, subset=sex==1)\nd2$agecat &lt;- (d2$age &lt; 35) + (d2$age &lt; 60) + 1\n\npar(mfrow=c(2,1))\nwith(d2, plot(height[sex==0], p60[sex==0], pch=agecat[sex==0], col=agecat[sex==0], xlim=c(53,80),\n   ylim=c(50,80), ylab=\"Avg P60\", xlab=\"Height\", main=\"Females\"))\nlines(53:83, predict(m.female, newdata=data.frame(age=30, height=53:83)), col=3)\nlines(53:83, predict(m.female, newdata=data.frame(age=50, height=53:83)), col=2)\nlines(53:83, predict(m.female, newdata=data.frame(age=70, height=53:83)), col=1)\nlegend(\"topleft\", c(\"30\",\"50\",\"70\"), lty=1, col=c(3,2,1), bty=\"n\", title=\"Age\")\n\nwith(d2, plot(height[sex==1], p60[sex==1], pch=agecat[sex==1], col=agecat[sex==1], xlim=c(53,80),\n   ylim=c(50,80), ylab=\"Avg P60\", xlab=\"Height\", main=\"Males\"))\nlines(53:83, predict(m.male, newdata=data.frame(age=30, height=53:83)), col=3)\nlines(53:83, predict(m.male, newdata=data.frame(age=50, height=53:83)), col=2)\nlines(53:83, predict(m.male, newdata=data.frame(age=70, height=53:83)), col=1)\nlegend(\"topleft\", c(\"&lt;35\",\"35-60\",\"60+\"), pch=c(3,2,1), col=c(3,2,1), bty=\"n\",title=\"Age\")\n\n\n\n\n\n\n\n\n4.4 Influence diagnostics\n\nFrom the output, we find a statistically significant three way interaction (p = 0.047)\nThis would argue for making prediction based on a model that include the 3-way interaction\nHowever, interactions might be significant only because of a single outlier\n\nIf that were the case, I might choose not to include the interaction\nBut, I would include the influential data point\nWe will look at the results of a “diagnosis of influential observations now, and cover in more detail later\n\nIn particular, I am interested in ensuring that the evidence for an interaction is not based solely on a single person’s observation\n\nHence, I consider 250 different regression in which I leave out each subject in turn\nI plot the slope estimates and p-values for each variable as a function of which case I left out\nFor comparison, case 0 corresponds to using the full dataset\n\nChanges in coefficients (\\(\\beta\\)s)\n\n\n\nCode\n# Calculate changes in p-values and coefficients\n# Create some variables to store the results\nid &lt;- 1:length(d2$p60)\ncase &lt;- 0:length(d2$p60)\n# p is number of predictors\np &lt;- 8\ncoeffs &lt;- matrix(NA, nrow=length(case), ncol=p)\npvals &lt;- matrix(NA, nrow=length(case), ncol=p)\n\n# Run the regression model with 1 observation deleted, save the coeffs and p-vals\n#  Note that case=0 corresponds to the full model (no observations deleted)\nfor (i in case) {\n  m &lt;- lm(p60 ~ height*age*sex, data=d2, subset=(id!=i))\n  coeffs[i+1,] &lt;- coef(m)\n  pvals[i+1,] &lt;- summary(m)$coeff[,4]\n}\n\n# Plot the regression coefficients by which subject was deleted; highlight the unusual subject\npar(mfrow=c(2,4))\nfor(i in 1:8) {\nplot(case, coeffs[,i], ylab=(names(coef(m))[i]))\npoints(x=140, y=coeffs[141,i], pch=19, col=\"Red\")\n}\n\n\n\n\n\n\nChanges in p-values for \\(\\beta\\)s\n\n\n\nCode\n# Plot the p-values for the regression coefficients by which subject was deleted\npar(mfrow=c(2,4))\nfor(i in 1:8) {\nplot(case, pvals[,i], ylab=(names(coef(m))[i]))\npoints(x=140, y=coeffs[141,i], pch=19, col=\"Red\")\n}\n\n\n\n\n\n\nContrary to my fear, the only influential observation actually lessened the evidence of an interaction\n\nWhen observation 140 is removed from the data, the evidence of an interaction is a larger estimate and lower p-value\nWe can examine the scatterplot to see why subject 140 might be so influential\nSubject 140 is a 43 year old, 57 inch female with an average p60 of 66.6\n\n\n\n\nCode\n# Scatter plot with subject 140 highlighted\nwith(d2, plot(height, p60, xlab=\"Height\", ylab=\"Avg p60\", main=\"Subject 140: 43 year old female\"))\nwith(d2, points(height[140], p60[140], pch=19, col=\"Red\"))\nlines(53:83, predict(m.female, newdata=data.frame(age=30, height=53:83)), col=1)\nlines(53:83, predict(m.female, newdata=data.frame(age=50, height=53:83)), col=2)\nlines(53:83, predict(m.male, newdata=data.frame(age=30, height=53:83)), col=3)\nlines(53:83, predict(m.male, newdata=data.frame(age=50, height=53:83)), col=4)\nlegend(\"topleft\", c(\"30 Female\",\"50 Female\",\"30 Male\", \"50 Male\"),\n  lty=1, col=1:4, bty=\"n\", title=\"Age, Gender\")\n\n\n\n\n\n\nSo, what do I do with observation 140?\n\nFrom the influence diagnostics, I am still comfortable that the data suggest a 3-way interaction\nPersonally, I do not remove observation 140 when making prediction intervals\n\nI do not know why observation 140 is unusual\nIt is possible that people like 140 are actually more prevalent in the population than my sample would suggest\nMy best guess is observation 140 represents only \\(0.4\\%\\) of the population, but would still leave her in the analysis\n\nRemoving subject 140 could bias parameter estimates, so I would rarely remove observations based on diagnostics"
  },
  {
    "objectID": "notes/Lec09.Poisson.html#count-data-and-event-rates",
    "href": "notes/Lec09.Poisson.html#count-data-and-event-rates",
    "title": "Poisson Regression",
    "section": "1 Count Data and Event Rates",
    "text": "1 Count Data and Event Rates\n\nSometimes a random variable measures the number of events occurring over some space and time interval\nExamples include\n\nNumber of polyps recurring in the three year interval between colonoscopies\nNumber of pulmonary exacerbations experienced by a cystic fibrosis patient in a year\nNumber of reflux events in a 24-hour period\n\nCount data have (in theory) no upper limit, although very large counts can be highly improbable\nWhen a response variable measures counts over space and time, we often summarize by considering the event rate\n\n“Event rate” is the expected number of events per unit of space-time\nThe rate is thus a mean count\nIn most statistical problems, we know the interval of time and the volume of space sampled\n\nPoisson models allow us to take into account the known interval of time/space using an “offset”"
  },
  {
    "objectID": "notes/Lec09.Poisson.html#poisson-model",
    "href": "notes/Lec09.Poisson.html#poisson-model",
    "title": "Poisson Regression",
    "section": "2 Poisson Model",
    "text": "2 Poisson Model\n\n2.1 Poisson distribution\n\nOften we assume that counts follow a Poisson distribution\nThe Poisson distribution can be derived from the following assumptions\n\nThe expected number of events in an interval is proportional to the size of the interval\nThe probability that two events occur with an infinitesimally small interval of space-time is zero\nThe number of events occurring in disjoint (separate) intervals of space-time are independent\n\n(Note that the assumption of a constant rate with independence over space-time is pretty strong and rarely holds completely)\nPoisson distribution\n\nCounts the events occurring at a constant rate \\(\\lambda\\) in a specified time (and space) \\(t\\)\n\nIndependent intervals of time and space\n\nProbability distribution has parameter \\(\\lambda &gt; 0\\)\n\nFor \\(k = 0, 1, 2, \\ldots\\)\n\\[\\textrm{Pr}(Y = k) = \\frac{e^{-\\lambda t} (\\lambda t)^k}{k!}\\]\nMean: \\(E[Y] = \\lambda t\\)\nVar: \\(V[Y] = \\lambda t\\)\n(Mean-variance relationship, like binary data)\n\n\n\n\n\n2.2 Regression Model\n\nWhen the response variable represent counts of some event, we usually model using the (log) rate with Poisson regression\n\nCompares rates of response per space-time (e.g. person-years) across groups\n“Rate ratio”\n\nWhy not use linear regression? The reasons are primarily statistical\n\nThe rate is in fact a mean\nFor Poisson \\(Y\\) having event rate \\(\\lambda\\) measured over time \\(t\\)\n\nThe mean is equal to the variance (both are \\(\\lambda t\\))\n\nWe want to be able to account for\n\nDifferent areas of space or length of time for measuring counts\nMean-variance relationship (if not using robust standard errors)\n\n\nIn Poisson regression, we tend to use a log link when modeling the event rate\n\nAs in other models, a log link means that we are assuming a multiplicative modeling\n\nMultiplicative model \\(\\rightarrow\\) comparisons between groups based on ratios\nAdditive model \\(\\rightarrow\\) comparisons between groups based on differences\n\nLog link also has the best technical statistical properties\n\nLog rate is the “canonical parameter” for the Poisson distribution\nBeing the canonical parameter makes the calculus and mathematical properties easier to derive, and thus easier to understand from a theoretical perspective\n\n\nPoisson regression\n\nResponse variable is count of event over space-time (often person-years)\nOffset variable specifies amount of space-time\nAllows continuous or multiple grouping variables\n\nBut will also work with binary grouping variables\n\n\nSimple Poisson Regression\n\nModeling rate of count response \\(Y\\) on predictor \\(X\\)\n\nDistribution: \\(\\textrm{Pr}(Y_i = k | T_i = t_i) = \\frac{e^{-\\lambda_i t_i} (\\lambda_i t_i)^k}{k!}\\)\nModel: \\(\\textrm{log } E[Y_i | T_i, X_i] = \\textrm{log}\\left(\\lambda_i T_i\\right) = \\textrm{log}(T_i) + \\beta_0 + \\beta_1 \\times X_i\\)\n\\(X_i = 0\\): log \\(\\lambda_i = \\beta_0\\)\n\\(X_i = x\\): log \\(\\lambda_i = \\beta_0 + \\beta_1 \\times x\\)\n\\(X_i = x+1\\): log \\(\\lambda_i = \\beta_0 + \\beta_1 \\times x + \\beta_1\\)\n\nTo interpret as rates, exponentiate the parameters\n\nDistribution: \\(\\textrm{Pr}(Y_i = k | T_i = t_i) = \\frac{e^{-\\lambda_i t_i} (\\lambda_i t_i)^k}{k!}\\)\nModel: \\(\\textrm{log } E[Y_i | T_i, X_i] = \\textrm{log}\\left(\\lambda_i T_i\\right) = \\textrm{log}(T_i) + \\beta_0 + \\beta_1 \\times X_i\\)\n\\(X_i = 0\\): \\(\\lambda_i = e^{\\beta_0}\\)\n\\(X_i = x\\): \\(\\lambda_i = e^{\\beta_0 + \\beta_1 \\times x}\\)\n\\(X_i = x+1\\): \\(\\lambda_i = e^{\\beta_0 + \\beta_1 \\times x + \\beta_1}\\)\n\n\nInterpretation of the model\n\nIntercept\n\nRate when the predictor is \\(0\\) is found by exponentiation of the intercept from Poisson regression: \\(e^{\\beta_0}\\)\n\nSlope\n\nRate ratio between groups differing in the value of the predictor by 1 unit is found by exponentiation of the slope from Poisson regression: \\(e^{\\beta_1}\\)"
  },
  {
    "objectID": "notes/Lec09.Poisson.html#example-acid-reflux-and-bmi",
    "href": "notes/Lec09.Poisson.html#example-acid-reflux-and-bmi",
    "title": "Poisson Regression",
    "section": "3 Example: Acid reflux and BMI",
    "text": "3 Example: Acid reflux and BMI\n\n3.1 Data description\n\nResearch question: Are the number of acid reflux events in a day related to body mass index (BMI)?\nEach subject pH in the esophagus in monitored continuously for about 24 hours\nCount the number of time pH drop below 4, which is called a “reflux event”\nAnalysis (statistical) goals\n\nPrimary goal: Determine if there is an association between BMI and acid reflux rate\nSecondary goal: Describe the (mean) trend in reflux rates as a function of BMI\n\nVariables\n\nResponse: Number of acid reflux events\nOffset: Number of minutes subject was monitored\nPredictor of interest: BMI\nOther covariates: Presence of esophagitis at baseline\n\n\n\n\n3.2 Descriptive Plots\n\n\nCode\nbmi.data &lt;- read.csv(\"data/bmi.csv\", header=TRUE)\n# Events are pH less than 4\nbmi.data$events &lt;- bmi.data$totalmins4\n\nggplot(bmi.data, aes(x=bmi, y=events)) + geom_point() + geom_smooth() + theme_bw() + xlab(\"BMI\") + ylab(\"Number of reflux events\")\n\n\n\n\n\nBMI by number of reflux events\n\n\n\n\n\n\nCode\nbmi.data$rate &lt;- bmi.data$totalmins4/bmi.data$totalmins*60*24\nggplot(bmi.data, aes(x=bmi, y=rate)) + geom_point() + geom_smooth() + theme_bw() + xlab(\"BMI\") + ylab(\"Reflux events per day\")\n\n\n\n\n\nBMI by number of reflux events per day\n\n\n\n\n\nCharacterization of plots\n\nPlots are visually similar if we consider the rate (events per day) or the raw number of events\nFirst order trend: Event rate increases with increasing BMI\nSecond order trend: Event rate increase until BMI of 32 (or so) and then flattens out\nWithin-group variability\n\nHard to visualize from the plots\nModel assumes increasing variability with increasing BMI, which looks reasonable\n\n\n\n\n\n3.3 Regression commands\n\nAs before, but need to specify the offset\n\nOffset is the log of the exposure time\nIn Stata, can alternatively specify the “exposure” and it will take the log for you\n\nStata\n\npoisson respvar predvar, exposure(time) [robust]\npoisson respvar predvar, offset(logtime) [robust]\n\nR\n\nOne method to fit Poisson models\n\nUses the sandwich and lmtest libraries\nMust install the above two libraries using install.packages(\"lmtest\") and install.packages(\"sandwich\")\nmodel.poisson &lt;- glm(response ~ predictors + offset(log(time)), data=data, family=\"poisson\")\ncoeftest(model.poisson, vcov=sandwich)\n\nAnother method to fit Poisson models using the Design package\n\nm1 &lt;- glmD(response ~ predictors + offset(log(time)), data=data, family=\"poisson\", x=TRUE, y=TRUE)\nbootcov(m1) for robust (bootstrap) confidence intervals\n\nCan also use methods within the gee library\n\n\n\n\n3.4 Estimation of the regression model\n\nRegression model for number of reflux events on BMI\n\nAnswer primary research question: Is there an association between BMI and the acid reflux event rate?\nEstimate the best fitting line to (log) number of reflux events within BMI groups using an offset of log time\n\n\\(\\textrm{log}(\\textrm{Events} | \\textrm{BMI}) = \\beta_0 + \\beta_1 \\times \\textrm{BMI} + \\textrm{log}(\\textrm{time})\\)\n\nAn association will exist if the slope \\(\\beta_1\\) is nonzero\n\n\n\n\nCode\nlibrary(lmtest)\nlibrary(sandwich)\nm1.poisson &lt;- glm(events ~ bmi + offset(log(totalmins)), data=bmi.data, family=\"poisson\")\nm1.poisson\n\n\n\nCall:  glm(formula = events ~ bmi + offset(log(totalmins)), family = \"poisson\", \n    data = bmi.data)\n\nCoefficients:\n(Intercept)          bmi  \n   -3.11999      0.02232  \n\nDegrees of Freedom: 278 Total (i.e. Null);  277 Residual\nNull Deviance:      22040 \nResidual Deviance: 20790    AIC: 22730\n\n\nCode\ncoeftest(m1.poisson, vcov=sandwich)\n\n\n\nz test of coefficients:\n\n              Estimate Std. Error z value  Pr(&gt;|z|)    \n(Intercept) -3.1199914  0.1392708 -22.402 &lt; 2.2e-16 ***\nbmi          0.0223194  0.0046038   4.848 1.247e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\nconfint(coeftest(m1.poisson, vcov=sandwich))\n\n\n                  2.5 %      97.5 %\n(Intercept) -3.39295712 -2.84702577\nbmi          0.01329611  0.03134273\n\n\n. poisson events bmi, offset(logmins) robust\n\nIteration 0:   log pseudolikelihood =  -11360.89  \nIteration 1:   log pseudolikelihood =  -11360.89  \n\nPoisson regression                                Number of obs   =        279\n                                                  Wald chi2(1)    =      23.42\n                                                  Prob &gt; chi2     =     0.0000\nLog pseudolikelihood =  -11360.89                 Pseudo R2       =     0.0520\n\n------------------------------------------------------------------------------\n             |               Robust\n      events |      Coef.   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n         bmi |   .0223194   .0046121     4.84   0.000     .0132799    .0313589\n       _cons |  -3.119991    .139521   -22.36   0.000    -3.393448   -2.846535\n     logmins |   (offset)\n------------------------------------------------------------------------------\n\nInterpretation of output\n\n\\(\\textrm{log rate} = -3.119991 + 0.0223194 \\times \\textrm{BMI}\\)\n\nInterpretation of intercept\n\nEstimated event rate when BMI is 0 is found by exponentiation: \\(e^{-3.12} = 0.044\\)\nThis is the rate per 2-minute interval. This unusual time interval is an artifact of the way in pH data is sampled\n\nTo convert to events per day, multiply by 720 (there are 720 2-minute intervals in a day)\n\\(720 \\times e^{-3.12} = 31.7 \\textrm{ events per day}\\)\n\n\nInterpretation of slope\n\nEstimated ratio of rates for two subjects differing by 1 in their BMI\nInterpretation by exponentiation of slope\n\nA subject with a 1 \\(\\textrm{kg} / m^2\\) higher BMI will have an acid reflux event rate that is \\(2.3\\%\\) higher. (calc: \\(e^{0.0223} = 1.023\\))\nWe are 95% confident that the increase in event rate is between \\(1.3\\%\\) higher and \\(3.2\\%\\) higher\nThere is a significant association between BMI and reflux events \\(p &lt; 0.001\\)"
  },
  {
    "objectID": "notes/Lec09.Poisson.html#example-acid-reflux-and-bmi-by-esophagitis-status",
    "href": "notes/Lec09.Poisson.html#example-acid-reflux-and-bmi-by-esophagitis-status",
    "title": "Poisson Regression",
    "section": "4 Example: Acid reflux and BMI by esophagitis status",
    "text": "4 Example: Acid reflux and BMI by esophagitis status\n\n4.1 BMI modeled as a linear term\n\nThe following results compare using a Poisson model to a linear regression model\nBoth models will control for Esophagitis status, so any interpretation must involve “Holding esophagitis status constant...” (“Among subjects with the same Esophagitis status...”)\nNote the different (numerical) estimates for the coefficients and standard errors for BMI and esophagitis, but the similar statistical significance\nAlso if we plot the predicted number of events per day versus BMI, the results are similar from either model\n\n\n4.1.1 R: Poisson regression of events with offset for log(total time monitored)\n\n\nCode\nm2.poisson &lt;- glm(events ~ bmi + esop + offset(log(totalmins)), data=bmi.data, family=\"poisson\")\nm2.poisson\n\n\n\nCall:  glm(formula = events ~ bmi + esop + offset(log(totalmins)), family = \"poisson\", \n    data = bmi.data)\n\nCoefficients:\n(Intercept)          bmi         esop  \n   -3.08903      0.01975      0.26222  \n\nDegrees of Freedom: 278 Total (i.e. Null);  276 Residual\nNull Deviance:      22040 \nResidual Deviance: 20220    AIC: 22150\n\n\nCode\ncoeftest(m2.poisson, vcov=sandwich)\n\n\n\nz test of coefficients:\n\n              Estimate Std. Error  z value  Pr(&gt;|z|)    \n(Intercept) -3.0890333  0.1420486 -21.7463 &lt; 2.2e-16 ***\nbmi          0.0197465  0.0047636   4.1453 3.393e-05 ***\nesop         0.2622171  0.0830528   3.1572  0.001593 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\nconfint(coeftest(m2.poisson, vcov=sandwich))\n\n\n                  2.5 %      97.5 %\n(Intercept) -3.36744333 -2.81062324\nbmi          0.01041013  0.02908293\nesop         0.09943667  0.42499753\n\n\nCode\n# Equivalent command\n#coefci(m2.poisson, vcov=sandwich)\n\n\n\n\n4.1.2 R: Linear regression of rate (events/time) as outcome\n\n\nCode\nbmi.data$rate &lt;- bmi.data$events / bmi.data$totalmins\nm2.lm &lt;-  lm(rate ~ bmi + esop, data=bmi.data)\nm2.lm\n\n\n\nCall:\nlm(formula = rate ~ bmi + esop, data = bmi.data)\n\nCoefficients:\n(Intercept)          bmi         esop  \n   0.027846     0.001839     0.025104  \n\n\nCode\ncoeftest(m2.lm, vcov=sandwich)\n\n\n\nt test of coefficients:\n\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.02784611 0.01283576  2.1694 0.030905 *  \nbmi         0.00183903 0.00045933  4.0037 8.02e-05 ***\nesop        0.02510400 0.00849883  2.9538 0.003409 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\nconfint(coeftest(m2.lm, vcov=sandwich))\n\n\n                   2.5 %      97.5 %\n(Intercept) 0.0025776689 0.053114542\nbmi         0.0009347996 0.002743261\nesop        0.0083732460 0.041834761\n\n\n\n\n4.1.3 Stata Output\n. poisson events bmi esop, offset(logmins) robust\n\nPoisson regression                                Number of obs   =        279\n                                                  Wald chi2(2)    =      30.30\n                                                  Prob &gt; chi2     =     0.0000\nLog pseudolikelihood = -11072.339                 Pseudo R2       =     0.0761\n\n------------------------------------------------------------------------------\n             |               Robust\n      events |      Coef.   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n         bmi |   .0197465   .0047721     4.14   0.000     .0103934    .0290997\n        esop |   .2622171    .083202     3.15   0.002     .0991442      .42529\n       _cons |  -3.089033   .1423038   -21.71   0.000    -3.367944   -2.810123\n     logmins |   (offset)\n------------------------------------------------------------------------------\n\n\n. gen eventsmins = events / mins\n. regress eventsmins bmi esop, robust\n\nLinear regression                                      Number of obs =     279\n                                                       F(  2,   276) =   14.16\n                                                       Prob &gt; F      =  0.0000\n                                                       R-squared     =  0.0856\n                                                       Root MSE      =  .05102\n\n------------------------------------------------------------------------------\n             |               Robust\n  eventsmins |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n         bmi |    .001839   .0004618     3.98   0.000     .0009299    .0027482\n        esop |    .025104   .0085449     2.94   0.004     .0082826    .0419254\n       _cons |   .0278461   .0129053     2.16   0.032     .0024407    .0532515\n------------------------------------------------------------------------------\n\n\n4.1.4 Comparison of predicted number of events from linear regression and Poisson regression models\n\nExample prediction calculations: BMI=30, with esophagitis\n\nLinear regression: \\(0.0278461 + .025104 + .001839 \\times 30 = 0.108\\)\n\nStata: adjust bmi=30 esop=1\n\nPoisson regression: \\(e^{-3.089033 + 0.2622171 + .01975465 \\times 30} = 0.107\\)\n\nStata: adjust bmi=30 esop=1, nooffset exp\n\nRemember the above rates are for a 2-minute time interval. To convert to daily rates, multiply by 720\n\n\nCode\n# Predicted events per two minute time interval\npredict(m2.lm, newdata=data.frame(bmi=30,esop=1))\n\n\n       1 \n0.108121 \n\n\nCode\nexp(predict(m2.poisson, newdata=data.frame(bmi=30,esop=1,totalmins=1), type=\"link\"))\n\n\n        1 \n0.1070542 \n\n\nCode\n# Predicted events per day (720 2-minute time intervals per day)\npredict(m2.lm, newdata=data.frame(bmi=30,esop=1))*720\n\n\n       1 \n77.84713 \n\n\nCode\nexp(predict(m2.poisson, newdata=data.frame(bmi=30,esop=1,totalmins=720), type=\"link\"))\n\n\n       1 \n77.07901 \n\n\n\n\n\n\nCode\nbmi.data$esop.factor &lt;- factor(bmi.data$esop, levels=0:1, labels=c(\"Esop Neg\", \"Esop Pos\"))\nm.spline2.adj &lt;- glm(events ~ bmi + offset(log(totalmins)) + esop, data=bmi.data, family=\"poisson\")\nm.spline3.adj &lt;- lm(events / totalmins ~ bmi + esop, data=bmi.data)\npar(mfrow=c(1,2), mar=c(5,4,4,0.5))\nplot(18:40, exp(predict(m.spline2.adj, newdata=data.frame(bmi=18:40, totalmins=720, esop=1), type=\"link\")), type='l', ylab=\"Predicted number of events per day\", xlab=\"BMI\", ylim=c(30,100), main=\"Poisson Reg\")\naxis(4, labels=FALSE, ticks=TRUE)\nlegend(\"bottomright\", c(\"Esophagitis Pos\",\"Esophagitis Neg\"), inset=0.05, col=1, lty=1:2)\nlines(18:40, exp(predict(m.spline2.adj, newdata=data.frame(bmi=18:40, totalmins=720, esop=0), type=\"link\")), lty=2)\nplot(18:40, 720*predict(m.spline3.adj, newdata=data.frame(bmi=18:40, esop=1), type=\"response\"), type='l', col='Blue', ylab=\"\", xlab=\"BMI\", ylim=c(30,100), main=\"Linear Reg\", axes=FALSE)\naxis(1)\naxis(4)\naxis(2, labels=FALSE, ticks=TRUE)\nbox()\nlines(18:40, 720*predict(m.spline3.adj, newdata=data.frame(bmi=18:40, esop=0), type=\"response\"), type='l', col='Blue', lty=2)\nlegend(\"bottomright\", c(\"Esophagitis Pos\",\"Esophagitis Neg\"), inset=0.05, col=\"Blue\", lty=1:2)\n\n\n\n\n\n\n\n\n4.2 BMI modeled using splines\n\nRegression splines are handled more naturally in R than in Stata\n\nglm(events ~ ns(bmi,4) + esop + offset(log(totalmins)), data=bmi.data, family=\"poisson\")\nns(bmi, 4) specified a natural spline for bmi with 4 degrees of freedom\n\nNote that there is an optical illusion in the following plots\n\nFor both plots, it appears as if the lines are closer in the middle ranges of BMI\nFor the Poisson regression, the true distance between lines is increasing with increasing with BMI\nFor the Linear regrression, the true distance between lines is constant\n\n\n\n\nCode\nm.spline2.adj &lt;- glm(events ~ ns(bmi,4) + offset(log(totalmins)) + esop, data=bmi.data, family=\"poisson\")\nm.spline3.adj &lt;- lm(events / totalmins ~ ns(bmi,4) + esop, data=bmi.data)\npar(mfrow=c(1,2), mar=c(5,4,4,0.5))\nplot(18:40, exp(predict(m.spline2.adj, newdata=data.frame(bmi=18:40, totalmins=720, esop=1), type=\"link\")), type='l', ylab=\"Predicted number of events per day\", xlab=\"BMI\", ylim=c(0,100), main=\"Poisson Reg\")\naxis(4, labels=FALSE, ticks=TRUE)\nlegend(\"bottomright\", c(\"Esophagitis Pos\",\"Esophagitis Neg\"), inset=0.05, col=1, lty=1:2)\nlines(18:40, exp(predict(m.spline2.adj, newdata=data.frame(bmi=18:40, totalmins=720, esop=0), type=\"link\")), lty=2)\nplot(18:40, 720*predict(m.spline3.adj, newdata=data.frame(bmi=18:40, esop=1), type=\"response\"), type='l', col='Blue', ylab=\"\", xlab=\"BMI\", ylim=c(0,100), main=\"Linear Reg\", axes=FALSE)\naxis(1)\naxis(4)\naxis(2, labels=FALSE, ticks=TRUE)\nbox()\nlines(18:40, 720*predict(m.spline3.adj, newdata=data.frame(bmi=18:40, esop=0), type=\"response\"), type='l', col='Blue', lty=2)\nlegend(\"bottomright\", c(\"Esophagitis Pos\",\"Esophagitis Neg\"), inset=0.05, col=\"Blue\", lty=1:2)\n\n\n\n\n\n\n\n4.3 Comparison of modeling linear BMI to using spline function\n\nFor all regression models, we are more confident modeling associations than predicting means\nWhen we use a linear term (i.e. a straight line) for the predictor, we are modeling a first-order association\n\nMost power to detect this type of association\nAlways need to check that a first-order association answers the scientific question\n\nCounter example: Interested in seasonal trends in air pollution. A linear effect of time would only answer if air pollution levels are increasing/decreasing over time, not how they are changing from month to month\n\n\nFlexible functions for predictors, including splines, are, in general, more useful if we care about predicting means or individual observations\nAcid reflux example: Which model you choose depends on the scientific goals\n\nPrimary goal: Is there an association between BMI and the rate of acid reflux?\n\nFitting the linear BMI term answers this question\n\nSecondary goal: Describe the (mean) trend in reflux rates as a function of BMI\n\nA priori, I would be less inclined to believe a linear function captures the true mean relationship\nTo answer this scientific question, a spline analysis is preferred"
  },
  {
    "objectID": "labs/lab5-multivariable.html",
    "href": "labs/lab5-multivariable.html",
    "title": "Lab 5: Multivariable Regression",
    "section": "",
    "text": "Another way to think about regression is the amount of variablity in the outcome (Y) that is explained by the predictors (X). In simple linear regression, we regress X on Y because we believe that X will explain some of the variability in Y. This leads to an alternate way of thinking about statistical tests for regression coefficients in terms of the variability of the outcome. Specifically, the null hypothesis is that X explains none of the variability in Y, and the alternative hypothesis is that X explains more variability than would be expected by chance alone. In this lab we will consider the interpretation of statistical tests in a multivariable model that adds another predictor (W) to the model. Salary will be outcome (Y), male gender the predictor of interest (X), and year of degree the additional covariate (W).\nAn added-variable plot is a scatterplot of the transformations of an independent variable (say, \\(X_1\\)) and the dependent variable (\\(y\\)) that nets out the influence of all the other independent variables. The fitted regression line through the origin between these transformed variables has the same slope as the coefficient on x1 in the full regression model which includes all the independent variables. An added-variable plot is the multivariable analogue of using a simple scatterplot with a regression fit when there are no other covariates to show the relationship between a single x variable and a y variable. An added-variable plot is a visually compelling method for showing the nature of the partial correlation between \\(X_1\\) and y as estimated in a multiple regression.\n\n\nTo obtain residuals for a model fit, use the resid() function on the model fit.\navPlots in the car packages will added-variable, also called partial-regression, plots for linear and generalized linear models.\nIn part 1 and part 2 of the lab, we will construct added variable plots in steps without the use of a specialized function."
  },
  {
    "objectID": "labs/lab5-multivariable.html#background",
    "href": "labs/lab5-multivariable.html#background",
    "title": "Lab 5: Multivariable Regression",
    "section": "",
    "text": "Another way to think about regression is the amount of variablity in the outcome (Y) that is explained by the predictors (X). In simple linear regression, we regress X on Y because we believe that X will explain some of the variability in Y. This leads to an alternate way of thinking about statistical tests for regression coefficients in terms of the variability of the outcome. Specifically, the null hypothesis is that X explains none of the variability in Y, and the alternative hypothesis is that X explains more variability than would be expected by chance alone. In this lab we will consider the interpretation of statistical tests in a multivariable model that adds another predictor (W) to the model. Salary will be outcome (Y), male gender the predictor of interest (X), and year of degree the additional covariate (W).\nAn added-variable plot is a scatterplot of the transformations of an independent variable (say, \\(X_1\\)) and the dependent variable (\\(y\\)) that nets out the influence of all the other independent variables. The fitted regression line through the origin between these transformed variables has the same slope as the coefficient on x1 in the full regression model which includes all the independent variables. An added-variable plot is the multivariable analogue of using a simple scatterplot with a regression fit when there are no other covariates to show the relationship between a single x variable and a y variable. An added-variable plot is a visually compelling method for showing the nature of the partial correlation between \\(X_1\\) and y as estimated in a multiple regression.\n\n\nTo obtain residuals for a model fit, use the resid() function on the model fit.\navPlots in the car packages will added-variable, also called partial-regression, plots for linear and generalized linear models.\nIn part 1 and part 2 of the lab, we will construct added variable plots in steps without the use of a specialized function."
  },
  {
    "objectID": "labs/lab5-multivariable.html#example-1-duncan-data",
    "href": "labs/lab5-multivariable.html#example-1-duncan-data",
    "title": "Lab 5: Multivariable Regression",
    "section": "Example 1: Duncan Data",
    "text": "Example 1: Duncan Data\nDuncan’s Occupational Prestige. Data on the prestige and other characteristics of 45 U. S. occupations in 1950.\n\nlibrary(car)\n\nLoading required package: carData\n\nhead(Duncan)\n\n           type income education prestige\naccountant prof     62        86       82\npilot      prof     72        76       83\narchitect  prof     75        92       90\nauthor     prof     55        90       76\nchemist    prof     64        86       90\nminister   prof     21        84       87\n\n\nObtain summary statistics for the variables in Duncan:\n\nsummary(Duncan)\n\n   type        income        education         prestige    \n bc  :21   Min.   : 7.00   Min.   :  7.00   Min.   : 3.00  \n prof:18   1st Qu.:21.00   1st Qu.: 26.00   1st Qu.:16.00  \n wc  : 6   Median :42.00   Median : 45.00   Median :41.00  \n           Mean   :41.87   Mean   : 52.56   Mean   :47.69  \n           3rd Qu.:64.00   3rd Qu.: 84.00   3rd Qu.:81.00  \n           Max.   :81.00   Max.   :100.00   Max.   :97.00  \n\n\nAs a first graph, we view a histogram of the variable prestige:\n\nwith(Duncan, hist(prestige))\n\n\n\n\n\nExamining the Data\nThe scatterplotMatrix() function in the car package produces scatterplots for all paris of variables. A few relatively remote points are marked by case names, in this instance by occupation.\n\nscatterplotMatrix( ~ prestige + education + income, \n    id=list(n=3), data=Duncan)\n\n\n\n\n\n\nRegression Analysis\nWe use thelm() function to fit a linear regression model to the data:\n\n(duncan.model &lt;- lm(prestige ~ education + income, data=Duncan))\n\n\nCall:\nlm(formula = prestige ~ education + income, data = Duncan)\n\nCoefficients:\n(Intercept)    education       income  \n    -6.0647       0.5458       0.5987  \n\n\n\nsummary(duncan.model)\n\n\nCall:\nlm(formula = prestige ~ education + income, data = Duncan)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.538  -6.417   0.655   6.605  34.641 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -6.06466    4.27194  -1.420    0.163    \neducation    0.54583    0.09825   5.555 1.73e-06 ***\nincome       0.59873    0.11967   5.003 1.05e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.37 on 42 degrees of freedom\nMultiple R-squared:  0.8282,    Adjusted R-squared:   0.82 \nF-statistic: 101.2 on 2 and 42 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nAdded-variable plot\nAdded-variable plots for Duncan’s regression, looking for influential cases:\n\navPlots(duncan.model, \n    id=list(cex=0.75, n=3, method=\"mahal\"))"
  },
  {
    "objectID": "labs/lab5-multivariable.html#example-2-fev-and-smoking-age-height",
    "href": "labs/lab5-multivariable.html#example-2-fev-and-smoking-age-height",
    "title": "Lab 5: Multivariable Regression",
    "section": "Example 2: FEV and smoking, age, height",
    "text": "Example 2: FEV and smoking, age, height\nRecall the FEV data presented in the notes\n\nIn unadjusted models, smokers had lower FEV that non-smokers. We suspect this is due to confouding by age\nAge is suspected to be associated with smoking (in the sample) and FEV (in truth)\nIn unadjusted models, height is likely associated with FEV and age. There is likely an association between height and smoking through age.\nIn adjusted models, we might expect\n\n\nLower FEV comparing a smoker to non-smoker of the same age and height\nHigher FEV comparing individuals differing by on year of age but with the same smoking status and height\nHigher FEV in taller individuals of the same age and smoking status\n\nTry to identify these observations in the plots that follow\n\nfev &lt;- read.table(\"http://www.emersonstatistics.com/Datasets/fev.txt\", header = TRUE)\nfev$smoker &lt;- ifelse(fev$smoke == 2, 0, 1)\nsummary(fev)\n\n     seqnbr          subjid           age              fev       \n Min.   :  1.0   Min.   :  201   Min.   : 3.000   Min.   :0.791  \n 1st Qu.:164.2   1st Qu.:15811   1st Qu.: 8.000   1st Qu.:1.981  \n Median :327.5   Median :36071   Median :10.000   Median :2.547  \n Mean   :327.5   Mean   :37170   Mean   : 9.931   Mean   :2.637  \n 3rd Qu.:490.8   3rd Qu.:53638   3rd Qu.:12.000   3rd Qu.:3.119  \n Max.   :654.0   Max.   :90001   Max.   :19.000   Max.   :5.793  \n     height           sex            smoke           smoker       \n Min.   :46.00   Min.   :1.000   Min.   :1.000   Min.   :0.00000  \n 1st Qu.:57.00   1st Qu.:1.000   1st Qu.:2.000   1st Qu.:0.00000  \n Median :61.50   Median :1.000   Median :2.000   Median :0.00000  \n Mean   :61.14   Mean   :1.486   Mean   :1.901   Mean   :0.09939  \n 3rd Qu.:65.50   3rd Qu.:2.000   3rd Qu.:2.000   3rd Qu.:0.00000  \n Max.   :74.00   Max.   :2.000   Max.   :2.000   Max.   :1.00000  \n\n\n\nScatterplot matrix\nThe scatterplot matrix summarizes the unadjusted associations.\n\nscatterplotMatrix( ~ fev + age + height + smoker, data=fev)\n\n\n\n\n\nRegression Analysis\nWe use thelm() function to fit a linear regression model to the data in those 9 and older.\n\n(fev.model &lt;- lm(fev ~ smoker + height + age, data=fev[fev$age&gt;=9,]))\n\n\nCall:\nlm(formula = fev ~ smoker + height + age, data = fev[fev$age &gt;= \n    9, ])\n\nCoefficients:\n(Intercept)       smoker       height          age  \n    -6.4313      -0.1784       0.1353       0.0712  \n\n\n\nsummary(fev.model)\n\n\nCall:\nlm(formula = fev ~ smoker + height + age, data = fev[fev$age &gt;= \n    9, ])\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.63401 -0.27416  0.00003  0.26550  1.82040 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -6.431311   0.355959 -18.068  &lt; 2e-16 ***\nsmoker      -0.178429   0.065107  -2.741  0.00639 ** \nheight       0.135303   0.006322  21.401  &lt; 2e-16 ***\nage          0.071201   0.011884   5.991 4.37e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4478 on 435 degrees of freedom\nMultiple R-squared:  0.6632,    Adjusted R-squared:  0.6609 \nF-statistic: 285.5 on 3 and 435 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nAdded-variable plot\nAdded-variable plots for FEV regression to visualize the adjusted associations\n\navPlots(fev.model)"
  },
  {
    "objectID": "labs/lab5-multivariable.html#initial-lab-setup",
    "href": "labs/lab5-multivariable.html#initial-lab-setup",
    "title": "Lab 5: Multivariable Regression",
    "section": "Initial Lab Setup",
    "text": "Initial Lab Setup\nFor this lab, we will be using salary data from they year 1995. We will focus on the variables: salary, sex, and yrdeg\nInitial dataset manipulations\n\nRead in the salary dataset\nRemove any observations that are not from 1995 (use the ‘year’ variable)\nDescribe the dataset\nCreate an indicator variable for male gender"
  },
  {
    "objectID": "labs/lab5-multivariable.html#lab-part-1",
    "href": "labs/lab5-multivariable.html#lab-part-1",
    "title": "Lab 5: Multivariable Regression",
    "section": "Lab Part 1",
    "text": "Lab Part 1\n\nModel 1: Fit a simple linear regression model with salary as the outcome and male as the predictor. Save the residuals from this model. Interpret these residuals in terms of the unexplained variability in salary.\n\n\nModel 2: Fit a simple linear regression model with yrdeg as the outcome and male as the predictor. Save the residuals from this model. Interpret these residuals in terms of the unexplained variability in yrdeg.\n\n\nPlot the residuals from model 2 (X-axis) versus the residuals from model 1 (y-axis). Describe any association you see. It may be helpful to add a lowess smooth or other smooth line to the plot.\n\n\nModel 3: Fit a simple linear regression model using the residuals from model 1 as the outcome and the residuals from model 2 as the predictor. Interpret the slope coefficient from this model.\n\n\nModel 4: Fit a multivariable linear regression model with salary as the outcome using predictors male and yrdeg. What is the interpretation of the male coefficient in this model? What is the interpretation of the yrdeg coefficient?\n\n\nCompare the slope estimate for yrdeg from Model 3 to the slope estimate obtained in Model 4. Explain your findings."
  },
  {
    "objectID": "labs/lab5-multivariable.html#lab-part-2",
    "href": "labs/lab5-multivariable.html#lab-part-2",
    "title": "Lab 5: Multivariable Regression",
    "section": "Lab Part 2",
    "text": "Lab Part 2\n\nCreate a new variable FULL that takes on the value 1 for full professors and 0 for Assistant or Associate Professors.\n\n\nDetermine if FULL explains some of the variability in salary after adjusting for year of degree and gender by fitting the multivariable regression model and by regressing residuals from “Model A” on the residuals from “Model B” other as was done previously (you will need to figure out what “Model A” and “Model B” should be). Compare the results from the two models."
  },
  {
    "objectID": "labs/quarto-intro.html",
    "href": "labs/quarto-intro.html",
    "title": "Quarto Introduction",
    "section": "",
    "text": "Open the quarto introduction file in your Rstudio and render to create an html output file\nLearn some basic markdown syntax to include text and run code\nAlter the YAML to include your name and output to a different file format (e.g. docx)"
  },
  {
    "objectID": "labs/quarto-intro.html#learning-objectives",
    "href": "labs/quarto-intro.html#learning-objectives",
    "title": "Quarto Introduction",
    "section": "",
    "text": "Open the quarto introduction file in your Rstudio and render to create an html output file\nLearn some basic markdown syntax to include text and run code\nAlter the YAML to include your name and output to a different file format (e.g. docx)"
  },
  {
    "objectID": "labs/quarto-intro.html#r-packages-used",
    "href": "labs/quarto-intro.html#r-packages-used",
    "title": "Quarto Introduction",
    "section": "R packages used",
    "text": "R packages used\nFor this introduction, I used three R packages. If you have never used these before, they will need to be installed once, e.g.\ninstall.packages(\"ggplot2\")\ninstall.packages(\"rms\")\ninstall.packages(\"Hmisc\")\nIf they have already been installed, you can load these packages.\n\nlibrary(ggplot2)\nlibrary(rms)\n\nLoading required package: Hmisc\n\n\nLoading required package: lattice\n\n\nLoading required package: survival\n\n\nLoading required package: Formula\n\n\n\nAttaching package: 'Hmisc'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    knitrSet\n\n\nThe following objects are masked from 'package:base':\n\n    format.pval, units\n\n\nLoading required package: SparseM\n\n\n\nAttaching package: 'SparseM'\n\n\nThe following object is masked from 'package:base':\n\n    backsolve\n\nlibrary(Hmisc)"
  },
  {
    "objectID": "labs/quarto-intro.html#quarto",
    "href": "labs/quarto-intro.html#quarto",
    "title": "Quarto Introduction",
    "section": "Quarto",
    "text": "Quarto\nQuarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org.\nQuarto includes many more features than I will cover in this course. It is not a learning objective of this course that you learn all of the features included in Quarto. Instead, I want to cover a few of the most useful things and provide a starting point for those interested in learning more."
  },
  {
    "objectID": "labs/quarto-intro.html#running-code",
    "href": "labs/quarto-intro.html#running-code",
    "title": "Quarto Introduction",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed).\n\nPlots\nYou can include plots\n\nggplot(cars, aes(x=speed, y=dist)) + \n  geom_smooth() + \n  xlab(\"Speed (mph)\") + \n  ylab(\"Stopping Distance (feet)\")\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nDate recorded in the 1920s\n\n\n\nSpeed and stopping distance of cars\n\n\n\nTables\nAnd tables that are simple text\n\n# Create some data\nset.seed(12345) # Fpr reproducibility\nexampledata &lt;- data.frame(age=rnorm(500,50,5),\n                          sbp=rnorm(500,120,12),\n                          trt=factor(sample(c(\"Drug\",\"Placebo\"), 500, replace=TRUE))\n)\nf &lt;- summaryM(age + sbp ~ trt, data=exampledata, test=TRUE)\nprint(f, digits=2)\n\n\n\nDescriptive Statistics  (N=500)\n\n+---+------------------+------------------+------------------------+\n|   |Drug              |Placebo           |  Test                  |\n|   |(N=260)           |(N=240)           |Statistic               |\n+---+------------------+------------------+------------------------+\n|age|        47/50/53  |        47/51/54  | F=0.5 d.f.=1,498 P=0.48|\n+---+------------------+------------------+------------------------+\n|sbp|       113/120/129|       112/120/128|F=0.02 d.f.=1,498 P=0.89|\n+---+------------------+------------------+------------------------+\n\n\nTable can also be formatted to improve appearance. Here is an html version of the same table.\n\nhtml(f, digits=3)\n\n\n\n\n\n\n\n\n\n\nDescriptive Statistics (N=500).\n\n\n\nDrug\nN=260\nPlacebo\nN=240\nTest Statistic\n\n\n\n\n\nage\n47.4 50.3 53.2\n47.0 50.7 54.0\nF1 498=0.5, P=0.479\n\n\nsbp\n113 120 129\n112 120 128\nF1 498=0.02, P=0.89\n\n\n\na b c represent the lower quartile a, the median b, and the upper quartile c for continuous variables.\nTest used: Wilcoxon test ."
  },
  {
    "objectID": "labs/quarto-intro.html#quarto-yaml",
    "href": "labs/quarto-intro.html#quarto-yaml",
    "title": "Quarto Introduction",
    "section": "Quarto YAML",
    "text": "Quarto YAML\nThe beginning of this document of the document contains metadata that controls the document appearance, output format, and many other options. The current lines were automatically generated when I create a new document (File &gt; New File &gt; Quarto Document…)\nFor longer documents, it can be useful to add a table of contents. A simple toc can added with the line.\ntoc: true\nAlso, the default output type is an html document. This is a good choice, but if you prefer other options, you can alter the yaml to produce Word files, PDFs, or many other options. Tab-completion is supported to see the various options.\nIn order to create PDFs you will need to install a recent distribution of TeX. We recommend the use of TinyTeX (which is based on TexLive), which you can install with the following command:\nquarto install tinytex\nMore details on authoring Quarto documents in Rstudio are available elsewhere."
  },
  {
    "objectID": "labs/quarto-intro.html#thing-to-try",
    "href": "labs/quarto-intro.html#thing-to-try",
    "title": "Quarto Introduction",
    "section": "Thing to try",
    "text": "Thing to try\n\nAlter the YAML to include your name\nAlter the YAML to output to a different file format, e.g. docx\nAdd some text, code, or graphics and render the document"
  },
  {
    "objectID": "labs/lab7-effectmodification-questions.html",
    "href": "labs/lab7-effectmodification-questions.html",
    "title": "Lab 7: Poisson Regression and Effect Modification",
    "section": "",
    "text": "In this lab, we will estimate the association between salary and gender and consider administrative responsibilities a potential effect modifier. To provide a Poisson Regresion review, we will consider salary to be a rate (dollars per month) and use the following Poisson regression model for the log of the salary rate (\\(\\lambda_i\\)). We will use robust standard error estimates\n\\(\\textrm{log}(\\lambda_i) = \\beta_0 + \\beta_M * \\textrm{Male} + \\beta_A * \\textrm{Admin} + \\beta_{MA} * \\textrm{Male} * \\textrm{Admin}\\)\n\n\nLoad packages\n\n\nLoading required package: Hmisc\n\n\nLoading required package: lattice\n\n\nLoading required package: survival\n\n\nLoading required package: Formula\n\n\nLoading required package: ggplot2\n\n\n\nAttaching package: 'Hmisc'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    knitrSet\n\n\nThe following objects are masked from 'package:base':\n\n    format.pval, units\n\n\nLoading required package: SparseM\n\n\n\nAttaching package: 'SparseM'\n\n\nThe following object is masked from 'package:base':\n\n    backsolve\n\n\nLoading required package: MASS\n\n\n\nAttaching package: 'biostat3'\n\n\nThe following object is masked from 'package:survival':\n\n    colon\n\n\nThe following object is masked from 'package:lattice':\n\n    melanoma\n\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following objects are masked from 'package:rms':\n\n    Predict, vif\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following object is masked from 'package:car':\n\n    recode\n\n\nThe following object is masked from 'package:MASS':\n\n    select\n\n\nThe following objects are masked from 'package:Hmisc':\n\n    src, summarize\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union"
  },
  {
    "objectID": "labs/lab7-effectmodification-questions.html#introduction",
    "href": "labs/lab7-effectmodification-questions.html#introduction",
    "title": "Lab 7: Poisson Regression and Effect Modification",
    "section": "",
    "text": "In this lab, we will estimate the association between salary and gender and consider administrative responsibilities a potential effect modifier. To provide a Poisson Regresion review, we will consider salary to be a rate (dollars per month) and use the following Poisson regression model for the log of the salary rate (\\(\\lambda_i\\)). We will use robust standard error estimates\n\\(\\textrm{log}(\\lambda_i) = \\beta_0 + \\beta_M * \\textrm{Male} + \\beta_A * \\textrm{Admin} + \\beta_{MA} * \\textrm{Male} * \\textrm{Admin}\\)\n\n\nLoad packages\n\n\nLoading required package: Hmisc\n\n\nLoading required package: lattice\n\n\nLoading required package: survival\n\n\nLoading required package: Formula\n\n\nLoading required package: ggplot2\n\n\n\nAttaching package: 'Hmisc'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    knitrSet\n\n\nThe following objects are masked from 'package:base':\n\n    format.pval, units\n\n\nLoading required package: SparseM\n\n\n\nAttaching package: 'SparseM'\n\n\nThe following object is masked from 'package:base':\n\n    backsolve\n\n\nLoading required package: MASS\n\n\n\nAttaching package: 'biostat3'\n\n\nThe following object is masked from 'package:survival':\n\n    colon\n\n\nThe following object is masked from 'package:lattice':\n\n    melanoma\n\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following objects are masked from 'package:rms':\n\n    Predict, vif\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following object is masked from 'package:car':\n\n    recode\n\n\nThe following object is masked from 'package:MASS':\n\n    select\n\n\nThe following objects are masked from 'package:Hmisc':\n\n    src, summarize\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union"
  },
  {
    "objectID": "labs/lab7-effectmodification-questions.html#part-1-initial-dataset-manipulation",
    "href": "labs/lab7-effectmodification-questions.html#part-1-initial-dataset-manipulation",
    "title": "Lab 7: Poisson Regression and Effect Modification",
    "section": "Part 1: Initial dataset manipulation",
    "text": "Part 1: Initial dataset manipulation\n\n1. Read in the salary dataset. Remove all observations that are not from 1995. Create an indicator variable for male gender\n\n# Initial dataset manipulations\n\ndata &lt;- stata.get(\"http://biostat.app.vumc.org/wiki/pub/Main/CourseBios312/salary.dta\")\ndata95 &lt;- subset(data, year==\"95\")\ndata95$male &lt;- as.numeric(data95$sex==\"M\")"
  },
  {
    "objectID": "labs/lab7-effectmodification-questions.html#part-2-poisson-model",
    "href": "labs/lab7-effectmodification-questions.html#part-2-poisson-model",
    "title": "Lab 7: Poisson Regression and Effect Modification",
    "section": "Part 2: Poisson Model",
    "text": "Part 2: Poisson Model\n\n2.1 In the above model, we did not specify an offset. Why was an offset not used needed in this situation? Give an example of where it might be appropriate to include an offset for the amount of money earned.\n\n\n2.2 An underlying assumption of the Poisson model is that the mean equals the varaince. Consider the following summary statistics of the mean and the variance of salary overall and by admin duties and sex.\n\ndata95 %&gt;% summarize(mean = mean(salary, na.rm = TRUE),\n                     var = var(salary, na.rm=TRUE))\n\n      mean     var\n1 6389.808 4148443\n\ndata95 %&gt;% group_by(sex, admin) %&gt;%\n  summarize(mean = mean(salary, na.rm = TRUE),\n                     var = var(salary, na.rm=TRUE))\n\n`summarise()` has grouped output by 'sex'. You can override using the `.groups`\nargument.\n\n\n# A tibble: 4 × 4\n# Groups:   sex [2]\n  sex   admin  mean      var\n  &lt;chr&gt; &lt;int&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 F         0 5280. 1988395.\n2 F         1 6770. 2647747.\n3 M         0 6507. 4010738.\n4 M         1 8458. 3756893.\n\n\n\n2.2.1 For evaluating the model assumption that the mean equals the variance, which set of output (the overall or stratified by admin and sex) corresponds to the effect modification model specified in the introduction? Is the assumption that mean equals the variance an assumption on the (marginal) distribution of salary or the conditional distribution of salary given covariates?\n\n\n2.2.2 Note that the stratum specific variance estimates are smaller than the overall variance because admin and male are significant predictors of salary. It is likely that we have not included important covariates from the model (e.g. years of experience, rank, field). Speculate on how inclusion of additional predictors could impact the conditional variance, and the mean-variance relationship assumption of the Poisson model.\n\n\n2.2.3 Why should we use robust standard error estimates? Should we base our decision to use robust standard error estimate on exploratory analyses like what is given above?\n\n\n2.2.4 Use the estimates of the stratum-specific means to find the estimate \\(\\beta_0\\), \\(\\beta_M\\), \\(\\beta_A\\), and \\(\\beta_{MA}\\) that will be obtained from the Poisson regression model."
  },
  {
    "objectID": "labs/lab7-effectmodification-questions.html#part-3-effect-modification-model-parameter-interpretation",
    "href": "labs/lab7-effectmodification-questions.html#part-3-effect-modification-model-parameter-interpretation",
    "title": "Lab 7: Poisson Regression and Effect Modification",
    "section": "Part 3: Effect modification model parameter interpretation",
    "text": "Part 3: Effect modification model parameter interpretation\n\n3.1 Fit the given model and use it to answer the following questions\nFor each of the following scientific questions, specify the null and alternative hypothesis need to answer the question based on this model. Then, conduct the appropriate test and interpret the results.\n\n\n3.2 Gender effects\n\n3.2.1 Is there a difference in mean salary, males compared to females, among faculty without administrative duties?\n\n\n3.2.2 Is there a difference in mean salary, males compared to females, among faculty with administrative duties?\n\n\n3.2.3 Is there any difference in mean salary, males compared to females (that is, in faculty with or without administrative duties)?\n\n\n\n3.3 Administrative effects\n\n3.3.1 Is there a difference in mean salary, comparing faculty with admin duties to those without admin duties, among female faculty?\n\n\n3.3.2 Is there a difference in mean salary, comparing faculty with admin duties to those without admin duties, among male faculty?\n\n\n3.3.3 Is there a any difference in mean salary, those with admin duties compared to those without admin duties (that is, in male or female faculty)?\n\n\n\n3.4 Effect Modification\n\n3.4.1 Is there evidence that administrative duties modifies the difference in salary, males compared to females?\n\n\n3.4.2 Is there evidence that gender modifies the difference in salary comparing faculty with administrative duties to faculty without administrative duties?"
  },
  {
    "objectID": "labs/lab7-effectmodification-questions.html#part-4-comparison-to-other-poisson-models",
    "href": "labs/lab7-effectmodification-questions.html#part-4-comparison-to-other-poisson-models",
    "title": "Lab 7: Poisson Regression and Effect Modification",
    "section": "Part 4: Comparison to other Poisson models",
    "text": "Part 4: Comparison to other Poisson models\n\n4.1 Impact of not including the effect modifier, \\(\\beta_{MA}\\) in the model. Suppose we had fit the following model instead. What can we say about our estimates of \\(\\beta_M\\) and \\(\\beta_A\\) from this model? Fit the model to see if you are right.\n\\(\\textrm{log}(\\lambda_i) = \\beta_0 + \\beta_M * \\textrm{Male} + \\beta_A * \\textrm{Admin}\\)\n\n\n4.2 Impact of not including the effect modifier, \\(\\beta_{MA}\\) or the main effect \\(\\beta_A\\) in the model. Suppose we had fit the following model instead.\n\\(\\textrm{log}(\\lambda_i) = \\beta_0 + \\beta_M * \\textrm{Male}\\)\n\n4.2.1 If Admin duties is a confounder, what can we say about our estimate of \\(\\beta_M\\) from this model?\n\n\n4.22 If Admin duties is not a confounder, what can we say about our estimate of \\(\\beta_M\\) from this model?"
  },
  {
    "objectID": "labs/lab1-linearregression.html",
    "href": "labs/lab1-linearregression.html",
    "title": "Lab 1: Linear regression",
    "section": "",
    "text": "Labs will begin with loading necessary packages (for R users). If this is the first time using these packages, they will need to be installed prior to being loaded.\nLab instructions follow in numbered steps. In general, you will need to add code (Insert… Code Cell… R), run it to obtain output, and provide an interpretation of the results. It is not enough to provide output without interpretation.\nWe will cover Parts 1 and 2 in one day. Part 3 will be revisited after we have covered more of the simple linear regression notes."
  },
  {
    "objectID": "labs/lab1-linearregression.html#general-instructions",
    "href": "labs/lab1-linearregression.html#general-instructions",
    "title": "Lab 1: Linear regression",
    "section": "",
    "text": "Labs will begin with loading necessary packages (for R users). If this is the first time using these packages, they will need to be installed prior to being loaded.\nLab instructions follow in numbered steps. In general, you will need to add code (Insert… Code Cell… R), run it to obtain output, and provide an interpretation of the results. It is not enough to provide output without interpretation.\nWe will cover Parts 1 and 2 in one day. Part 3 will be revisited after we have covered more of the simple linear regression notes."
  },
  {
    "objectID": "labs/lab1-linearregression.html#learning-objectives",
    "href": "labs/lab1-linearregression.html#learning-objectives",
    "title": "Lab 1: Linear regression",
    "section": "Learning objectives",
    "text": "Learning objectives\n\nBe able to fit and interpret frequentist and Bayesian simple linear regression models\nCompare the results to those that would be obtained using a 2-sample t-test\nCompare the results of robust versus classical standard error estimates for the frequentist approach"
  },
  {
    "objectID": "labs/lab1-linearregression.html#load-r-packages",
    "href": "labs/lab1-linearregression.html#load-r-packages",
    "title": "Lab 1: Linear regression",
    "section": "Load R packages",
    "text": "Load R packages\nLoad the following packages for use in this lab.\n\nlibrary(rms)\n\nLoading required package: Hmisc\n\n\nLoading required package: lattice\n\n\nLoading required package: survival\n\n\nLoading required package: Formula\n\n\nLoading required package: ggplot2\n\n\n\nAttaching package: 'Hmisc'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    knitrSet\n\n\nThe following objects are masked from 'package:base':\n\n    format.pval, units\n\n\nLoading required package: SparseM\n\n\n\nAttaching package: 'SparseM'\n\n\nThe following object is masked from 'package:base':\n\n    backsolve\n\nlibrary(rstanarm)\n\nLoading required package: Rcpp\n\n\nThis is rstanarm version 2.26.1\n\n\n- See https://mc-stan.org/rstanarm/articles/priors for changes to default priors!\n\n\n- Default priors may change, so it's safest to specify priors, even if equivalent to the defaults.\n\n\n- For execution on a local, multicore CPU with excess RAM we recommend calling\n\n\n  options(mc.cores = parallel::detectCores())\n\n\n\n1. Read in dataset\nWe will be using the inflammation dataset. Full documentation is available\nThe data is available as tab delimited or Stata format\n\n\n2. Perform basic descriptive statistics for variables of interest\nWe will only be considering the three variables: age, male, and bmi\n\nMale is coded as 0 for female and 1 for male. Create a factor variable (R) or label (Stata) this variable accordingly"
  },
  {
    "objectID": "labs/lab1-linearregression.html#part-1-bmi-and-gender-classical-standard-error-estimate",
    "href": "labs/lab1-linearregression.html#part-1-bmi-and-gender-classical-standard-error-estimate",
    "title": "Lab 1: Linear regression",
    "section": "Part 1: BMI and gender (classical standard error estimate)",
    "text": "Part 1: BMI and gender (classical standard error estimate)\n\n1. Create and describe a plot of BMI by gender\n\n\n2. Fit a simple linear regression model of BMI (outcome) on gender (predictor) using a frequentist approach.\n\n\n3. Conduct an (equal-variance) t-test of BMI by gender. Compare to the output of the linear regression model\n\n\n4. Fit a Bayesian linear regression model of BMI (outcome) on gender (predictor). Use the default priors. Compare the results."
  },
  {
    "objectID": "labs/lab1-linearregression.html#part-2-bmi-and-age",
    "href": "labs/lab1-linearregression.html#part-2-bmi-and-age",
    "title": "Lab 1: Linear regression",
    "section": "Part 2: BMI and age",
    "text": "Part 2: BMI and age\n\n1. Create and describe a plot of BMI by age\n\n\n2. Fit a simple linear regression model of BMI (outcome) on age (predictor) using a frequentist approach\n\n\n3. Interpret the slope and corresponding 95% confidence interval\n\n\n4. What is the estimate association for a 5-year increase in age? A 10-year increase in age? Give the confidence interval for each.\n\n\n5. Fit a similar Bayesian linear regression model and interpret the results."
  },
  {
    "objectID": "labs/lab1-linearregression.html#part-3-bmi-and-gender-robust-standard-error",
    "href": "labs/lab1-linearregression.html#part-3-bmi-and-gender-robust-standard-error",
    "title": "Lab 1: Linear regression",
    "section": "Part 3: BMI and gender, robust standard error",
    "text": "Part 3: BMI and gender, robust standard error\nWe will complete this part after covering robust standard errors in the notes\n\n1. Fit a regression using the robust standard error estimate. Compare the results from this regression model to a t-test assuming unequal variance between group."
  },
  {
    "objectID": "LICENSE.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "href": "LICENSE.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "title": "Attribution-ShareAlike 4.0 International",
    "section": "Creative Commons Attribution-ShareAlike 4.0 International Public License",
    "text": "Creative Commons Attribution-ShareAlike 4.0 International Public License\nBy exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License (“Public License”). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\n\nSection 1 – Definitions.\n\nAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\nAdapter’s License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.\nBY-SA Compatible License means a license listed at creativecommons.org/compatiblelicenses, approved by Creative Commons as essentially the equivalent of this Public License.\nCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\nEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\nExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\nLicense Elements means the license attributes listed in the name of a Creative Commons Public License. The License Elements of this Public License are Attribution and ShareAlike.\nLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\nLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\nLicensor means the individual(s) or entity(ies) granting rights under this Public License.\nShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\nSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\nYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\n\n\nSection 2 – Scope.\n\nLicense grant.\n\nSubject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\nA. reproduce and Share the Licensed Material, in whole or in part; and\nB. produce, reproduce, and Share Adapted Material.\nExceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\nTerm. The term of this Public License is specified in Section 6(a).\nMedia and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material.\nDownstream recipients.\nA. Offer from the Licensor – Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\nB. Additional offer from the Licensor – Adapted Material. Every recipient of Adapted Material from You automatically receives an offer from the Licensor to exercise the Licensed Rights in the Adapted Material under the conditions of the Adapter’s License You apply.\nC. No downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\nNo endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nOther rights.\n\nMoral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\nPatent and trademark rights are not licensed under this Public License.\nTo the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties.\n\n\n\n\nSection 3 – License Conditions.\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\nAttribution.\n\nIf You Share the Licensed Material (including in modified form), You must:\nA. retain the following if it is supplied by the Licensor with the Licensed Material:\n\nidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\na copyright notice;\na notice that refers to this Public License;\na notice that refers to the disclaimer of warranties;\na URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\nB. indicate if You modified the Licensed Material and retain an indication of any previous modifications; and\nC. indicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\nYou may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\nIf requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\n\nShareAlike.\n\nIn addition to the conditions in Section 3(a), if You Share Adapted Material You produce, the following conditions also apply.\n\nThe Adapter’s License You apply must be a Creative Commons license with the same License Elements, this version or later, or a BY-SA Compatible License.\nYou must include the text of, or the URI or hyperlink to, the Adapter’s License You apply. You may satisfy this condition in any reasonable manner based on the medium, means, and context in which You Share Adapted Material.\nYou may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, Adapted Material that restrict exercise of the rights granted under the Adapter’s License You apply.\n\n\n\nSection 4 – Sui Generis Database Rights.\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\nfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database;\nif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material, including for purposes of Section 3(b); and\nYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\n\n\nSection 5 – Disclaimer of Warranties and Limitation of Liability.\n\nUnless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You.\nTo the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You.\nThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\n\n\nSection 6 – Term and Termination.\n\nThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\nWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\nautomatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\nupon express reinstatement by the Licensor.\n\nFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\nFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\nSections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\n\n\nSection 7 – Other Terms and Conditions.\n\nThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\nAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.\n\n\n\nSection 8 – Interpretation.\n\nFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\nTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\nNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\nNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n\n\nCreative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the “Licensor.” The text of the Creative Commons public licenses is dedicated to the public domain under the CC0 Public Domain Dedication. Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark “Creative Commons” or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\nCreative Commons may be contacted at creativecommons.org."
  },
  {
    "objectID": "officehours.html",
    "href": "officehours.html",
    "title": "Office Hours",
    "section": "",
    "text": "We are available to discuss any questions that you might have at the following times\nChris Slaughter\n\n2525 West End, 11-107\n\nMonday 12 - 12:45\nMonday 2 - 3\n\nBy appointment\n\nIn person or virtual\nEmail to arrange\n\n\nLucy Yan\n\nFridays 12:30 - 1:30\n2525 West End, 11-105"
  },
  {
    "objectID": "supplemental/slr-derivations.html",
    "href": "supplemental/slr-derivations.html",
    "title": "Deriving the Least-Squares Estimates for Simple Linear Regression",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr. Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\n\n\nThis document contains the mathematical details for deriving the least-squares estimates for slope (\\(\\beta_1\\)) and intercept (\\(\\beta_0\\)). We obtain the estimates, \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\) by finding the values that minimize the sum of squared residuals, as shown in Equation 1.\n\\[\nSSR = \\sum\\limits_{i=1}^{n}[y_i - \\hat{y}_i]^2 = [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)]^2 = [y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i]^2\n\\tag{1}\\]\nRecall that we can find the values of \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\) that minimize /eq-ssr by taking the partial derivatives of Equation 1 and setting them to 0. Thus, the values of \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\) that minimize the respective partial derivative also minimize the sum of squared residuals. The partial derivatives are shown in Equation 2.\n\\[\n\\begin{aligned}\n\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_1} &= -2 \\sum\\limits_{i=1}^{n}x_i(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)  \\\\\n\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_0} &= -2 \\sum\\limits_{i=1}^{n}(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)\n\\end{aligned}\n\\tag{2}\\]\nThe derivation of deriving \\(\\hat{\\beta}_0\\) is shown in Equation 3.\n\\[\n\\begin{aligned}\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_0} &= -2 \\sum\\limits_{i=1}^{n}(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0 \\\\&\\Rightarrow -\\sum\\limits_{i=1}^{n}(y_i + \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i) = 0 \\\\&\\Rightarrow - \\sum\\limits_{i=1}^{n}y_i + n\\hat{\\beta}_0 + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i = 0 \\\\&\\Rightarrow n\\hat{\\beta}_0  = \\sum\\limits_{i=1}^{n}y_i - \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i \\\\&\\Rightarrow \\hat{\\beta}_0  = \\frac{1}{n}\\Big(\\sum\\limits_{i=1}^{n}y_i - \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i\\Big)\\\\&\\Rightarrow \\hat{\\beta}_0  = \\bar{y} - \\hat{\\beta}_1 \\bar{x} \\\\\\end{aligned}\n\\tag{3}\\]\nThe derivation of \\(\\hat{\\beta}_1\\) using the \\(\\hat{\\beta}_0\\) we just derived is shown in Equation 4.\n\\[\n\\begin{aligned}&\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_1} = -2 \\sum\\limits_{i=1}^{n}x_i(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0  \\\\&\\Rightarrow -\\sum\\limits_{i=1}^{n}x_iy_i + \\hat{\\beta}_0\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = 0 \\\\\\text{(Fill in }\\hat{\\beta}_0\\text{)}&\\Rightarrow -\\sum\\limits_{i=1}^{n}x_iy_i + (\\bar{y} - \\hat{\\beta}_1\\bar{x})\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = 0 \\\\&\\Rightarrow  (\\bar{y} - \\hat{\\beta}_1\\bar{x})\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = \\sum\\limits_{i=1}^{n}x_iy_i \\\\&\\Rightarrow \\bar{y}\\sum\\limits_{i=1}^{n}x_i - \\hat{\\beta}_1\\bar{x}\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = \\sum\\limits_{i=1}^{n}x_iy_i \\\\&\\Rightarrow n\\bar{y}\\bar{x} - \\hat{\\beta}_1n\\bar{x}^2 + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = \\sum\\limits_{i=1}^{n}x_iy_i \\\\&\\Rightarrow \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 - \\hat{\\beta}_1n\\bar{x}^2  = \\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x} \\\\&\\Rightarrow \\hat{\\beta}_1\\Big(\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2\\Big)  = \\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x} \\\\ &\\hat{\\beta}_1 = \\frac{\\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x}}{\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2}\\end{aligned}\n\\tag{4}\\]\nTo write \\(\\hat{\\beta}_1\\) in a form that’s more recognizable, we will use the following:\n\\[\n\\sum x_iy_i - n\\bar{y}\\bar{x} = \\sum(x - \\bar{x})(y - \\bar{y}) = (n-1)\\text{Cov}(x,y)\n\\tag{5}\\]\n\\[\n\\sum x_i^2 - n\\bar{x}^2 - \\sum(x - \\bar{x})^2 = (n-1)s_x^2\n\\tag{6}\\]\nwhere \\(\\text{Cov}(x,y)\\) is the covariance of \\(x\\) and \\(y\\), and \\(s_x^2\\) is the sample variance of \\(x\\) (\\(s_x\\) is the sample standard deviation).\nThus, applying Equation 5 and Equation 6, we have\n\\[\n\\begin{aligned}\\hat{\\beta}_1 &= \\frac{\\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x}}{\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2} \\\\&= \\frac{\\sum\\limits_{i=1}^{n}(x-\\bar{x})(y-\\bar{y})}{\\sum\\limits_{i=1}^{n}(x-\\bar{x})^2}\\\\&= \\frac{(n-1)\\text{Cov}(x,y)}{(n-1)s_x^2}\\\\&= \\frac{\\text{Cov}(x,y)}{s_x^2}\\end{aligned}\n\\tag{7}\\]\nThe correlation between \\(x\\) and \\(y\\) is \\(r = \\frac{\\text{Cov}(x,y)}{s_x s_y}\\). Thus, \\(\\text{Cov}(x,y) = r s_xs_y\\). Plugging this into Equation 7, we have\n\\[\n\\hat{\\beta}_1 = \\frac{\\text{Cov}(x,y)}{s_x^2} = r\\frac{s_ys_x}{s_x^2} = r\\frac{s_y}{s_x}\n\\tag{8}\\]"
  },
  {
    "objectID": "supplemental/log-transformations.html",
    "href": "supplemental/log-transformations.html",
    "title": "Log Transformations in Linear Regression",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr. Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document provides details about the model interpretation when the predictor and/or response variables are log-transformed. For simplicity, we will discuss transformations for the simple linear regression model as shown in Equation 1.\n\\[\n\\label{orig}\ny = \\beta_0 + \\beta_1 x\n\\tag{1}\\]\nAll results and interpretations can be easily extended to transformations in multiple regression models.\nNote: log refers to the natural logarithm."
  },
  {
    "objectID": "supplemental/log-transformations.html#log-transformation-on-the-response-variable",
    "href": "supplemental/log-transformations.html#log-transformation-on-the-response-variable",
    "title": "Log Transformations in Linear Regression",
    "section": "Log-transformation on the response variable",
    "text": "Log-transformation on the response variable\nSuppose we fit a linear regression model with \\(\\log(y)\\), the log-transformed \\(y\\), as the response variable. Under this model, we assume a linear relationship exists between \\(x\\) and \\(\\log(y)\\), such that \\(\\log(y) \\sim N(\\beta_0 + \\beta_1 x, \\sigma^2)\\) for some \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\). In other words, we can model the relationship between \\(x\\) and \\(\\log(y)\\) using the model in Equation 2.\n\\[\n\\log(y) = \\beta_0 + \\beta_1 x\n\\tag{2}\\]\nIf we interpret the model in terms of \\(\\log(y)\\), then we can use the usual interpretations for slope and intercept. When reporting results, however, it is best to give all interpretations in terms of the original response variable \\(y\\), since interpretations using log-transformed variables are often more difficult to truly understand.\nIn order to get back on the original scale, we need to use the exponential function (also known as the anti-log), \\(\\exp\\{x\\} = e^x\\). Therefore, we use the model in Equation 2 for interpretations and predictions, we will use Equation 3 to state our conclusions in terms of \\(y\\).\n\\[\n\\begin{aligned}\n&\\exp\\{\\log(y)\\} = \\exp\\{\\beta_0 + \\beta_1 x\\} \\\\[10pt]\n\\Rightarrow &y = \\exp\\{\\beta_0 + \\beta_1 x\\} \\\\[10pt]\n\\Rightarrow &y = \\exp\\{\\beta_0\\}\\exp\\{\\beta_1 x\\}\n\\end{aligned}\n\\tag{3}\\]\nIn order to interpret the slope and intercept, we need to first understand the relationship between the mean, median and log transformations.\n\nMean, Median, and Log Transformations\nSuppose we have a dataset y that contains the following observations:\n\n\n[1] 3 5 6 7 8\n\n\nIf we log-transform the values of y then calculate the mean and median, we have\n\n\n\n\n\nmean_log_y\nmedian_log_y\n\n\n\n\n1.70503\n1.79176\n\n\n\n\n\nIf we calculate the mean and median of y, then log-transform the mean and median, we have\n\n\n\n\n\nlog_mean\nlog_median\n\n\n\n\n1.75786\n1.79176\n\n\n\n\n\nThis is a simple illustration to show\n\n\\(\\text{Mean}[{\\log(y)}] \\neq \\log[\\text{Mean}(y)]\\) - the mean and log are not commutable\n\\(\\text{Median}[\\log(y)] = \\log[\\text{Median}(y)]\\) - the median and log are commutable\n\n\n\nInterpretaton of model coefficients\nUsing Equation 2, the mean \\(\\log(y)\\) for any given value of \\(x\\) is \\(\\beta_0 + \\beta_1 x\\); however, this does not indicate that the mean of \\(y = \\exp\\{\\beta_0 + \\beta_1 x\\}\\) (see previous section). From the assumptions of linear regression, we assume that for any given value of \\(x\\), the distribution of \\(\\log(y)\\) is Normal, and therefore symmetric. Thus the median of \\(\\log(y)\\) is equal to the mean of \\(\\log(y)\\), i.e \\(\\text{Median}(\\log(y)) = \\beta_0 + \\beta_1 x\\).\nSince the log and the median are commutable, \\(\\text{Median}(\\log(y)) = \\beta_0 + \\beta_1 x \\Rightarrow \\text{Median}(y) = \\exp\\{\\beta_0 + \\beta_1 x\\}\\). Thus, when we log-transform the response variable, the interpretation of the intercept and slope are in terms of the effect on the median of \\(y\\).\nIntercept: The intercept is expected median of \\(y\\) when the predictor variable equals 0. Therefore, when \\(x=0\\),\n\\[\n\\begin{aligned}\n&\\log(y) = \\beta_0 + \\beta_1 \\times 0 = \\beta_0 \\\\[10pt]\n\\Rightarrow &y = \\exp\\{\\beta_0\\}\n\\end{aligned}\n\\]\nInterpretation: When \\(x=0\\), the median of \\(y\\) is expected to be \\(\\exp\\{\\beta_0\\}\\).\nSlope: The slope is the expected change in the median of \\(y\\) when \\(x\\) increases by 1 unit. The change in the median of \\(y\\) is\n\\[\n\\exp\\{[\\beta_0 + \\beta_1 (x+1)] - [\\beta_0 + \\beta_1 x]\\} = \\frac{\\exp\\{\\beta_0 + \\beta_1 (x+1)\\}}{\\exp\\{\\beta_0 + \\beta_1 x\\}} = \\frac{\\exp\\{\\beta_0\\}\\exp\\{\\beta_1 x\\}\\exp\\{\\beta_1\\}}{\\exp\\{\\beta_0\\}\\exp\\{\\beta_1 x\\}} = \\exp\\{\\beta_1\\}\n\\]\nThus, the median of \\(y\\) for \\(x+1\\) is \\(\\exp\\{\\beta_1\\}\\) times the median of \\(y\\) for \\(x\\).\nInterpretation: When \\(x\\) increases by one unit, the median of \\(y\\) is expected to multiply by a factor of \\(\\exp\\{\\beta_1\\}\\)."
  },
  {
    "objectID": "supplemental/log-transformations.html#log-transformation-on-the-predictor-variable",
    "href": "supplemental/log-transformations.html#log-transformation-on-the-predictor-variable",
    "title": "Log Transformations in Linear Regression",
    "section": "Log-transformation on the predictor variable",
    "text": "Log-transformation on the predictor variable\nSuppose we fit a linear regression model with \\(\\log(x)\\), the log-transformed \\(x\\), as the predictor variable. Under this model, we assume a linear relationship exists between \\(\\log(x)\\) and \\(y\\), such that \\(y \\sim N(\\beta_0 + \\beta_1 \\log(x), \\sigma^2)\\) for some \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\). In other words, we can model the relationship between \\(\\log(x)\\) and \\(y\\) using the model in #eq-log-x.\n\\[\ny = \\beta_0 + \\beta_1 \\log(x)\n\\tag{4}\\]\nIntercept: The intercept is the mean of \\(y\\) when \\(\\log(x) = 0\\), i.e. \\(x = 1\\).\nInterpretation: When \\(x = 1\\) \\((\\log(x) = 0)\\), the mean of \\(y\\) is expected to be \\(\\beta_0\\).\nSlope: The slope is interpreted in terms of the change in the mean of \\(y\\) when \\(x\\) is multiplied by a factor of \\(C\\), since \\(\\log(Cx) = \\log(x) + \\log(C)\\). Thus, when \\(x\\) is multiplied by a factor of \\(C\\), the change in the mean of \\(y\\) is\n\\[\n\\begin{aligned}\n(\\beta_0 + \\beta_1 \\log(Cx)) - (\\beta_0 + \\beta_1 \\log(x)) &= \\beta_1 [\\log(Cx) - \\log(x)] \\\\[10pt]\n& = \\beta_1[\\log(C) + \\log(x) - \\log(x)] \\\\[10pt]\n& = \\beta_1 \\log(C)\n\\end{aligned}\n\\]\nThus the mean of \\(y\\) changes by \\(\\beta_1 \\log(C)\\) units.\nInterpretation: When \\(x\\) is multiplied by a factor of \\(C\\), the mean of \\(y\\) is expected to change by \\(\\beta_1 \\log(C)\\) units. For example, if \\(x\\) is doubled, then the mean of \\(y\\) is expected to change by \\(\\beta_1 \\log(2)\\) units."
  },
  {
    "objectID": "supplemental/log-transformations.html#log-transformation-on-the-the-response-and-predictor-variable",
    "href": "supplemental/log-transformations.html#log-transformation-on-the-the-response-and-predictor-variable",
    "title": "Log Transformations in Linear Regression",
    "section": "Log-transformation on the the response and predictor variable",
    "text": "Log-transformation on the the response and predictor variable\nSuppose we fit a linear regression model with \\(\\log(x)\\), the log-transformed \\(x\\), as the predictor variable and \\(\\log(y)\\), the log-transformed \\(y\\), as the response variable. Under this model, we assume a linear relationship exists between \\(\\log(x)\\) and \\(\\log(y)\\), such that \\(\\log(y) \\sim N(\\beta_0 + \\beta_1 \\log(x), \\sigma^2)\\) for some \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\). In other words, we can model the relationship between \\(\\log(x)\\) and \\(\\log(y)\\) using the model in Equation 5.\n\\[\n\\log(y) = \\beta_0 + \\beta_1 \\log(x)\n\\tag{5}\\]\nBecause the response variable is log-transformed, the interpretations on the original scale will be in terms of the median of \\(y\\) (see the section on the log-transformed response variable for more detail).\nIntercept: The intercept is the mean of \\(y\\) when \\(\\log(x) = 0\\), i.e. \\(x = 1\\). Therefore, when \\(\\log(x) = 0\\),\n\\[\n\\begin{aligned}\n&\\log(y) = \\beta_0 + \\beta_1 \\times 0 = \\beta_0 \\\\[10pt]\n\\Rightarrow &y = \\exp\\{\\beta_0\\}\n\\end{aligned}\n\\]\nInterpretation: When \\(x = 1\\) \\((\\log(x) = 0)\\), the median of \\(y\\) is expected to be \\(\\exp\\{\\beta_0\\}\\).\nSlope: The slope is interpreted in terms of the change in the median \\(y\\) when \\(x\\) is multiplied by a factor of \\(C\\), since \\(\\log(Cx) = \\log(x) + \\log(C)\\). Thus, when \\(x\\) is multiplied by a factor of \\(C\\), the change in the median of \\(y\\) is\n\\[\n\\begin{aligned}\n\\exp\\{[\\beta_0 + \\beta_1 \\log(Cx)] - [\\beta_0 + \\beta_1 \\log(x)]\\} &=\n\\exp\\{\\beta_1 [\\log(Cx) - \\log(x)]\\} \\\\[10pt]\n& = \\exp\\{\\beta_1[\\log(C) + \\log(x) - \\log(x)]\\} \\\\[10pt]\n& = \\exp\\{\\beta_1 \\log(C)\\} = C^{\\beta_1}\n\\end{aligned}\n\\]\nThus, the median of \\(y\\) for \\(Cx\\) is \\(C^{\\beta_1}\\) times the median of \\(y\\) for \\(x\\).\nInterpretation: When \\(x\\) is multiplied by a factor of \\(C\\), the median of \\(y\\) is expected to multiple by a factor of \\(C^{\\beta_1}\\). For example, if \\(x\\) is doubled, then the median of \\(y\\) is expected to multiply by \\(2^{\\beta_1}\\)."
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html",
    "href": "supplemental/model-diagnostics-matrix.html",
    "title": "Model Diagnostics",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr. Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document discusses some of the mathematical details of the model diagnostics - leverage, standardized residuals, and Cook’s distance. We assume the reader knowledge of the matrix form for multiple linear regression. Please see Matrix Form of Linear Regression for a review."
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#introduction",
    "href": "supplemental/model-diagnostics-matrix.html#introduction",
    "title": "Model Diagnostics",
    "section": "Introduction",
    "text": "Introduction\nSuppose we have \\(n\\) observations. Let the \\(i^{th}\\) be \\((x_{i1}, \\ldots, x_{ip}, y_i)\\), such that \\(x_{i1}, \\ldots, x_{ip}\\) are the explanatory variables (predictors) and \\(y_i\\) is the response variable. We assume the data can be modeled using the least-squares regression model, such that the mean response for a given combination of explanatory variables follows the form in Equation 1.\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_p x_p\n\\tag{1}\\]\nWe can write the response for the \\(i^{th}\\) observation as shown in Equation 2.\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip} + \\epsilon_i\n\\tag{2}\\]\nsuch that \\(\\epsilon_i\\) is the amount \\(y_i\\) deviates from \\(\\mu\\{y|x_{i1}, \\ldots, x_{ip}\\}\\), the mean response for a given combination of explanatory variables. We assume each \\(\\epsilon_i \\sim N(0,\\sigma^2)\\), where \\(\\sigma^2\\) is a constant variance for the distribution of the response \\(y\\) for any combination of explanatory variables \\(x_1, \\ldots, x_p\\)."
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#matrix-form-for-the-regression-model",
    "href": "supplemental/model-diagnostics-matrix.html#matrix-form-for-the-regression-model",
    "title": "Model Diagnostics",
    "section": "Matrix Form for the Regression Model",
    "text": "Matrix Form for the Regression Model\nWe can represent the Equation 1 and Equation 2 using matrix notation. Let\n\\[\n\\mathbf{Y} = \\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\y_n\\end{bmatrix}\n\\hspace{15mm}\n\\mathbf{X} = \\begin{bmatrix}x_{11} & x_{12} & \\dots & x_{1p} \\\\\nx_{21} & x_{22} & \\dots & x_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\nx_{n1} & x_{n2} & \\dots & x_{np} \\end{bmatrix}\n\\hspace{15mm}\n\\boldsymbol{\\beta}= \\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{bmatrix}\n\\hspace{15mm}\n\\boldsymbol{\\epsilon}= \\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix}\n\\tag{3}\\]\nThus,\n\\[\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{\\epsilon}\\]\nTherefore the estimated response for a given combination of explanatory variables and the associated residuals can be written as\n\\[\n\\hat{\\mathbf{Y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\hspace{10mm} \\mathbf{e} = \\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\n\\tag{4}\\]"
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#hat-matrix-leverage",
    "href": "supplemental/model-diagnostics-matrix.html#hat-matrix-leverage",
    "title": "Model Diagnostics",
    "section": "Hat Matrix & Leverage",
    "text": "Hat Matrix & Leverage\nRecall from the notes Matrix Form of Linear Regression that \\(\\hat{\\boldsymbol{\\beta}}\\) can be written as the following:\n\\[\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\n\\tag{5}\\]\nCombining Equation 4 and Equation 5, we can write \\(\\hat{\\mathbf{Y}}\\) as the following:\n\\[\n\\begin{aligned}\n\\hat{\\mathbf{Y}} &= \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\[10pt]\n&= \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\\\\n\\end{aligned}\n\\tag{6}\\]\nWe define the hat matrix as an \\(n \\times n\\) matrix of the form \\(\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\). Thus Equation 6 becomes\n\\[\n\\hat{\\mathbf{Y}} = \\mathbf{H}\\mathbf{Y}\n\\tag{7}\\]\nThe diagonal elements of the hat matrix are a measure of how far the predictor variables of each observation are from the means of the predictor variables. For example, \\(h_{ii}\\) is a measure of how far the values of the predictor variables for the \\(i^{th}\\) observation, \\(x_{i1}, x_{i2}, \\ldots, x_{ip}\\), are from the mean values of the predictor variables, \\(\\bar{x}_1, \\bar{x}_2, \\ldots, \\bar{x}_p\\). In the case of simple linear regression, the \\(i^{th}\\) diagonal, \\(h_{ii}\\), can be written as\n\\[\nh_{ii} =  \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_{j=1}^{n}(x_j-\\bar{x})^2}\n\\]\nWe call these diagonal elements, the leverage of each observation.\nThe diagonal elements of the hat matrix have the following properties:\n\n\\(0 \\leq h_ii \\leq 1\\)\n\\(\\sum\\limits_{i=1}^{n} h_{ii} = p+1\\), where \\(p\\) is the number of predictor variables in the model.\nThe mean hat value is \\(\\bar{h} = \\frac{\\sum\\limits_{i=1}^{n} h_{ii}}{n} = \\frac{p+1}{n}\\).\n\nUsing these properties, we consider a point to have high leverage if it has a leverage value that is more than 2 times the average. In other words, observations with leverage greater than \\(\\frac{2(p+1)}{n}\\) are considered to be high leverage points, i.e. outliers in the predictor variables. We are interested in flagging high leverage points, because they may have an influence on the regression coefficients.\nWhen there are high leverage points in the data, the regression line will tend towards those points; therefore, one property of high leverage points is that they tend to have small residuals. We will show this by rewriting the residuals from Equation 4 using Equation 7.\n\\[\n\\begin{aligned}\n\\mathbf{e} &= \\mathbf{Y} - \\hat{\\mathbf{Y}} \\\\[10pt]\n& = \\mathbf{Y} - \\mathbf{H}\\mathbf{Y} \\\\[10pt]\n&= (1-\\mathbf{H})\\mathbf{Y}\n\\end{aligned}\n\\tag{8}\\]\nNote that the identity matrix and hat matrix are idempotent, i.e. \\(\\mathbf{I}\\mathbf{I} = \\mathbf{I}\\), \\(\\mathbf{H}\\mathbf{H} = \\mathbf{H}\\). Thus, \\((\\mathbf{I} - \\mathbf{H})\\) is also idempotent. These matrices are also symmetric. Using these properties and Equation 8, we have that the variance-covariance matrix of the residuals \\(\\boldsymbol{e}\\), is\n\\[\n\\begin{aligned}\nVar(\\mathbf{e}) &= \\mathbf{e}\\mathbf{e}^T \\\\[10pt]\n&=  (1-\\mathbf{H})Var(\\mathbf{Y})^T(1-\\mathbf{H})^T \\\\[10pt]\n&= (1-\\mathbf{H})\\hat{\\sigma}^2(1-\\mathbf{H})^T  \\\\[10pt]\n&= \\hat{\\sigma}^2(1-\\mathbf{H})(1-\\mathbf{H})  \\\\[10pt]\n&= \\hat{\\sigma}^2(1-\\mathbf{H})\n\\end{aligned}\n\\tag{9}\\]\nwhere \\(\\hat{\\sigma}^2 = \\frac{\\sum_{i=1}^{n}e_i^2}{n-p-1}\\) is the estimated regression variance. Thus, the variance of the \\(i^{th}\\) residual is \\(Var(e_i) = \\hat{\\sigma}^2(1-h_{ii})\\). Therefore, the higher the leverage, the smaller the variance of the residual. Because the expected value of the residuals is 0, we conclude that points with high leverage tend to have smaller residuals than points with lower leverage."
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#standardized-residuals",
    "href": "supplemental/model-diagnostics-matrix.html#standardized-residuals",
    "title": "Model Diagnostics",
    "section": "Standardized Residuals",
    "text": "Standardized Residuals\nIn general, we standardize a value by shifting by the expected value and rescaling by the standard deviation (or standard error). Thus, the \\(i^{th}\\) standardized residual takes the form\n\\[\nstd.res_i = \\frac{e_i - E(e_i)}{SE(e_i)}\n\\]\nThe expected value of the residuals is 0, i.e. \\(E(e_i) = 0\\). From Equation 9), the standard error of the residual is \\(SE(e_i) = \\hat{\\sigma}\\sqrt{1-h_{ii}}\\). Therefore,\n\\[\nstd.res_i = \\frac{e_i}{\\hat{\\sigma}\\sqrt{1-h_{ii}}}\n\\tag{10}\\]"
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#cooks-distance",
    "href": "supplemental/model-diagnostics-matrix.html#cooks-distance",
    "title": "Model Diagnostics",
    "section": "Cook’s Distance",
    "text": "Cook’s Distance\nCook’s distance is a measure of how much each observation influences the model coefficients, and thus the predicted values. The Cook’s distance for the \\(i^{th}\\) observation can be written as\n\\[\nD_i = \\frac{(\\hat{\\mathbf{Y}} -\\hat{\\mathbf{Y}}_{(i)})^T(\\hat{\\mathbf{Y}} -\\hat{\\mathbf{Y}}_{(i)})}{(p+1)\\hat{\\sigma}}\n\\tag{11}\\]\nwhere \\(\\hat{\\mathbf{Y}}_{(i)}\\) is the vector of predicted values from the model fitted when the \\(i^{th}\\) observation is deleted. Cook’s Distance can be calculated without deleting observations one at a time, since Equation 12 below is mathematically equivalent to Equation 11.\n\\[\nD_i = \\frac{1}{p+1}std.res_i^2\\Bigg[\\frac{h_{ii}}{(1-h_{ii})}\\Bigg] = \\frac{e_i^2}{(p+1)\\hat{\\sigma}^2(1-h_{ii})}\\Bigg[\\frac{h_{ii}}{(1-h_{ii})}\\Bigg]\n\\tag{12}\\]"
  },
  {
    "objectID": "supplemental/model-selection-criteria.html",
    "href": "supplemental/model-selection-criteria.html",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr. Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document discusses some of the mathematical details of Akaike’s Information Criterion (AIC) and Schwarz’s Bayesian Information Criterion (BIC). We assume the reader knowledge of the matrix form for multiple linear regression.Please see Matrix Notation for Multiple Linear Regression for a review."
  },
  {
    "objectID": "supplemental/model-selection-criteria.html#maximum-likelihood-estimation-of-boldsymbolbeta-and-sigma",
    "href": "supplemental/model-selection-criteria.html#maximum-likelihood-estimation-of-boldsymbolbeta-and-sigma",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "Maximum Likelihood Estimation of \\(\\boldsymbol{\\beta}\\) and \\(\\sigma\\)",
    "text": "Maximum Likelihood Estimation of \\(\\boldsymbol{\\beta}\\) and \\(\\sigma\\)\nTo understand the formulas for AIC and BIC, we will first briefly explain the likelihood function and maximum likelihood estimates for regression.\nLet \\(\\mathbf{Y}\\) be \\(n \\times 1\\) matrix of responses, \\(\\mathbf{X}\\), the \\(n \\times (p+1)\\) matrix of predictors, and \\(\\boldsymbol{\\beta}\\), \\((p+1) \\times 1\\) matrix of coefficients. If the multiple linear regression model is correct then,\n\\[\n\\mathbf{Y} \\sim N(\\mathbf{X}\\boldsymbol{\\beta}, \\sigma^2)\n\\tag{1}\\]\nWhen we do linear regression, our goal is to estimate the unknown parameters \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) from Equation 1. In Matrix Notation for Multiple Linear Regression, we showed a way to estimate these parameters using matrix alegbra. Another approach for estimating \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) is using maximum likelihood estimation.\nA likelihood function is used to summarise the evidence from the data in support of each possible value of a model parameter. Using Equation 1, we will write the likelihood function for linear regression as\n\\[\nL(\\mathbf{X}, \\mathbf{Y}|\\boldsymbol{\\beta}, \\sigma^2) = \\prod\\limits_{i=1}^n (2\\pi \\sigma^2)^{-\\frac{1}{2}} \\exp\\bigg\\{-\\frac{1}{2\\sigma^2}\\sum\\limits_{i=1}^n(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta})^T(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta})\\bigg\\}\n\\tag{2}\\]\nwhere \\(Y_i\\) is the \\(i^{th}\\) response and \\(\\mathbf{X}_i\\) is the vector of predictors for the \\(i^{th}\\) observation. One approach estimating \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) is to find the values of those parameters that maximize the likelihood in Equation 2, i.e. maximum likelhood estimation. To make the calculations more manageable, instead of maximizing the likelihood function, we will instead maximize its logarithm, i.e. the log-likelihood function. The values of the parameters that maximize the log-likelihood function are those that maximize the likelihood function. The log-likelihood function we will maximize is\n\\[\n\\begin{aligned}\n\\log L(\\mathbf{X}, \\mathbf{Y}|\\boldsymbol{\\beta}, \\sigma^2) &= \\sum\\limits_{i=1}^n -\\frac{1}{2}\\log(2\\pi\\sigma^2) -\\frac{1}{2\\sigma^2}\\sum\\limits_{i=1}^n(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta})^T(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta}) \\\\\n&= -\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})^T(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})\\\\\n\\end{aligned}\n\\tag{3}\\]\n\nThe maximum likelihood estimate of \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) are \\[\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} \\hspace{10mm} \\hat{\\sigma}^2 = \\frac{1}{n}(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})^T(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta}) = \\frac{1}{n}RSS\n\\tag{4}\\]\nwhere \\(RSS\\) is the residual sum of squares. Note that the maximum likelihood estimate is not exactly equal to the estimate of \\(\\sigma^2\\) we typically use \\(\\frac{RSS}{n-p-1}\\). This is because the maximum likelihood estimate of \\(\\sigma^2\\) in Equation 4 is a biased estimator of \\(\\sigma^2\\). When \\(n\\) is much larger than the number of predictors \\(p\\), then the differences in these two estimates are trivial."
  },
  {
    "objectID": "supplemental/model-selection-criteria.html#aic",
    "href": "supplemental/model-selection-criteria.html#aic",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "AIC",
    "text": "AIC\nAkaike’s Information Criterion (AIC) is\n\\[\nAIC = -2 \\log L + 2(p+1)\n\\tag{5}\\]\nwhere \\(\\log L\\) is the log-likelihood. This is the general form of AIC that can be applied to a variety of models, but for now, let’s focus on AIC for mutliple linear regression.\n\\[\n\\begin{aligned}\nAIC &= -2 \\log L + 2(p+1) \\\\\n&= -2\\bigg[-\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})^T(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})\\bigg] + 2(p+1) \\\\\n&= n\\log\\big(2\\pi\\frac{RSS}{n}\\big) + \\frac{1}{RSS/n}RSS \\\\\n&= n\\log(2\\pi) + n\\log(RSS) - n\\log(n) + 2(p+1)\n\\end{aligned}\n\\tag{6}\\]"
  },
  {
    "objectID": "supplemental/model-selection-criteria.html#bic",
    "href": "supplemental/model-selection-criteria.html#bic",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "BIC",
    "text": "BIC\n[To be added.]"
  },
  {
    "objectID": "supplemental/mlr-matrix.html",
    "href": "supplemental/mlr-matrix.html",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr. Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document provides the details for the matrix notation for multiple linear regression. We assume the reader has familiarity with some linear algebra. Please see Chapter 1 of An Introduction to Statistical Learning for a brief review of linear algebra."
  },
  {
    "objectID": "supplemental/mlr-matrix.html#introduction",
    "href": "supplemental/mlr-matrix.html#introduction",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "Introduction",
    "text": "Introduction\nSuppose we have \\(n\\) observations. Let the \\(i^{th}\\) be \\((x_{i1}, \\ldots, x_{ip}, y_i)\\), such that \\(x_{i1}, \\ldots, x_{ip}\\) are the explanatory variables (predictors) and \\(y_i\\) is the response variable. We assume the data can be modeled using the least-squares regression model, such that the mean response for a given combination of explanatory variables follows the form in Equation 1.\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_p x_p\n\\tag{1}\\]\nWe can write the response for the \\(i^{th}\\) observation as shown in Equation 2\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip} + \\epsilon_i\n\\tag{2}\\]\nsuch that \\(\\epsilon_i\\) is the amount \\(y_i\\) deviates from \\(\\mu\\{y|x_{i1}, \\ldots, x_{ip}\\}\\), the mean response for a given combination of explanatory variables. We assume each \\(\\epsilon_i \\sim N(0,\\sigma^2)\\), where \\(\\sigma^2\\) is a constant variance for the distribution of the response \\(y\\) for any combination of explanatory variables \\(x_1, \\ldots, x_p\\)."
  },
  {
    "objectID": "supplemental/mlr-matrix.html#matrix-representation-for-the-regression-model",
    "href": "supplemental/mlr-matrix.html#matrix-representation-for-the-regression-model",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "Matrix Representation for the Regression Model",
    "text": "Matrix Representation for the Regression Model\nWe can represent the Equation 1 and Equation 2 using matrix notation. Let\n\\[\n\\mathbf{Y} = \\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\y_n\\end{bmatrix}\n\\hspace{15mm}\n\\mathbf{X} = \\begin{bmatrix}x_{11} & x_{12} & \\dots & x_{1p} \\\\\nx_{21} & x_{22} & \\dots & x_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\nx_{n1} & x_{n2} & \\dots & x_{np} \\end{bmatrix}\n\\hspace{15mm}\n\\boldsymbol{\\beta}= \\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{bmatrix}\n\\hspace{15mm}\n\\boldsymbol{\\epsilon}= \\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix}\n\\tag{3}\\]\nThus,\n\\[\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{\\epsilon}\\]\nTherefore the estimated response for a given combination of explanatory variables and the associated residuals can be written as\n\\[\n\\hat{\\mathbf{Y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\hspace{10mm} \\mathbf{e} = \\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\n\\tag{4}\\]"
  },
  {
    "objectID": "supplemental/mlr-matrix.html#estimating-the-coefficients",
    "href": "supplemental/mlr-matrix.html#estimating-the-coefficients",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "Estimating the Coefficients",
    "text": "Estimating the Coefficients\nThe least-squares model is the one that minimizes the sum of the squared residuals. Therefore, we want to find the coefficients, \\(\\hat{\\boldsymbol{\\beta}}\\) that minimizes\n\\[\n\\sum\\limits_{i=1}^{n} e_{i}^2 = \\mathbf{e}^T\\mathbf{e} = (\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})^T(\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})\n\\tag{5}\\]\nwhere \\(\\mathbf{e}^T\\), the transpose of the matrix \\(\\mathbf{e}\\).\n\\[\n(\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})^T(\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{Y}^T\\mathbf{Y} -\n\\mathbf{Y}^T \\mathbf{X}\\hat{\\boldsymbol{\\beta}} - (\\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{Y} +\n\\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{X}\n\\hat{\\boldsymbol{\\beta}})\n\\tag{6}\\]\nNote that \\((\\mathbf{Y^T}\\mathbf{X}\\hat{\\boldsymbol{\\beta}})^T = \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{Y}\\). Since these are both constants (i.e. \\(1\\times 1\\) vectors), \\(\\mathbf{Y^T}\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{Y}\\). Thus, Equation 7 becomes\n\\[\n\\mathbf{Y}^T\\mathbf{Y} - 2 \\mathbf{X}^T\\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{Y} + \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{X}\n\\hat{\\boldsymbol{\\beta}}\n\\tag{7}\\]\nSince we want to find the \\(\\hat{\\boldsymbol{\\beta}}\\) that minimizes Equation 5, will find the value of \\(\\hat{\\boldsymbol{\\beta}}\\) such that the derivative with respect to \\(\\hat{\\boldsymbol{\\beta}}\\) is equal to 0.\n\\[\n\\begin{aligned}\n\\frac{\\partial \\mathbf{e}^T\\mathbf{e}}{\\partial \\hat{\\boldsymbol{\\beta}}} & = \\frac{\\partial}{\\partial \\hat{\\boldsymbol{\\beta}}}(\\mathbf{Y}^T\\mathbf{Y} - 2 \\mathbf{X}^T\\hat{\\boldsymbol{\\beta}}{}^T\\mathbf{Y} + \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}}) = 0 \\\\\n&\\Rightarrow - 2 \\mathbf{X}^T\\mathbf{Y} + 2 \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = 0 \\\\\n& \\Rightarrow 2 \\mathbf{X}^T\\mathbf{Y} = 2 \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\\n& \\Rightarrow \\mathbf{X}^T\\mathbf{Y} = \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\\n& \\Rightarrow (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\\n& \\Rightarrow (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} = \\mathbf{I}\\hat{\\boldsymbol{\\beta}}\n\\end{aligned}\n\\tag{8}\\]\nThus, the estimate of the model coefficients is \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\)."
  },
  {
    "objectID": "supplemental/mlr-matrix.html#variance-covariance-matrix-of-the-coefficients",
    "href": "supplemental/mlr-matrix.html#variance-covariance-matrix-of-the-coefficients",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "Variance-covariance matrix of the coefficients",
    "text": "Variance-covariance matrix of the coefficients\nWe will use two properties to derive the form of the variance-covariance matrix of the coefficients:\n\n\\(E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] = \\sigma^2I\\)\n\\(\\hat{\\boldsymbol{\\beta}} = \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\epsilon\\)\n\nFirst, we will show that \\(E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] = \\sigma^2I\\)\n\\[\n\\begin{aligned}\nE[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] &= E \\begin{bmatrix}\\epsilon_1  & \\epsilon_2 & \\dots & \\epsilon_n \\end{bmatrix}\\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix}  \\\\\n& = E \\begin{bmatrix} \\epsilon_1^2  & \\epsilon_1 \\epsilon_2 & \\dots & \\epsilon_1 \\epsilon_n \\\\\n\\epsilon_2 \\epsilon_1 & \\epsilon_2^2 & \\dots & \\epsilon_2 \\epsilon_n \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\epsilon_n \\epsilon_1 & \\epsilon_n \\epsilon_2 & \\dots & \\epsilon_n^2\n\\end{bmatrix} \\\\\n& = \\begin{bmatrix} E[\\epsilon_1^2]  & E[\\epsilon_1 \\epsilon_2] & \\dots & E[\\epsilon_1 \\epsilon_n] \\\\\nE[\\epsilon_2 \\epsilon_1] & E[\\epsilon_2^2] & \\dots & E[\\epsilon_2 \\epsilon_n] \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nE[\\epsilon_n \\epsilon_1] & E[\\epsilon_n \\epsilon_2] & \\dots & E[\\epsilon_n^2]\n\\end{bmatrix}\n\\end{aligned}\n\\tag{9}\\]\nRecall, the regression assumption that the errors \\(\\epsilon_i's\\) are Normally distributed with mean 0 and variance \\(\\sigma^2\\). Thus, \\(E(\\epsilon_i^2) = Var(\\epsilon_i) = \\sigma^2\\) for all \\(i\\). Additionally, recall the regression assumption that the errors are uncorrelated, i.e. \\(E(\\epsilon_i \\epsilon_j) = Cov(\\epsilon_i, \\epsilon_j) = 0\\) for all \\(i,j\\). Using these assumptions, we can write Equation 9 as\n\\[\nE[\\mathbf{\\epsilon}\\mathbf{\\epsilon}^T]  = \\begin{bmatrix} \\sigma^2  & 0 & \\dots & 0 \\\\\n0 & \\sigma^2  & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\dots & \\sigma^2\n\\end{bmatrix} = \\sigma^2 \\mathbf{I}\n\\tag{10}\\]\nwhere \\(\\mathbf{I}\\) is the \\(n \\times n\\) identity matrix.\nNext, we show that \\(\\hat{\\boldsymbol{\\beta}} = \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\epsilon\\).\nRecall that the \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\) and \\(\\mathbf{Y} = \\mathbf{X}\\mathbf{\\beta} + \\mathbf{\\epsilon}\\). Then,\n\\[\n\\begin{aligned}\n\\hat{\\boldsymbol{\\beta}} &= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} \\\\\n&= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T(\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}) \\\\\n&= \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T \\mathbf{\\epsilon} \\\\\n\\end{aligned}\n\\tag{11}\\]\nUsing these two properties, we derive the form of the variance-covariance matrix for the coefficients. Note that the covariance matrix is \\(E[(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})^T]\\)\n\\[\n\\begin{aligned}\nE[(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})^T] &= E[(\\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon} - \\boldsymbol{\\beta})(\\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon} - \\boldsymbol{\\beta})^T]\\\\\n& = E[(\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}] \\\\\n& = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T]\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n& = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T (\\sigma^2\\mathbf{I})\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n&= \\sigma^2\\mathbf{I}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n& = \\sigma^2\\mathbf{I}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n&  = \\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1} \\\\\n\\end{aligned}\n\\tag{12}\\]"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus BIOS 6312",
    "section": "",
    "text": "Learn how to use modern regression methods to answer scientific questions\nBecome familiar with statistical concepts including exploratory data analysis, estimation, testing in linear, logistic, and survival models\nUnderstand how the development of statistical methodology is motivated by biological and medical problems\nDevelop data analytic skills including familiarity with several statistical programs\nDevelop writing skills needed to communicate the results of a data analysis\nIntroduce reproducible research approaches using R, Rstudio, and Quarto"
  },
  {
    "objectID": "syllabus.html#course-goals",
    "href": "syllabus.html#course-goals",
    "title": "Syllabus BIOS 6312",
    "section": "",
    "text": "Learn how to use modern regression methods to answer scientific questions\nBecome familiar with statistical concepts including exploratory data analysis, estimation, testing in linear, logistic, and survival models\nUnderstand how the development of statistical methodology is motivated by biological and medical problems\nDevelop data analytic skills including familiarity with several statistical programs\nDevelop writing skills needed to communicate the results of a data analysis\nIntroduce reproducible research approaches using R, Rstudio, and Quarto"
  },
  {
    "objectID": "syllabus.html#topics-of-discussion",
    "href": "syllabus.html#topics-of-discussion",
    "title": "Syllabus BIOS 6312",
    "section": "Topics of Discussion",
    "text": "Topics of Discussion\n\nBayesian and Frequentist approaches to fitting regression models\nLinear regression\nLogistic regression\nPoisson regression\nSurvival models, primarily Cox Regression\nMultinomial and ordinal logistic regression\nMultivariable Regression\nMatrix algebra and important results of random vectors\nPrecision, effect modification, and confounding\nSpecification issues in regression models\nModel selection\nCase Studies\nUnderstanding model assumptions and the impact of assumptions on interpretation\nModel Checking: diagnostics, transformations, influential observations, lack-of-fit test"
  },
  {
    "objectID": "syllabus.html#course-description",
    "href": "syllabus.html#course-description",
    "title": "Syllabus BIOS 6312",
    "section": "Course Description",
    "text": "Course Description\n\nBIOS 6312\nThis is the second in a two-course series designed for students who seek to develop skills in modern biostatistical reasoning and data analysis. Students learn modern regression analysis and modeling building techniques from an applied perspective. Theoretical principles will be demonstrated with real-world examples from biomedical studies. This course requires substantial statistical computing in software packages and focuses on R; familiarity with R or proficiency in another high-level statistical program (e.g. Stata) is required. The course covers regression modeling for continuous outcomes, including simple linear regression, multiple linear regression, and analysis of variance with one-way, two-way, three-way, and analysis of covariance models. This is a brief introduction to models for binary outcomes (logistic models), ordinal outcomes (proportional odds models), count outcomes (Poisson/negative binomial models), and time to event outcomes (Kaplan-Meier curves, Cox proportional hazard modeling). Incorporated into the presentation of these models are subtopic topics such as regression diagnostics, nonparametric regression, splines, data reduction techniques, model validation, parametric bootstrapping, and a brief introduction to methods for handling missing data. Students are required to take 6312L concurrently. Prerequisite: Biostatistics 6311 or equivalent; familiarity with R or Stata software packages. SPRING.\n\n\nBIOS 6312L\nThis is a discussion section/lab for Modern Regression Analysis. Students will review relevant theory and work on applications as a group. Computing solutions and extensions will be emphasized. Students are required to take 6312 concurrently."
  },
  {
    "objectID": "syllabus.html#course-materials",
    "href": "syllabus.html#course-materials",
    "title": "Syllabus BIOS 6312",
    "section": "Course Materials",
    "text": "Course Materials\n\nCourse notes\n\nCourse notes will be the primary source\nAvailable on web page\nDaily class schedule will indicate notes being covered\nNotes will be updated throughout semester\n\n\n\nTextbooks\n\nThere are no required textbooks for this course\nThe following are provided as references that are at an appropriate level for this course\n\nRegression Methods in Biostatistics. Vittinghoff, Glidden, Shiboski, and McCulloch\nApplied Liner Regression. Weisberg.\nBayesian and Frequentist Regression Methods. Wakefield"
  },
  {
    "objectID": "syllabus.html#grading-and-evaluation",
    "href": "syllabus.html#grading-and-evaluation",
    "title": "Syllabus BIOS 6312",
    "section": "Grading and Evaluation",
    "text": "Grading and Evaluation\n\nEvaluation components and grade percentages\n\nMidterm (25%)\nTake Home Exam (25%)\nFinal Exam (25%)\nHomework (25%)\nClass participation\nThis is a 4-credit course. Your lab and lecture grades will be the same\n\n\n\nHomework\n\nUp to 1 per week (probably 6 or 7 total)\nWill focus on real data analysis and interpretation with some mathematical derivations of important quantities\nQuestions will focus on specific analyses, with questions stated in as scientific terms as possible\nWork handed in should address the scientific questions\n\nFormat Table and Figures\n\nKeys will be provided shortly after the homework is turned in\n\nNo late homework accepted after the key is posted\n\nAnswers in keys may go beyond what is expected of your homework and present concepts in more detail. You are responsible for any material in the keys for exams.\nYou may discuss the homework with others in the class, but the work you turn in should be your own\nUse Brightspace to turn in homeworks and receive feedback and grade\n\n\n\nIn Class Exams\n\nMidterm and Final in class\n\nFocus on understanding concepts, not memorizing formulas\nI will provide an example midterm and final\nFor midterm, you will be allowed 1 page of your own notes\nFor final, you will be allowed 2 pages of your own notes\n\nAll output will be provide for you to interpret\n\n\n\nTake Home Exam\n\nWill be given approximately mid point between Midterm and Final\nDemonstrate ability to obtain results through software and interpret findings\nOne day to complete and turn in\n\nLikely will be a Monday with no lab scheduled for that day\n\nSimilar to Homework, but work must be your own"
  },
  {
    "objectID": "syllabus.html#expectations-and-policies",
    "href": "syllabus.html#expectations-and-policies",
    "title": "Syllabus BIOS 6312",
    "section": "Expectations and Policies",
    "text": "Expectations and Policies\n\nExpecations you can have of me\n\nYou should expect me to provide feedback on homeworks and exams in a timely fashion\nYou should expect me to be responsive to your questions and concerns. If you have emailed me and not received a response with 24 hours, please feel free to email again. It is best to use my VUMC rather than Vanderbilt email address.\n\nAttendance. The course is offered in-person. If you expect to be absent, please let me know and make plans to catch up. Class will start on time.\nCollaboration\n\nDiscussing course content is highly encouraged\nCollaborating on homeworks is highly encouraged, but you need to turn in your own assignment written in your own words\nExams (take home and in class) are individual effort\n\nAcademic honesty\n\nStudents are expected to follow the Vanderbilt Honor Code\n“Vanderbilt University students pursue all academic endeavors with integrity. They conduct themselves honorably, professionally and respectfully in all realms of their studies in order to promote and secure an atmosphere of dignity and trust. The keystone of our honor system is self-regulation, which requires cooperation and support from each member of the University community.”\n\nUse of generative AI algorithms such as ChatGPT\n\nYou are free to use generative AI in your work, however you must\n\nCite any text that AI generated (even if you edited it) with a bibliographic entry indicating the name and version of the AI model that you used, the date and time it was used, and the exact query or prompt to generate the results\nCite as above any code that was generated for you. I recommend that you do not use it to blindly write code for you. Doing so will probably be more work than simply writing the code yourself. You must verify any code that is generated for you is accurate and answers the question in the assignment instructions.\n\nI recommend using Vanderbilt or VUMC resources\n\nhttps://www.vanderbilt.edu/generative-ai/\nhttps://www.vumc.org/dbmi/GenerativeAI\n\n\nLate work\n\nWhile I expect that work will be turned in on time, things can happen to interrupt your schedule\nMy goal is to provide sufficient time for completing all assignments\nIf you anticipate a problem with a due date, it is best to let me know sooner (e.g. when a homework is assigned) rather than later\nLate homeworks will be accepted on a case by case basis. No late homeworks will be accepted after the key is provided.\n\nVoicing concerns and evaluations\n\nPlease feel free to bring up any concerns you have about the course material, how it is being presented, or how you are being evaluated at any time during the semester. I want you to know that your voice will be heard.\nPlease complete the end of course evaluations. They are a valuable resource for me and help to guide changes from year to year. I read all comments and will take them seriously. Comments about what worked well as well as constructive criticism are appreciated"
  },
  {
    "objectID": "syllabus.html#accommodations",
    "href": "syllabus.html#accommodations",
    "title": "Syllabus BIOS 6312",
    "section": "Accommodations",
    "text": "Accommodations\n\nI encourage students who encounter accessibility challenges to communicate with me regardless of whether they are registered with Equal Opportunity and Access\nPlease communicate with me at your earliest convenience so we can discuss specific actions to address your needs. I will make every effort to accommodate reasonable requests.\nIf you have established accommodations with Equal Opportunity and Access, I will receive an email notifying me of the request.\nIf you need to contact Equal Opportunity and Access to establish service, the address is https://www.vanderbilt.edu/eeo/disability_services/contact_us.php"
  },
  {
    "objectID": "computing.html",
    "href": "computing.html",
    "title": "Computing",
    "section": "",
    "text": "Course notes and primary examples are based on R. I used the following version of R to build the course notes\n\nR.version$version.string\n\n[1] \"R version 4.3.2 (2023-10-31)\"\n\n\nIt is best to be using the latest version of R."
  },
  {
    "objectID": "computing.html#r",
    "href": "computing.html#r",
    "title": "Computing",
    "section": "",
    "text": "Course notes and primary examples are based on R. I used the following version of R to build the course notes\n\nR.version$version.string\n\n[1] \"R version 4.3.2 (2023-10-31)\"\n\n\nIt is best to be using the latest version of R."
  },
  {
    "objectID": "computing.html#rstudio",
    "href": "computing.html#rstudio",
    "title": "Computing",
    "section": "Rstudio",
    "text": "Rstudio\nCurrent versions of RStudio support editing and preview of Quarto documents.\nTo efficiently use Quarto within RStudio it is strongly recommended that you use the latest release of RStudio. You need to be using RStudio (v2023.06) or later.\nYou can download RStudio from https://posit.co/download/rstudio-desktop/."
  },
  {
    "objectID": "computing.html#quarto",
    "href": "computing.html#quarto",
    "title": "Computing",
    "section": "Quarto",
    "text": "Quarto\nQuarto is a multi-language, next generation version of R Markdown from RStudio, with many new features and capabilities. Like R Markdown, Quarto uses Knitr to execute R code, and is therefore able to render most existing Rmd files without modification.\nTo use Quarto with R, you should install the rmarkdown R package:\ninstall.packages(\"rmarkdown\")\nInstallation of the rmarkdown package will also install the knitr package so you will have everything required to render documents containing R code."
  },
  {
    "objectID": "computing.html#r-packages",
    "href": "computing.html#r-packages",
    "title": "Computing",
    "section": "R packages",
    "text": "R packages\nThere are many ways to achieve the same results in R. To create the notes, I utilize base R and a variety of different packages that provide one way of obtaining the results. The packages needed used to create the notes are listed at the beggining of each set of notes.\nIf you do not have the packages I used installed, they will need to be installed first to recreate the analyses in the notes. The packages I use and demonstrate may not be unique. Other packages may do something similar and allow you to arrive at the same final answer.\n\nInstalling and loading R packages\nThe simplest way to install an R packages is to use the install.package function. For example,\ninstall.packages(\"rms\")\nAfter installation, you can load the package\nlibrary(\"rms\")\nSome packages install smoothly, others can generate errors if necessary dependencies are not installed. I will add known issues and solutions to this document as needed throughout the course."
  },
  {
    "objectID": "computing.html#stata",
    "href": "computing.html#stata",
    "title": "Computing",
    "section": "Stata",
    "text": "Stata\nStata also should be able to conduct all of these analyses necessary for this course. The main advantage to Stata is that its syntax is relatively straightforward. It also has a menu system that can be used to create code and make the analysis reproducible through .do files.\nUCLA Statistical Methods and Data Analytics maintain a good website for those interested in learning more about the capabilities of Stata\nThere is a Statamarkdown package under development that allows for Stata code to be used in Quarto. I do not have wide experience with this package, but more information is available at https://github.com/hemken/Statamarkdown. It is of no use if you do not have Stata installed."
  },
  {
    "objectID": "labs/lab2-linearassumptions.html",
    "href": "labs/lab2-linearassumptions.html",
    "title": "Lab 2: Assumption in Linear Regression",
    "section": "",
    "text": "Original simulation (Patrick Ward)\n\nSimulations in R Part 5: Homoskedasticity Assumption in Regression\nConsult parts 1 to 4 if you want more background information simulations and resampling in R\n\nModifications (Chris Slaughter)\n\nAllow for different sample sizes\nAdd robust standard error estimates for comparison\n\nUse the rms packages and ols function to fit linear models\n\nConsider different error distributions"
  },
  {
    "objectID": "labs/lab2-linearassumptions.html#introduction",
    "href": "labs/lab2-linearassumptions.html#introduction",
    "title": "Lab 2: Assumption in Linear Regression",
    "section": "",
    "text": "Original simulation (Patrick Ward)\n\nSimulations in R Part 5: Homoskedasticity Assumption in Regression\nConsult parts 1 to 4 if you want more background information simulations and resampling in R\n\nModifications (Chris Slaughter)\n\nAllow for different sample sizes\nAdd robust standard error estimates for comparison\n\nUse the rms packages and ols function to fit linear models\n\nConsider different error distributions"
  },
  {
    "objectID": "labs/lab2-linearassumptions.html#regression-assumptions",
    "href": "labs/lab2-linearassumptions.html#regression-assumptions",
    "title": "Lab 2: Assumption in Linear Regression",
    "section": "Regression Assumptions",
    "text": "Regression Assumptions\nThere are a number of assumptions that underpin linear regression models. Simulation can be a useful way of exploring these assumptions and understanding how violating these assumptions can lead to bias, large variance in the regression coefficients, and/or poor predictions.\nSome typical assumptions include:\n\nHomoskedasticity\nMulticollinearity of independent variables\nMeasurement Error\nSerial correlation\n\nToday, we will explore the assumption of homoskedasticity and the distribution of the error terms for varying sample sizes."
  },
  {
    "objectID": "labs/lab2-linearassumptions.html#coverage-probability",
    "href": "labs/lab2-linearassumptions.html#coverage-probability",
    "title": "Lab 2: Assumption in Linear Regression",
    "section": "Coverage Probability",
    "text": "Coverage Probability\nWe will primarily be evaluating the coverage probability. The coverage probability is the probability that confidence interval will contain the true values of interest. Coverage probability is a common frequentist statistic that is used to describe the behavior of a model.\nWe can estimate the the coverage probability through statistical simulation. If we simulate many datasets and calculate a 95% confidence interval based on each dataset, we can count the number of times that the confidence interval contains the true value. If a 95% CI contains the true value 95% of the time, the coverage probability is correct."
  },
  {
    "objectID": "labs/lab2-linearassumptions.html#lab-instructions",
    "href": "labs/lab2-linearassumptions.html#lab-instructions",
    "title": "Lab 2: Assumption in Linear Regression",
    "section": "Lab Instructions",
    "text": "Lab Instructions\n\nConsider the following simulation as currently written. Run the simulation an obtain the results for baseline simulation and the homoskedastiicity simulation.\nModify the simulation code to include robust standard error estimates in addition to classical standard error estimates. Then, compare the coverage probabilities of the intercept and slope when using robust standard errors to classical standard errors.\nCreate a new section to allow us to modify the models further. Consider\n\nDifferent sample sizes, particularly sample sizes below 500. How does sample size impact the coverage probabilities when using the robust and classical standard error estimates?\nDifferent distributions for the error variance rather than Normal. Such as,\n\nt-distribution with 1 d.f. (Cauchy; very heavy tailed)\nt-distribution with 7 d.f. (heavy tailed, less so)\nexponential (1) - 1 (skewed right, subtract 1 so has mean 0)\nOther distributions of your choice\n\nI will collect results from the class and collect in a table so we can see that patterns, if any, that appear"
  },
  {
    "objectID": "labs/lab2-linearassumptions.html#creating-the-baseline-simulation",
    "href": "labs/lab2-linearassumptions.html#creating-the-baseline-simulation",
    "title": "Lab 2: Assumption in Linear Regression",
    "section": "Creating the baseline simulation",
    "text": "Creating the baseline simulation\nBefore exploring how violations of the homoskedasticity assumption influence a regression model, we need a baseline model to compare it against. So, we will begin by simulating a simple linear regression with 1 predictor. Our model will look like this:\n\\(y = 2 + 5*x + e\\)\nWhere e will be random error from a normal distribution with a mean of 0 and standard deviation of 1.\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n\n\n✔ broom        1.0.5     ✔ recipes      1.0.9\n✔ dials        1.2.0     ✔ rsample      1.2.0\n✔ dplyr        1.1.2     ✔ tibble       3.2.1\n✔ ggplot2      3.4.4     ✔ tidyr        1.3.0\n✔ infer        1.0.5     ✔ tune         1.1.2\n✔ modeldata    1.2.0     ✔ workflows    1.1.3\n✔ parsnip      1.1.1     ✔ workflowsets 1.0.1\n✔ purrr        1.0.2     ✔ yardstick    1.2.0\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Search for functions across packages at https://www.tidymodels.org/find/\n\nlibrary(patchwork)\nlibrary(rms)\n\nLoading required package: Hmisc\n\n\nLoading required package: lattice\n\n\nLoading required package: survival\n\n\nLoading required package: Formula\n\n\n\nAttaching package: 'Hmisc'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    knitrSet\n\n\nThe following object is masked from 'package:parsnip':\n\n    translate\n\n\nThe following objects are masked from 'package:dplyr':\n\n    src, summarize\n\n\nThe following objects are masked from 'package:base':\n\n    format.pval, units\n\n\nLoading required package: SparseM\n\n\n\nAttaching package: 'SparseM'\n\n\nThe following object is masked from 'package:base':\n\n    backsolve\n\n## set seed for reproducibility\nset.seed(58968)\n\n## create a data frame to store intercept values, slope values, their standard errors, and the model residual standard error, for each simulation\nsim_params &lt;- data.frame(intercept = NA,\n                      slope = NA,\n                      intercept_se = NA,\n                      slope_se = NA,\n                      model_rse = NA)\n\n# true intercept value\nintercept &lt;- 2\n\n# true slope value\nslope &lt;- 5\n\n## Number of indepdendent observations\nn &lt;- 500\n\n## Number of simulation replications to run\nreps &lt;- 5000\n\n# random draw from a uniform distribution to simulate the predictor variable\nX &lt;- runif(n = n, min = -1, max = 1)\n\n## loop for regression model\nfor(i in 1:reps){\n  \n  # create dependent variable, Y\n  Y &lt;- intercept + slope*X + rnorm(n = n, mean = 0, sd = 1)\n  \n  # build model\n  model &lt;- ols(Y ~ X, x=TRUE)\n  \n  # # store predictions\n  fitted_vals &lt;- model$fitted.values\n\n  # # store residuals\n  # output_df[i, 2] &lt;- model$residuals\n  \n  # variance-covariance matrix for the model\n  vcv &lt;- vcov(model)\n  \n  # estimates for the intercept\n  sim_params[i, 1] &lt;- model$coef[1]\n  \n  # estimates for the slope\n  sim_params[i, 2] &lt;- model$coef[2]\n  \n  # SE for the intercept\n  sim_params[i, 3] &lt;- sqrt(diag(vcv)[1])\n  \n  # SE for the slope\n  sim_params[i, 4] &lt;- sqrt(diag(vcv)[2])\n  \n  # model RSE\n  sim_params[i, 5] &lt;- model$stats[\"Sigma\"]\n  \n}\n\nhead(sim_params)\n\n  intercept    slope intercept_se   slope_se model_rse\n1  1.966323 4.977753   0.04495439 0.07883016 1.0036923\n2  1.947826 4.966831   0.04460191 0.07821206 0.9958226\n3  2.111264 4.900025   0.04508832 0.07906500 1.0066825\n4  2.029259 4.970287   0.04317960 0.07571795 0.9640667\n5  1.976044 5.064834   0.04446964 0.07798012 0.9928694\n6  1.977989 5.071687   0.04461868 0.07824146 0.9961969\n\n\nNow we summarize the data to see if we have values close to the specified model parameters\n\n# Calculate mean for each column of sim_params\napply(sim_params, 2, mean)\n\n   intercept        slope intercept_se     slope_se    model_rse \n  1.99925865   4.99970448   0.04479625   0.07855285   1.00016164 \n\n\nThe final model of the ‘r reps’ iterations is also stored from our for loop and we can look directly at it and create plots of the model fit.\n\nd&lt;- datadist(X)\noptions(datadist=\"d\")\n# model summary\nmodel\n\nLinear Regression Model\n \n ols(formula = Y ~ X, x = TRUE)\n \n                  Model Likelihood    Discrimination    \n                        Ratio Test           Indexes    \n Obs     500    LR chi2    1121.81    R2       0.894    \n sigma0.9745    d.f.             1    R2 adj   0.894    \n d.f.    498    Pr(&gt; chi2)  0.0000    g        3.264    \n \n Residuals\n \n      Min       1Q   Median       3Q      Max \n -2.42004 -0.63347 -0.02173  0.71448  2.72872 \n \n \n           Coef   S.E.   t     Pr(&gt;|t|)\n Intercept 2.0326 0.0436 46.57 &lt;0.0001 \n X         4.9585 0.0765 64.78 &lt;0.0001 \n \n\n# model fit plots\nplot(model, which=1)\n\n\n\n\nWe can also create a function that lets us evaluate how often the 95% confidence interval of our simulated beta coefficients cover the true beta coefficients that we specified for the simulation. From there, we can get a coverage probability and a 95% probability coverage interval\n\n### Create a coverage probability function\ncoverage_interval95 &lt;- function(beta_coef, se_beta_coef, true_beta_val, model_df){\n  \n  level95 &lt;- 1 - (1 - 0.95) / 2\n  \n  # lower 95\n  lower95 &lt;- beta_coef - qt(level95, df = model_df)*se_beta_coef\n  \n  # upper 95\n  upper95 &lt;- beta_coef + qt(level95, df = model_df)*se_beta_coef\n  \n  # what rate did we cover the true value (hits and misses)\n  hits &lt;- ifelse(true_beta_val &gt;= lower95 & true_beta_val &lt;= upper95, 1, 0)\n  prob_cover &lt;- mean(hits)\n  \n  # create the probability coverage intervals\n  low_coverage_interval &lt;- prob_cover - 1.96 * sqrt((prob_cover * (1 - prob_cover)) / length(beta_coef))\n  \n  upper_coverage_interval &lt;- prob_cover + 1.96 * sqrt((prob_cover * (1 - prob_cover)) / length(beta_coef))\n  \n  # results in a list\n  return(list('Probability of Covering the True Value' = prob_cover,\n              '95% Probability Coverage Intervals' = c(low_coverage_interval, upper_coverage_interval)))\n  \n}\n\nLet’s apply it to the intercept.\n\ncoverage_interval95(beta_coef = sim_params$intercept,\n                    se_beta_coef = sim_params$intercept_se,\n                    true_beta = intercept,\n                    model_df = model$df.residual)\n\n$`Probability of Covering the True Value`\n[1] 0.948\n\n$`95% Probability Coverage Intervals`\n[1] 0.9418457 0.9541543\n\n\nNow apply it to the slope\n\ncoverage_interval95(beta_coef = sim_params$slope,\n                    se_beta_coef = sim_params$slope_se,\n                    true_beta = slope,\n                    model_df = model$df.residual)\n\n$`Probability of Covering the True Value`\n[1] 0.9538\n\n$`95% Probability Coverage Intervals`\n[1] 0.9479814 0.9596186\n\n\nIn both cases we are covering the true betas around 95% of the time, with relatively small intervals."
  },
  {
    "objectID": "labs/lab2-linearassumptions.html#homoskedasticity",
    "href": "labs/lab2-linearassumptions.html#homoskedasticity",
    "title": "Lab 2: Assumption in Linear Regression",
    "section": "Homoskedasticity",
    "text": "Homoskedasticity\nLinear models make an assumption that the variance of the residuals remain constant across the predicted values (homoskedastic). We can see what this looks like by plotting the fitted values relative to the residuals, which was the first plot in the model check plots we created for the last simulation above. We can see that the residuals exhibit relatively the same amount of variance across the fitted values.\n\nplot(model, which = 1)\n\n\n\n\nLet’s simulate a model with heteroskedastic residuals and see what it looks like. We will keep the same intercept and slope parameters as above. The only thing will we do is add an exponential parameter to the error term of the model to create a heteroskedastic outcome in the residuals.\n\n## parameter for heteroskedasticity \nheteroskedasticity_param &lt;- 2\n\n## set seed for reproducibility\nset.seed(22)\n\n## data frame for results\nheteroskedastic_sim_params &lt;- data.frame(intercept = NA,\n                      slope = NA,\n                      intercept_se = NA,\n                      slope_se = NA,\n                      model_rse = NA)\n\n## for loop\nfor(i in 1:reps){\n  \n  # the error variance of Y is a function of X plus some random noise\n  Y &lt;- intercept + slope*X + rnorm(n = n, mean = 0, sd = exp(X*heteroskedasticity_param))\n  \n  # model\n  heteroskedastic_model &lt;- ols(Y ~ X, x=TRUE)\n  \n  \n  # variance-covariance matrix\n  vcv &lt;- vcov(heteroskedastic_model)\n  \n  # estimates for the intercept\n  heteroskedastic_sim_params[i, 1] &lt;- heteroskedastic_model$coef[1]\n  \n  # estimates for the slope\n  heteroskedastic_sim_params[i, 2] &lt;- heteroskedastic_model$coef[2]\n  \n  # SE for the intercept\n  heteroskedastic_sim_params[i, 3] &lt;- sqrt(diag(vcv)[1])\n  \n  # SE for the slope\n  heteroskedastic_sim_params[i, 4] &lt;- sqrt(diag(vcv)[2])\n  \n  # model RSE\n  heteroskedastic_sim_params[i, 5] &lt;- heteroskedastic_model$stats[\"Sigma\"]\n  \n}\n\nhead(heteroskedastic_sim_params)\n\n  intercept    slope intercept_se  slope_se model_rse\n1  1.884165 4.743169    0.0989576 0.1735279  2.209417\n2  2.096654 5.246191    0.1156089 0.2027270  2.581189\n3  1.950191 4.884566    0.1109784 0.1946072  2.477805\n4  1.996244 5.062478    0.1135296 0.1990809  2.534765\n5  2.048404 5.156628    0.1130880 0.1983065  2.524906\n6  2.043749 4.992856    0.1283863 0.2251330  2.866469\n\nplot(X, Y, pch = 19)\n\n\n\n\nThe relationship between X and Y certainly looks weird given how it starts very tightly on the left side and then fans out on the right side.\nLet’s take the average across all simulations for each coefficient and their corresponding standard errors.\n\napply(heteroskedastic_sim_params, 2, mean)\n\n   intercept        slope intercept_se     slope_se    model_rse \n   2.0006111    5.0038511    0.1119348    0.1962842    2.4991571 \n\n\nThe coefficients of 2.0 for the intercept and 5 for the slope are exactly what we set them as for the simulation. However, notice how much larger the standard errors are for the intercept and slope compared to the original model above. Additionally, notice that the model residual standard error has increased substantially compared to the previous model.\nLet’s get the last model again and check out the fitted vs residual plot.\n\n# fitted vs residuals\nplot(heteroskedastic_model, which = 1)\n\n\n\n\nThat looks like a large amount of heteroskedasticity as the residual variance is no longer homogenous across the range of fitted values. Notice the large fanning out towards the right side of the plot. As the predictions get larger so two does the variability in residuals, which we noticed when we plotted Y and X above.\nWhat we’ve learned is that the estimate of intercept and slope is unbiased for both the heteroskedastic and homoskedastic models, as they both are centered on the parameters that we specified for the simulation (intercept = 2, slope = 5). However, the heteroskedastic model creates greater variance in our coefficients. We can visualize how much uncertainty there is under the heteroskedastic model relative to the homoskedastic model by visualizing the density of the coefficient estimates from our two model simulations.\n\nplt_intercept &lt;- sim_params %&gt;%\n  mutate(model = 'homoskedastic model') %&gt;%\n  bind_rows(\n    heteroskedastic_sim_params %&gt;%\n      mutate(model = 'heteroskedastic model')\n  ) %&gt;%\n  ggplot(aes(x = intercept, fill = model)) +\n  geom_density(alpha = 0.6) +\n  theme_classic() +\n  theme(legend.position = \"top\")\n\nplt_slope &lt;- sim_params %&gt;%\n  mutate(model = 'homoskedastic model') %&gt;%\n  bind_rows(\n    heteroskedastic_sim_params %&gt;%\n      mutate(model = 'heteroskedastic model')\n  ) %&gt;%\n  ggplot(aes(x = slope, fill = model)) +\n  geom_density(alpha = 0.6) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\nplt_intercept | plt_slope\n\n\n\n\nFinally, let’s see how often the 95% coverage interval is covering the true intercept and slope in the heteroskedastic model.\n\ncoverage_interval95(beta_coef = heteroskedastic_sim_params$intercept,\n                    se_beta_coef = heteroskedastic_sim_params$intercept_se,\n                    true_beta = intercept,\n                    model_df = model$df.residual)\n\n$`Probability of Covering the True Value`\n[1] 0.9344\n\n$`95% Probability Coverage Intervals`\n[1] 0.9275374 0.9412626\n\ncoverage_interval95(beta_coef = heteroskedastic_sim_params$slope,\n                    se_beta_coef = heteroskedastic_sim_params$slope_se,\n                    true_beta = slope,\n                    model_df = model$df.residual)\n\n$`Probability of Covering the True Value`\n[1] 0.829\n\n$`95% Probability Coverage Intervals`\n[1] 0.8185637 0.8394363\n\n\nNotice that we are no longer covering the true model values at the 95% level."
  },
  {
    "objectID": "labs/lab6-survival.html",
    "href": "labs/lab6-survival.html",
    "title": "Lab 6: Survival Analysis",
    "section": "",
    "text": "All questions relate to the question of whether treatment (DES) and index (a measure of disease severity) are prognostic of survival time for prostate cancer. The data is posted on the class web pages (prostaticcancer.dta, prostaticcancer.dat). We will consider four variables in this lab:\n\nTime: Time to death or censoring (months)\nStatus: Indicator of death (status=0 if subject censored, status=1 if subject died)\nTreatment: Two treatments are considered. Treatment 1 is placebo, Treatment 2 is DES\nIndex: Gleason index, a measure of disease severity\n\nPerform analyses to determine whether the distribution of time to relapse differs across groups defined by treatment and index.\n\n\nSome useful R functions\n\nSurv\nsurvfit\nsurvdiff\nggsurvplot\ncoxph"
  },
  {
    "objectID": "labs/lab6-survival.html#introduction",
    "href": "labs/lab6-survival.html#introduction",
    "title": "Lab 6: Survival Analysis",
    "section": "",
    "text": "All questions relate to the question of whether treatment (DES) and index (a measure of disease severity) are prognostic of survival time for prostate cancer. The data is posted on the class web pages (prostaticcancer.dta, prostaticcancer.dat). We will consider four variables in this lab:\n\nTime: Time to death or censoring (months)\nStatus: Indicator of death (status=0 if subject censored, status=1 if subject died)\nTreatment: Two treatments are considered. Treatment 1 is placebo, Treatment 2 is DES\nIndex: Gleason index, a measure of disease severity\n\nPerform analyses to determine whether the distribution of time to relapse differs across groups defined by treatment and index.\n\n\nSome useful R functions\n\nSurv\nsurvfit\nsurvdiff\nggsurvplot\ncoxph"
  },
  {
    "objectID": "labs/lab6-survival.html#setup",
    "href": "labs/lab6-survival.html#setup",
    "title": "Lab 6: Survival Analysis",
    "section": "Setup",
    "text": "Setup\n\nPacakges\n\n\nLoading required package: Hmisc\n\n\nLoading required package: lattice\n\n\nLoading required package: Formula\n\n\nLoading required package: ggplot2\n\n\n\nAttaching package: 'Hmisc'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    knitrSet\n\n\nThe following objects are masked from 'package:base':\n\n    format.pval, units\n\n\nLoading required package: SparseM\n\n\n\nAttaching package: 'SparseM'\n\n\nThe following object is masked from 'package:base':\n\n    backsolve\n\n\nLoading required package: ggpubr\n\n\n\nAttaching package: 'survminer'\n\n\nThe following object is masked from 'package:survival':\n\n    myeloma\n\n\n\n\nLoad data\n\ncancer &lt;- stata.get(\"http://biostat.app.vumc.org/wiki/pub/Main/CourseBios312/prostaticcancer.dta\")\nnames(cancer)\n\n[1] \"patient\"   \"treatment\" \"time\"      \"status\"    \"age\"       \"shb\"      \n[7] \"size\"      \"index\"    \n\nsummary(cancer)\n\n    patient        treatment          time           status      \n Min.   : 1.00   Min.   :1.000   Min.   : 2.00   Min.   :0.0000  \n 1st Qu.:10.25   1st Qu.:1.000   1st Qu.:42.25   1st Qu.:0.0000  \n Median :19.50   Median :2.000   Median :56.00   Median :0.0000  \n Mean   :19.50   Mean   :1.526   Mean   :49.74   Mean   :0.1579  \n 3rd Qu.:28.75   3rd Qu.:2.000   3rd Qu.:65.00   3rd Qu.:0.0000  \n Max.   :38.00   Max.   :2.000   Max.   :70.00   Max.   :1.0000  \n      age             shb             size           index       \n Min.   :51.00   Min.   :10.70   Min.   : 2.00   Min.   : 6.000  \n 1st Qu.:65.00   1st Qu.:13.43   1st Qu.: 4.00   1st Qu.: 8.000  \n Median :71.00   Median :13.85   Median : 7.50   Median : 9.000  \n Mean   :68.63   Mean   :13.94   Mean   :10.47   Mean   : 9.132  \n 3rd Qu.:73.00   3rd Qu.:14.68   3rd Qu.:13.75   3rd Qu.:10.000  \n Max.   :77.00   Max.   :16.40   Max.   :37.00   Max.   :12.000"
  },
  {
    "objectID": "labs/lab6-survival.html#questions",
    "href": "labs/lab6-survival.html#questions",
    "title": "Lab 6: Survival Analysis",
    "section": "Questions",
    "text": "Questions\n\n1. Before looking at the data, we should decide if we are going to robust standard errors or not\n\nWhat are the benefits of using robust standard errors over classical PH regression?\nWhat are the benefits of using classical standard error over robust standard errors?\n\n\n\n2. Provide suitable descriptive statistics regarding the distribution of time to relapse according to treatment status.\n\nCreate and plot Kaplan-Meier estimate of the survival curves by treatment\nWhat is the (approximate) survival estimate at 30 months for each treatment arm? What is it at 60 months? Obtain the estimate and 95% confidence interval for survival at 30 months. Interpret the estimate and CI.\nWhat is the estimated median survival time in each treatment arm? Provide a 95% confidence interval for these estimates. Interpret the estimate and CI.\n\n\n\n3. List the variables time and status for the subjects with treatment==2\n\nBe able to interpret what each row indicates in terms of event/censoring time and event/censoring indicator. That is, which observations are events and which are censored? When did each occur?\nCalculate by hand the Kaplan-Meier estimate of survivorship for the first few event times. Compare to the plot. At which time points does survivorship decrease? At which time points does it stay the same?\n\n\n\n4. Perform analysis comparing the instantaneous risk of relapse across groups defined treatment status using the following approaches. Compare the inference obtained from each approach.\n\nThe log rank test (this is a score test)\nCox proportional hazard regression using classical standard errors. This will give both a Wald and Likelihood Ratio test\nCox proportional hazard regression using robust standard errors. This will give a Wald test\nWhen you “compare the inference obtained…”, interpret the hazard ratio and corresponding 95% confidence intervals.\n\n\n\n5. Perform a proportional hazards regression comparing the instantaneous risk of relapse across groups defined by Gleason index (index). Compare the inference obtained from each approach.\n\nCox proportional hazard regression using classical standard errors.\nCox proportional hazard regression using robust standard errors."
  },
  {
    "objectID": "labs/lab3-logistic-questions.html",
    "href": "labs/lab3-logistic-questions.html",
    "title": "Lab 3: Logistic Regression",
    "section": "",
    "text": "In this lab, we will use logistic regression to analyze the salary dataset. We will consider the covariates gender and year of degree.\n\n\nLoad packages\n\n\nLoading required package: Hmisc\n\n\nLoading required package: lattice\n\n\nLoading required package: survival\n\n\nLoading required package: Formula\n\n\nLoading required package: ggplot2\n\n\n\nAttaching package: 'Hmisc'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    knitrSet\n\n\nThe following objects are masked from 'package:base':\n\n    format.pval, units\n\n\nLoading required package: SparseM\n\n\n\nAttaching package: 'SparseM'\n\n\nThe following object is masked from 'package:base':\n\n    backsolve\n\n\nWelcome to contingencytables 2.2.1.\nPlease run ?chapX or list_functions(x) to see the functions related to chapter X.\n\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n\n\nAttaching package: 'lmtest'\n\n\nThe following object is masked from 'package:rms':\n\n    lrtest\n\n\n\nThe contigencytables packages contains a function for calculating the likelihood ratio test for 2x2 tables\n\n\nperondi_2004\n\n     [,1] [,2]\n[1,]    7   27\n[2,]    1   33\n\nLR_test_2x2(perondi_2004)\n\nThe likelihood ratio test: P = 0.01733, T = 5.663 (df = 1)"
  },
  {
    "objectID": "labs/lab3-logistic-questions.html#introduction",
    "href": "labs/lab3-logistic-questions.html#introduction",
    "title": "Lab 3: Logistic Regression",
    "section": "",
    "text": "In this lab, we will use logistic regression to analyze the salary dataset. We will consider the covariates gender and year of degree.\n\n\nLoad packages\n\n\nLoading required package: Hmisc\n\n\nLoading required package: lattice\n\n\nLoading required package: survival\n\n\nLoading required package: Formula\n\n\nLoading required package: ggplot2\n\n\n\nAttaching package: 'Hmisc'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    knitrSet\n\n\nThe following objects are masked from 'package:base':\n\n    format.pval, units\n\n\nLoading required package: SparseM\n\n\n\nAttaching package: 'SparseM'\n\n\nThe following object is masked from 'package:base':\n\n    backsolve\n\n\nWelcome to contingencytables 2.2.1.\nPlease run ?chapX or list_functions(x) to see the functions related to chapter X.\n\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n\n\nAttaching package: 'lmtest'\n\n\nThe following object is masked from 'package:rms':\n\n    lrtest\n\n\n\nThe contigencytables packages contains a function for calculating the likelihood ratio test for 2x2 tables\n\n\nperondi_2004\n\n     [,1] [,2]\n[1,]    7   27\n[2,]    1   33\n\nLR_test_2x2(perondi_2004)\n\nThe likelihood ratio test: P = 0.01733, T = 5.663 (df = 1)"
  },
  {
    "objectID": "labs/lab3-logistic-questions.html#part-1-initial-dataset-manipulation",
    "href": "labs/lab3-logistic-questions.html#part-1-initial-dataset-manipulation",
    "title": "Lab 3: Logistic Regression",
    "section": "Part 1: Initial dataset manipulation",
    "text": "Part 1: Initial dataset manipulation\n\n1. Read in the salary dataset\n\n\n2. Remove all observations that are not from 1995\n\n\n3. Create an indicator variable for male gender\n\n\n4. Create a new outcome variable (salhigh) to indicate if salary is above $7600 per month.\n\n4.1 What proportion of subjects have a salary above $7600"
  },
  {
    "objectID": "labs/lab3-logistic-questions.html#part-2-high-salary-and-gender-unadjusted-model",
    "href": "labs/lab3-logistic-questions.html#part-2-high-salary-and-gender-unadjusted-model",
    "title": "Lab 3: Logistic Regression",
    "section": "Part 2: High salary and gender (unadjusted model)",
    "text": "Part 2: High salary and gender (unadjusted model)\nIn part 2, we will estimate the association between salary and gender. First, we will conduct this analysis using the familiar method, a 2x2 table. We will then conduct the same analysis using logistic regression and compare the two approaches\n\n1. Create a 2x2 contingency table of salhigh by male. From this table, calculate\n\n1.1. Pearson’s Chi-squared test statistic (without continuity correction)\n\n\n1.2. The likelihood ratio Chi-squared test statistic\n\n\n1.3. The odds of high salary for males compared to the odds of a high salary for females (the odds ratio). Interpret\n\n\n1.4. The standard error of the log odds ratio (hint: the variance of the log odds ratio is given by 1/a + 1/b + 1/c + 1/d where a, b, c, and d are the cell totals)\n\n\n1.5. Pr(high salary | male)\n\n\n1.6. Pr(high salary | female)\n\n\n\n2. Conduct a logistic regression of male on salhigh. From the regression output, calculate or identify\n\n2.1. The Wald Chi-squared test statistic (hint: Recall if Z is Standard Normal, the Z^2 follow a Chi-squared distribution)\n\n\n2.2. The likelihood ratio Chi-squared statistic\n\n\n2.3. The odds of high salary for males compared to the odds of a high salary for females (the odds ratio). Interpret the odds ratio.\n\n\n2.4. The standard error of the log odds ratio\n\n\n2.5. Pr(high salary | male)\n\n\n2.6. Pr(high salary | female)\n\n\n\n3. Obtain estimates for the predicted probabilities 2.5 and 2.6 (use Stata or R commands to get these numbers)"
  },
  {
    "objectID": "labs/lab3-logistic-questions.html#part-3-high-salary-and-year-of-degree-unadjusted-model",
    "href": "labs/lab3-logistic-questions.html#part-3-high-salary-and-year-of-degree-unadjusted-model",
    "title": "Lab 3: Logistic Regression",
    "section": "Part 3: High salary and year of degree (unadjusted model)",
    "text": "Part 3: High salary and year of degree (unadjusted model)\n\n1. Create suitable summary statistics for the probability of high salary by year of degree\n\n\n2. Fit a logistic regression model with salhigh and yrdeg\n\n2.1. Calculate the odds ratio and interpret the results\n\n\n\n3. Suppose the investigators would rather model the number of years it has been since each professor has achieved his or her degree\n\n3.1. Create a new variable yrssince95 = 95 - yrdeg\n\n\n3.2. Fit a logistic regression model with salhigh and yrssince95\n\n\n3.3. Compare the output from model 3.2 with 2.1\n\n\n\n4. From model 3.2, create a variable for the predicted probability of having a high salary.\n\n4.1. Calculate the odds and log odds of having a high salary\n\n\n4.2. Plot yrssince95 versus the log odds of a having a high salary\n\n\n4.3. Plot yrssince95 versus the odds of having a high salary\n\n\n4.4. Plot yrssince95 versus the probability of having a high salary\n\n\n\n5. Conduct a likelihood ratio test to test the statistical significance of yrssince95\n\n5.1. Fit a logistic regression model with just the intercept. Save the deviance from this model\n\n\n5.2. Fit logistic regression model 3.2. Save the deviance from this model.\n\n\n5.3. Subtract the deviance obtained from 5.2 from the deviance obtained from 5.1. Compare this result to the likelihood ratio test statistic given in the output from model 3.2 (and 2.1)"
  },
  {
    "objectID": "labs/lab4-precision-questions.html",
    "href": "labs/lab4-precision-questions.html",
    "title": "Lab 4: Precision and adjustment for baseline",
    "section": "",
    "text": "Baseline covariates impact the outcome in many clinical trials. Although baseline adjustment is not always necessary, in case of a strong or moderate association between a baseline covariate(s) and the primary outcome measure, adjustment for such covariate(s) generally improves the efficiency of the analysis and avoids conditional bias from chance covariate imbalance."
  },
  {
    "objectID": "labs/lab4-precision-questions.html#introduction",
    "href": "labs/lab4-precision-questions.html#introduction",
    "title": "Lab 4: Precision and adjustment for baseline",
    "section": "",
    "text": "Baseline covariates impact the outcome in many clinical trials. Although baseline adjustment is not always necessary, in case of a strong or moderate association between a baseline covariate(s) and the primary outcome measure, adjustment for such covariate(s) generally improves the efficiency of the analysis and avoids conditional bias from chance covariate imbalance."
  },
  {
    "objectID": "labs/lab4-precision-questions.html#part-1-continuous-outcomes",
    "href": "labs/lab4-precision-questions.html#part-1-continuous-outcomes",
    "title": "Lab 4: Precision and adjustment for baseline",
    "section": "Part 1: Continuous outcomes",
    "text": "Part 1: Continuous outcomes\nIf a baseline value of a continuous primary outcome measure is available, then this should usually be included as a covariate.\nFirst, we will go over the overview slides on Supplemental Materials on course web page: Analyzing change from baseline in trials. The mean difference presented in the slides can be expressed using the following three linear regression model.\n\nModels\nFor a continuous outcome \\(Y\\), also measured at baseline \\(W\\) and treatment group \\(X\\), consider the following linear regression models. Let \\(Z = Y - W\\), the change from baseline.\n\nModel 1: Compare the mean final value by treatment group\n\\[\nY_i = \\gamma_0 + \\gamma_1*X_i + \\epsilon_i\n\\]\n\n\nModel 2: Compare the mean change, final minus initial, by treatment group.\n\\[\nZ_i = \\alpha_0 + \\alpha_1 * X_i + e_i\n\\]\n\n\nModel 3: Final value adjusted for baseline by treatment group\n\\[\nY_i = \\beta_0 + \\beta_1 * X_i + \\beta_2 * W_i + e_i\n\\]\n\n\nQ 1.1 When is model 1 preferred or model 2? That is, what values of \\(\\rho\\) (correlation between baseline and followup values of outcome) lead to smaller variance under each model?\n\nIf we assume equal variance for the baseline and final values\n\n\\(\\textrm{Var}(Y) = \\textrm{Var}(W) = \\sigma^2\\)\n\\(\\textrm{Cov}(Y,W) = \\rho \\sigma^2\\)\n\nFor Model 1\n\n\\[\n\\textrm{Var(Y)} = \\sigma^2\n\\]\n\nAnd we can calculate for Model 2\n\n\\[\n\\begin{aligned}\n\\textrm{Var}(Z) & = \\textrm{Var}(Y-W) \\\\\n& = \\textrm{Var}(Y) + \\textrm{Var}(W) - 2\\textrm{Cov}(Y,W) \\\\\n& = 2\\sigma^2 (1 - \\rho)\n\\end{aligned}\n\\]\n\n\nQ 1.2 When is model 3 preferred over models 1 and 2? Consider different values of \\(\\rho\\).\n\nIn model 3 where we adjust for baseline\n\n\\[\n\\begin{aligned}\n\\textrm{Var}(Y - \\rho W) & = \\textrm{Var(Y)} - \\rho(\\textrm{Cov}(Y,W)) + \\rho^2\\textrm{Var(W)} \\\\\n& = \\sigma^2 - 2\\rho^2\\sigma^2 + \\rho^2\\sigma^2. \\\\\n& = \\sigma^2(1-\\rho^2)\n\\end{aligned}\n\\]\n\n\nSummary table\nThe following table gives the calculated SD from the formulas for Models 1, 2 and 3 for values of rho ranging from -0.2 to 0.9. Models 1 is more efficient than model 2 for low correlations below 0.5; Model 2 is more efficient than model 1 for high correlations above 0.5; Model 1 and Model 2 are equivalent at rho of 0.5. Model 3, adjustment for baseline, is equivalent to Model 1 when rho is 0 (baseline and followup are not correlated). Otherwise, model 3 is always better than models 1 or 2. For very high correlations, Models 2 and 3 have similar efficiency. Model 3 improves efficiency if rho is positive or negative.\n\nrho &lt;- seq(-.2,.9,by=.1)\nsigma &lt;- 1\n\n# Model 1\nsd1 &lt;- rep(sigma, length(rho))\n\n# Model 2\nsd2 &lt;- sqrt(2*sigma^2*(1-rho))\n\n# Model 3\nsd3 &lt;- sqrt(sigma^2*(1-rho^2))\n\ncbind(rho, sd1, sd2, sd3)\n\n       rho sd1       sd2       sd3\n [1,] -0.2   1 1.5491933 0.9797959\n [2,] -0.1   1 1.4832397 0.9949874\n [3,]  0.0   1 1.4142136 1.0000000\n [4,]  0.1   1 1.3416408 0.9949874\n [5,]  0.2   1 1.2649111 0.9797959\n [6,]  0.3   1 1.1832160 0.9539392\n [7,]  0.4   1 1.0954451 0.9165151\n [8,]  0.5   1 1.0000000 0.8660254\n [9,]  0.6   1 0.8944272 0.8000000\n[10,]  0.7   1 0.7745967 0.7141428\n[11,]  0.8   1 0.6324555 0.6000000\n[12,]  0.9   1 0.4472136 0.4358899\n\n\n\n\n\nPower/sample size calculations\n\nSupplemental Notes Example\n\nAssuming a correlation of 0.4 between baseline and follow-up pain scores, a clinically important difference of 1.8 NRS units, a standard deviation of 3 NRS units, power of 80% and significance level of 5%, the following sample sizes are required:\n\n\nR code and results\n\nUsing the formulas above, we can find the standard deviation under the three analysis approaches (Final, Change, Regression adjusted for baseline)\nEffect size is then difference in means (1.8) divided by the standard deviations\nIn R, input this effect size divided by the standard deviation (d) and either power (to output sample size) or sample size (to output power)\n\n\nlibrary(pwr)\n\n# For final value\ns1 &lt;- 3\n\n# For change\ns2 &lt;- sqrt(2*3^2*(1-0.4))\ns2\n\n[1] 3.286335\n\n# For linear regression (Stata calls this ANCOVA)\ns3 &lt;- sqrt(3^2*(1-.4^2))\ns3\n\n[1] 2.749545\n\n# Final value\npwr.t.test(n=NULL, d=1.8/s1, power=.9)\n\n\n     Two-sample t test power calculation \n\n              n = 59.35155\n              d = 0.6\n      sig.level = 0.05\n          power = 0.9\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n# Change\npwr.t.test(n=NULL, d=1.8/s2, power=.9)\n\n\n     Two-sample t test power calculation \n\n              n = 71.02369\n              d = 0.5477226\n      sig.level = 0.05\n          power = 0.9\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n# Regression adjusting for baseline\npwr.t.test(n=NULL, d=1.8/s3, power=.9)\n\n\n     Two-sample t test power calculation \n\n              n = 50.01479\n              d = 0.6546537\n      sig.level = 0.05\n          power = 0.9\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n\n\nStata approach\n\nStata can give these same results, but doesn’t require you to know the standard deviation formulas given above for Models 1, 2 and 3\nFollowing is the full Stata output for the example.\n(You would have to know that ANCOVA is equivalent to the linear regression model adjusted for baseline)\n\n. sampsi 0 1.8, sd(3) pre(1) post(1) r01(.4)\n\nEstimated sample size for two samples with repeated measures\nAssumptions:\n                                      alpha =   0.0500  (two-sided)\n                                      power =   0.9000\n                                         m1 =        0\n                                         m2 =      1.8\n                                        sd1 =        3\n                                        sd2 =        3\n                                      n2/n1 =     1.00\n           number of follow-up measurements =        1\n            number of baseline measurements =        1\n   correlation between baseline & follow-up =    0.400\n\nMethod: POST\n relative efficiency =    1.000\n    adjustment to sd =    1.000\n        adjusted sd1 =    3.000\n\n Estimated required sample sizes:\n                  n1 =       59\n                  n2 =       59\n\nMethod: CHANGE\n relative efficiency =    0.833\n    adjustment to sd =    1.095\n        adjusted sd1 =    3.286\n\n Estimated required sample sizes:\n                  n1 =       71\n                  n2 =       71\n\nMethod: ANCOVA\n relative efficiency =    1.190\n    adjustment to sd =    0.917\n        adjusted sd1 =    2.750\n\n Estimated required sample sizes:\n                  n1 =       50\n                  n2 =       50\n\n. di sqrt(1-0.4^2)\n.91651514\n\n. di 3*sqrt(1-.4^2)\n2.7495454\n\n. di sqrt(2*1*(1-0.4))\n1.0954451"
  },
  {
    "objectID": "labs/lab4-precision-questions.html#part-2-binary-outcomes",
    "href": "labs/lab4-precision-questions.html#part-2-binary-outcomes",
    "title": "Lab 4: Precision and adjustment for baseline",
    "section": "Part 2: Binary outcomes",
    "text": "Part 2: Binary outcomes\nFor generalized linear models or non-linear models, adjusted and unadjusted treatment effects may not have the same interpretation and, sometimes, different results may be obtained from adjusted and unadjusted analyses. Thus, the choice of the appropriate covariates and the pre-specification of the primary model are critically important.\nRCT analysis with covariate adjustment (binary outcome)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bios 6312: Modern Regression",
    "section": "",
    "text": "This page contains an outline of the topics, content, and assignments for the semester. Note that this schedule will be updated as the semester progresses, with all changes documented here.\n\nNotes and Lab refer to course notes\nRMB is Regression Methods in Biostatistics\n\nSchedule updated Wed Apr 03 04:26:13 PM 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek\nDay\nDate\nTopic\nNotes.or.Lab\nSupplemental\nHomework\nReading\n\n\n\n\n1\nM\n2024/01/08\nIntroduction\nWelcome and Introduction\n\nInstall R and Rstudio\n\n\n\n1\nTu\n2024/01/09\nReview of Key Concepts\nLec 2\n\n\n\n\n\n1\nTh\n2024/01/11\nSimple Linear Regression\nLec 3\n\n\nRMB 3.3\n\n\n2\nM\n2024/01/15\nMartin Luther King, Jr Holiday\nno class\n\n\n\n\n\n2\nTu\n2024/01/16\nClass cancelled (weather)\nno class\n\n\n\n\n\n2\nTh\n2024/01/18\nSimple Linear Regression\nLab 1\n\n\nRMB 3.2\n\n\n3\nM\n2024/01/22\nSimple Linear Regression\nLab 1 and Lec 3\nMatrix Algebra Review\nHW 1 assigned\n\n\n\n3\nTu\n2024/01/23\nSimple Linear Regression\nLec 3\nLog Transformations\n\n\n\n\n3\nTh\n2024/01/25\nLinear Regression model assumptions\nLab 2\n\n\n\n\n\n4\nM\n2024/01/29\nSimple Linear Regression\nLec 3\n\n\n\n\n\n4\nTu\n2024/01/30\nSimple Logistic Regression\nLec 4\n\nHW 1 due; HW 2 assigned\nRMB 3.4; 6.1\n\n\n4\nTh\n2024/02/01\nLogistic Regression/Precision\nLec 4/Lec 5\n\n\n\n\n\n5\nM\n2024/02/05\nLogistic Regression Lab\nLab 3\n\n\n\n\n\n5\nTu\n2024/02/06\nPrecison/Confouding\nLec 5/Lec 6\n\nHW 2 due; HW 3 assigned\nRMB 5.1\n\n\n5\nTh\n2024/02/08\nConfounding and Effect Modification\nLec 6\n\n\n\n\n\n6\nM\n2024/02/12\nLab: Precision\nLab 4\n\n\n\n\n\n6\nTu\n2024/02/13\nMultivariable Models\nLec 7\n\nHW 3 due; HW 4 assigned\n\n\n\n6\nTh\n2024/02/15\nMultivariable Models\nLec 7\n\n\n\n\n\n7\nM\n2024/02/19\nLab: Multivariable models\nLab 5\n\n\n\n\n\n7\nTu\n2024/02/20\nModel estimation\n\n\nHW 4 due\n\n\n\n7\nTh\n2024/02/22\nModel estimation\n\n\n\n\n\n\n8\nM\n2024/02/26\nReview\n\n\n\n\n\n\n8\nTu\n2024/02/27\nMidterm Exam\n\n\nExam\n\n\n\n8\nTh\n2024/02/29\nTime to event models\n\n\n\nRMB: 3.5; 7.1; 7.2.1-7.2.7\n\n\n9\nM\n2024/03/04\nLab: Go over midterm exam\n\n\n\n\n\n\n9\nTu\n2024/03/05\nTime to event models\n\n\n\n\n\n\n9\nTh\n2024/03/07\nLab: Time to event\nLab 6\n\nHW 5\n\n\n\n10\nM\n2024/03/11\nSpring Break\n\n\n\n\n\n\n10\nTu\n2024/03/12\nSpring Break\n\n\n\n\n\n\n10\nTh\n2024/03/14\nSpring Break\n\n\n\n\n\n\n11\nM\n2024/03/18\nPoisson Regression\n\n\n\n\n\n\n11\nTu\n2024/03/19\nModeling effect Modification\n\n\n\n\n\n\n11\nTh\n2024/03/21\nModeling effect Modification\n\n\nHW 5 due\n\n\n\n12\nM\n2024/03/25\nNo Lab\n\n\nTake Home Exam\n\n\n\n12\nTu\n2024/03/26\nLab: Poisson Models and Effect Modfiction\n\n\nHW 6\n\n\n\n12\nTh\n2024/03/28\nMultinomial and Ordinal Regression\n\n\n\n\n\n\n13\nM\n2024/04/01\nLab: Discrete Outcome Regression\n\n\n\n\n\n\n13\nTu\n2024/04/02\nFlexible Modeling of Dose Reponse\n\n\n\n\n\n\n13\nTh\n2024/04/04\nANOVA, Model Choice\n\n\nHW 6 due; HW 7\n\n\n\n14\nM\n2024/04/08\nRegression based prediction\n\n\n\n\n\n\n14\nTu\n2024/04/09\nCorrelated Data\n\n\n\n\n\n\n14\nTh\n2024/04/11\nMissing data\n\n\n\n\n\n\n15\nM\n2024/04/15\nBootstrap\n\n\n\n\n\n\n15\nTu\n2024/04/16\nReview\n\n\nHW 7 due\n\n\n\n15\nTh\n2024/04/18\nFinal Exam (in class)\n\n\nExam\n\n\n\n16\nM\n2024/04/22\nGo over final exam"
  },
  {
    "objectID": "notes/Lec02.review.html",
    "href": "notes/Lec02.review.html",
    "title": "Review of Key Concepts",
    "section": "",
    "text": "Code\nlibrary(Exact)"
  },
  {
    "objectID": "notes/Lec02.review.html#samples-from-populations",
    "href": "notes/Lec02.review.html#samples-from-populations",
    "title": "Review of Key Concepts",
    "section": "1 Samples from populations",
    "text": "1 Samples from populations\n\nScientific hypotheses concern a population\n\nDo teenagers learn math more quickly using the Singapore Method?\nWill people with heart disease live longer if they are prescribed medication X?\n\nTeenagers and people with heart disease, are the populations of interest. However, it is rare that we can ever study the entire population\nThe purpose of inferential statistics is to make valid inference about a populations based on a sample from that population\n\nWe commonly make inference about population parameters such as the mean, median, probability, odds, etc."
  },
  {
    "objectID": "notes/Lec02.review.html#inference-and-variability",
    "href": "notes/Lec02.review.html#inference-and-variability",
    "title": "Review of Key Concepts",
    "section": "2 Inference and variability",
    "text": "2 Inference and variability\n\nWith biological questions, there is inevitably variation in the response across repetitions of the experiment\n\nExposure to a carcinogen increases your risk of cancer, but does not guarantee a cancer will develop\nThus, biological questions must be phrased in a probabilistic (not deterministic) language\nDeterministic: Does medication decrease blood pressure?\nProbabilistic: Does medication tend to decrease blood pressure?\n\nThe wording “tends to” is intentionally vague. There are many possible definitions\n\nA lower average value (arithmetic mean)\nA lower geometric mean (arithmetic mean on log scale)\nA lower median\nMedian(Trt) - Median(Ctrl) \\(&lt;\\) 0.0\nMedian(Trt - Ctrl) \\(&lt;\\) 0.0\nA lower proportion exceeding some threshold\nA lower odds of exceeding some threshold\nPr(Trt \\(&gt;\\) Control) \\(&lt;\\) 0.5\nAnd many others...\n\nDefining “tends to” is primarily dictated by scientific considerations\n\nYou, not the data, get to choose which summary measure you care about\nWhich measure is most important to advance science?\n\nIn this course, we will describe the most common models for modeling means (linear regression), odds (logistic regression), hazards (Cox proportional hazards regression), and rates (Poisson or negative binomial regression). Other modeling choices could be used to answer different scientific questions."
  },
  {
    "objectID": "notes/Lec02.review.html#scientific-versus-statistical-questions",
    "href": "notes/Lec02.review.html#scientific-versus-statistical-questions",
    "title": "Review of Key Concepts",
    "section": "3 Scientific versus statistical questions",
    "text": "3 Scientific versus statistical questions\n\nTo formally answer scientific questions, they must be refined into statistical questions\nScientific question: Does aspirin prevent heart attacks?\n\nAn important question, but can’t be addressed by statistics\n\nCause and effect dependent on study design\n\n\nRefinement 1: Do people who take aspirin not have heart attacks?\n\nThis is problematic because it is deterministic. Some people will have heart attacks even if they take aspirin because there is variability across subjects.\n\nRefinement 2: Do people who take aspirin tend to have fewer heart attacks?\n\nThis refinement acknowledges variability in response across subjects, but lacks a control group. We would need to know how many heart attacks they would have otherwise\n\nFinal refinement: Is the incidence of heart attacks less in people who take aspirin than those who do not?\nBasic science: Is the incidence less by any amount?\nClinical science: Is the incidence less by a clinically relevant amount?\n\nNote that we are addressing statistical association, not causation"
  },
  {
    "objectID": "notes/Lec02.review.html#associations-between-variables",
    "href": "notes/Lec02.review.html#associations-between-variables",
    "title": "Review of Key Concepts",
    "section": "4 Associations between variables",
    "text": "4 Associations between variables\n\nAn association exists between two variables if their probability distributions are not independent\nFor random variables \\(X\\) and \\(Y\\) with joint probability density function (pdf) \\(f(x,y)\\), marginal pdfs \\(f_X(x)\\) and \\(f_Y(y)\\), and conditional pdfs \\(f(x | y)\\) and \\(f(y | x)\\)\n\n\\[f(x,y) = f_X(x)*f_Y(y)\\] \\[f(x | y) = f_X(x)\\] \\[f(y | x) = f_Y(y)\\]\n\nIndependence means that there is no way that information about one variable could ever give any information at all about the probability that the other variable might take on a particular value\nAssociation means that the distribution of one variable differs in some way (e.g. mean, median, variance, probability of being greater than 10) across at least two groups differing in their values of the other variable\nCan we ever establish independence between two variables?\n\nYes, but it is very difficult\nTwo variables can be associated in many different ways. It is hard to examine every characteristic of a distribution across groups\n\nConversely, we can show associations as soon as we establish some information that one variable provides about the other\n\nNegative studies (e.g. studies with p \\(&gt;\\) 0.05 or CI that contains the null value) are sometimes misinterpreted as being evidence of no association\n\nHowever, “Absence of evidence is not the same as evidence of absence”\n\nTo make negative studies meaningful, we must...\n\nSpecify the type of association that we are looking for (e.g. mean, median)\nQuantify the amount of uncertainty that might differ across groups\n\nIf the uncertainty is small enough, we can conclude there is no evidence for a meaningful difference\n\n\n\n\n\n4.1 Example: Inference about an association between exposure (E) and disease (D)\n\n5 Exposed (E+) and 5 Unexposed (E-) subjects were followed for one year, and the number of subjects with and without disease are summarized in the following table\n\n\n\nCode\ntab1 &lt;- matrix(c(3,0,2,5),nrow=2, dimnames=list(c(\"E+\",\"E-\"),c(\"D+\",\"D-\")))\ntab1\n\n\n   D+ D-\nE+  3  2\nE-  0  5\n\n\n\nThe number of E+ and E- were fixed by design (5 each). The random variables are the number of subjects (out of 5) who are D+.\nThe scientific question of interest there an association between exposure and increased (or decreased) risk of disease?\nOur analysis plan should take into account the fact that we have a small sample size. For illustration, I will consider both a Bayesian and frequentist approach so the methods can be compared.\n\n\n4.1.1 Frequentist analysis\n\nResults from the unconditional exact test are provided below. The binomial model assumes that the row or columns margins are fixed, which corresponds to the study design. 1\n\n1 A multinomial model would assume the total sample size is fixed, which is a common design but not appropriate here. Fisher’s exact test assumes that both the row and columns margins are fixed, which is rarely found in practice.\n\nCode\n# For comparing proportions\nexact.test(tab1, model=\"binomial\", conf.int = TRUE, to.plot=FALSE)\n\n\n\n    Z-pooled Exact Test\n\ndata:  3 out of 5 vs. 0 out of 5\ntest statistic = 2.0702, first sample size = 5, second sample size = 5,\np-value = 0.06185\nalternative hypothesis: true difference in proportion is not equal to 0\n95 percent confidence interval:\n -4.551214e-05  9.235547e-01\nsample estimates:\ndifference in proportion \n                     0.6 \n\n\nCode\n# For confidnece intervals in each group\nbinom.test(x=3,n=5)\n\n\n\n    Exact binomial test\n\ndata:  3 and 5\nnumber of successes = 3, number of trials = 5, p-value = 1\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.1466328 0.9472550\nsample estimates:\nprobability of success \n                   0.6 \n\n\nCode\nbinom.test(x=0,n=5)\n\n\n\n    Exact binomial test\n\ndata:  0 and 5\nnumber of successes = 0, number of trials = 5, p-value = 0.0625\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.0000000 0.5218238\nsample estimates:\nprobability of success \n                     0 \n\n\n\nHow would you summarize the results of the study examining the association between E and D? Critique the following\n\nAnswer 1: Since the p-value is greater than 0.05, we conclude that there is no association between exposure E and disease D.\nAnswer 2: Since the p-value is greater than 0.05, we lack evidence to conclude that there is an association between exposure E and disease D.\nAnswer 3: We observed incidence rates of 60% in the exposed (95% CI: [15%, 95%]) and 0% in the unexposed (95% CI: [0%, 52%]). The precision of the study was not adequate to demonstrate that such a large difference in incidence rates would be unlikely in the absence of a true association. We estimated a 60% difference in the probability of disease between exposure group (95% CI: [0% to 92%].\n\n\n\n\n4.1.2 Bayesian approach to 2x2 table analysis\n\nA Bayesian analysis requires specifying appropriate prior distributions for our parameter. Let \\(\\theta_1 = \\textrm{Pr(D+|E+)}\\), \\(\\theta_2 = \\textrm{Pr(D+|E-)}\\), and \\(\\theta = \\theta_1 - \\theta_2\\).\nAssume these prior distribution\n\n\\[\\pi(\\theta_1) \\sim Beta(a_1,b_1)\\] \\[\\pi(\\theta_2) \\sim Beta(a_2,b_2)\\]\n\nWe also have to specify the likelihood the number of subjects (out of 5) who are diseased in the Exposed group and the number of subjects who are diseased in the Unexposed group. Note that this is the same binomial likelihood that is assumed in the frequentist approach above.\n\n\\[X_1 \\sim Bin(5, \\theta_1)\\] \\[X_2 \\sim Bin(5, \\theta_2)\\]\n\nUsing Bayes Rule, the posterior distribution is proportional to the likelihood multiplied by the prior distribution\n\n\\[\n\\begin{split}\np(\\theta_1 | X_1) &\\propto p(X_1|\\theta_1) \\times \\pi(\\theta_1) \\\\\n&\\vdots& \\\\\np(\\theta_1 | X_1) &\\sim Beta(X_1+a_1, (5-X_1) + b_1)\n\\end{split}\n\\]\nand\n\\[\n\\begin{split}\np(\\theta_2 | X_2) & \\propto p(X_2|\\theta_2) \\times \\pi(\\theta_2) \\\\\n& \\vdots \\\\\np(\\theta_2 | X_2) & \\sim Beta(X_2+a_2, (5-X_2) + b_2)\n\\end{split}\n\\]\n\nFor the prior distributions, we choose \\(a_1 = a_2 = 0.5\\) and \\(b_1 = b_2 = 2\\). That is, the prior parameters are the same for \\(\\theta_1\\) and \\(\\theta_2\\) with prior mean 0.20.2\nThe prior and posterior distributions for \\(\\theta_1\\) and \\(\\theta_2\\) are summarized below\n\nThe distribution for the Exposed is shifted to the right (relative to the prior) because 3/5 subjects were D+\nThe distribution for the Unexposed is shifted to the left (relative to the prior) because 0/5 subjects were D+\n\n\n2 Mean of Beta(a,b) is a/(a+b)\n\nCode\n# Prior parameters\na &lt;- .5\nb &lt;- 2\n\na/(a+b) # Prior mean\n\n\n[1] 0.2\n\n\nCode\nplot(function(x) dbeta(x,a,b), xlab=\"Theta_1 or Theta_2\", ylab=\"Density\")\nplot(function(x) dbeta(x,a+3,b+2), add=TRUE, col='red')\nplot(function(x) dbeta(x,a+0,b+5), add=TRUE, col='blue')\nlegend(\"topright\",inset=.05,c(\"Prior distribution\",\"Posterior for theta_1 (Exposed)\",\"Posterior for theta_2 (Unexposed)\"), col=c(\"Black\",\"Red\",\"Blue\"), lty=1)\n\n\n\n\n\n\nOur scientific question of interest was to compare the disease rates in the exposed versus the unexposed. This was parameterized as a risk difference, \\(\\theta = \\theta_1 - \\theta_2\\). One could derive the distribution for the difference in two beta random variables for the analytical solution. Instead, I will estimate an approximate numerical solution and summarize the posterior distribution of theta using the 2.5th, 50th, 97.5th quantiles and the posterior mean.\n\n\n\nCode\nset.seed(1234)\nbeta.diff.posterior &lt;- rbeta(10000,a+3,b+2) - rbeta(10000,a+0,b+5)\nbeta.diff.prior &lt;- rbeta(10000,a,b) - rbeta(10000,a,b)\nquantile(beta.diff.posterior, c(.025, .5, .975))\n\n\n      2.5%        50%      97.5% \n0.02510584 0.40590062 0.76104128 \n\n\nCode\nmean(beta.diff.posterior)\n\n\n[1] 0.4015628\n\n\nCode\nplot(density(beta.diff.prior), main=\"\", xlab=\"Theta = Theta_1 - Theta_2\")\nlines(density(beta.diff.posterior), col=\"Red\")\nlegend(\"topleft\",inset=0.05, c(\"Prior\",\"Posterior\"), col=c(\"Black\",\"Red\"), lty=1)\n\n\n\n\n\n\nBayesian Interpretation\n\nA priori, we assumed the probability of disease in the exposed and unexposed groups was 20%, as represented by independent Beta(0.5, 2) distributions. We were interested in estimating the difference in the risk of disease, exposed versus unexposed. Given the data, the posterior mean probability of disease was 40% higher in the exposed group with a 95% credible interval from 2.5% to 76% higher.\n\nComparison of Bayesian approach to frequentist approach\n\nHere the sample size is small, so the prior distribution has an impact on the findings. If you believe the prior is appropriate, then this impact is appropriate. If you believe the prior is inappropriate, then the results should be calculated under different prior assumptions. It is good practice to consider a range of reasonable prior assumptions (which I have not done here) and evaluate the impact on the findings.\nThe mean estimate of the difference between groups (40% Bayesian, 60% frequentist) differ because of the Bayesian priors used. This is an example of shrinkage– the Bayesian estimate is a weighted average of the the prior (0%, no effect) and the data (60%) while the frequentist estimate puts all of the weight on the observed data (60%).\nThe Bayesian credible interval is tighter than the Frequentist confidence interval. Again, this is due the prior distribution being included in the Bayesian calculation. If you feel the prior is appropriate, then this narrowing of the credible interval is also appropriate. As sample size increases, the two approaches would become more similar."
  },
  {
    "objectID": "notes/Lec02.review.html#multiple-comparisons",
    "href": "notes/Lec02.review.html#multiple-comparisons",
    "title": "Review of Key Concepts",
    "section": "5 Multiple comparisons",
    "text": "5 Multiple comparisons\n\nWhen you perform many hypothesis tests, your chance of making a type 1 error increases\n\nType 1 error\n\nProbability of rejecting the null hypothesis when the null hypothesis is true\nProbability of declaring a “statistically significant difference” when, in truth, there is no difference\n\n\nConsider the follow hypothetical example\n\nAn investigator decides to examine an association between eating red meat and cancer\nThe investigator collects clinical data on a cohort of individuals who eat red meat and a cohort who does not eat red meat\nIn the analysis, the investigator compares incidence rates between the two groups\nAlso makes comparisons stratified by gender, race, and lifestyle factors\nIn summary, the investigator claims “The research study uncovered a significant association between consuming red meat and the incidence of lung cancer in non-smoking males (p \\(&lt;\\) 0.05)” (No significant associations were found in any other subgroup.)\n\nTwo possible conclusions\n\nConclusion 1: There is an association between consuming red meat and cancer in non-smoking men\nConclusion 2: This finding is a type 1 error\nWhich do you suspect is the truth?\n\nProbability of multiple comparisons\n\nFor a null hypothesis \\(H_0\\) that is true, and a test performed at significance level \\(\\alpha\\)\n\n\n\\[\\textrm{Pr}(\\textrm{reject } H_0 | H_0 \\textrm{ is true}) = \\alpha\\]\n\\[\\textrm{Pr}(\\textrm{do not reject } H_0 | H_0 \\textrm{ is true}) = 1 - \\alpha\\]\nNext suppose that \\(n\\) independent hypothesis tests (\\(H1_0, H2_0, \\ldots, Hn_0\\)) are performed at level \\(\\alpha\\) and all \\(n\\) null hypotheses are true\n\\[\\textrm{Pr}(\\textrm{do not reject } H1_0, H2_0, \\ldots, Hn_0 | \\textrm{ all } Hi_0 \\textrm{ are true}) = (1 - \\alpha)^n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nn\n1\n2\n4\n8\n12\n16\n20\n30\n\n\n\n\n\\((1 - .05)^n\\)\n0.95\n0.90\n0.81\n0.66\n0.54\n0.44\n0.36\n0.22\n\n\n\\((1 - .01)^n\\)\n0.99\n0.98\n0.96\n0.92\n0.89\n0.85\n0.82\n0.74\n\n\n\n\n\nIf 30 independent trials are performed at \\(\\alpha=0.05\\) and all null hypotheses are true, the probability of falsely rejecting at least one null hypothesis is \\(78\\%\\)! We are very likely to make a mistake.\nChoosing a smaller \\(\\alpha = 0.01\\) helps, but the probability of at least one type 1 error is still \\(26\\%\\) for 30 tests"
  },
  {
    "objectID": "notes/Lec02.review.html#regression",
    "href": "notes/Lec02.review.html#regression",
    "title": "Review of Key Concepts",
    "section": "6 Regression",
    "text": "6 Regression\n\nThis semester, I will introduce several regression models\nWhich regression model you choose to use is based on the parameter being compared across groups\n\n\\[\n\\begin{split}\n\\textrm{Means} & \\rightarrow  \\textrm{ Linear regression}\\\\\n\\textrm{Odds} & \\rightarrow  \\textrm{ Logistic regression}\\\\\n\\textrm{Rates} & \\rightarrow  \\textrm{ Poisson regression}\\\\\n\\textrm{Hazards} & \\rightarrow  \\textrm{ Cox (proportional hazards) regression}\\\\\n\\end{split}\n\\]\n\nIn Bios 6311, we discussed how to examine association between an outcome and a predictor of interest (POI)\n\nRegression models generalize two sample tests\n\n\n\\[\n\\begin{split}\n\\textrm{2-sample t-test} & \\rightarrow \\textrm{ Linear regression}\\\\\n\\textrm{Pearson } \\chi^2 \\textrm{ test} & \\rightarrow \\textrm{ Logistic regression}\\\\\n\\textrm{Wilcoxon signed-rank test} & \\rightarrow \\textrm{ Propotional odds regression}\\\\\n\\end{split}\n\\]\n\nWe will learn how to adjust for additional variables using a regression model\n\nConfounders\nEffect modifiers\nPrecision variables\n\nWhen building a multivariable regression model, many possible choices along the way\n\nWhich covariates to include?\nWhat model provides the best fit to the data?\nDoes the model satisfy underlying assumptions?\netc.\n\nTheme of this course\n\nWhen building a multivariable model, data driven decisions lead to multiple comparisons problem.\n\nInstead, I will emphasize\n\nPutting science before statistics. Decide the scientific question first, propose a model that answers that scientific question.\nRegression models that are robust to distributional assumptions (relieving the need for extensive model checking)\nDetailed, pre-specified analysis plans. Lay out the process for conducting your analysis before looking at the data to avoid data-driven decision making."
  },
  {
    "objectID": "notes/Lec.template.html",
    "href": "notes/Lec.template.html",
    "title": "Review of Key Concepts",
    "section": "",
    "text": "Code\nlibrary(rms)\nlibrary(ggplot2)"
  },
  {
    "objectID": "notes/Lec08.Survival.html#acknowledgments",
    "href": "notes/Lec08.Survival.html#acknowledgments",
    "title": "Introduction to Survival Analysis",
    "section": "1 Acknowledgments",
    "text": "1 Acknowledgments\nParts of the introductory material is borrowed from Emily Zabor’s introduction to survival analysis in R."
  },
  {
    "objectID": "notes/Lec08.Survival.html#background",
    "href": "notes/Lec08.Survival.html#background",
    "title": "Introduction to Survival Analysis",
    "section": "2 Background",
    "text": "2 Background\nIn logistic regression, we were interested in studying how risk factors were associated with presence or absence of disease. Sometimes, though, we are interested in how a risk factor or treatment affects time to disease or some other event. Or we may have study dropout, and therefore subjects who we are not sure if they had disease or not. In these cases, logistic regression is not appropriate.\nSurvival analysis is used to analyze data in which the time until the event is of interest. The response is often referred to as a failure time, survival time, or event time.\n\n2.1 Examples\n\nTime from randomization until cardiovascular death after some treatment intervention\nTime from diagnosis until tumor recurrence\nTime from diagnosis until AIDS for HIV patients\nTime from production until a machine part fails\nNote that there needs to be a clearly defined starting and ending point\n\n\n\n2.2 The survival time response\n\nUsually continuous\nMay be incompletely determined for some subjects\n\ni.e. For some subjects we may know that their survival time was at least equal to some time t. Whereas, for other subjects, we will know their exact time of event.\n\nIncompletely observed responses are censored\nTime is always \\(&gt;0\\)\n\n\n\n2.3 Analysis issues\n\nIf there is no censoring, standard regression procedures could be used\nHowever, these may be inadequate because\n\nTime to event is restricted to be positive and has a skewed distribution.\nThe probability of surviving past a certain point in time may be of more interest than the expected time of event. Linear regression models means.\nA multiplicative model for the hazard may be a better fit\nMay desire an interpretation that is similar to a relative risk\n\n\n\n\n2.4 Censoring\nCensoring is present when we have some information about a subject’s event time, but we don’t know the exact event time. For the analysis methods we will discuss to be valid, censoring mechanism must be independent of the survival mechanism.\nThere are generally three reasons why censoring might occur:\n\nA subject does not experience the event before the study ends\nA person is lost to follow-up during the study period\nA person withdraws from the study\n\nThese are all examples of right-censoring.\n\n\n\nTrial Anatomy\n\n\n\nRICH JT, NEELY JG, PANIELLO RC, VOELKER CCJ, NUSSENBAUM B, WANG EW. A PRACTICAL GUIDE TO UNDERSTANDING KAPLAN-MEIER CURVES. Otolaryngology head and neck surgery: official journal of American Academy of Otolaryngology Head and Neck Surgery. 2010;143(3):331-336. doi:10.1016/j.otohns.2010.05.007.\n\nTo illustrate the impact of censoring, suppose we have the following data:\n\n\n\n\n\nHow would we compute the proportion who are event-free at 10 years?\n\nSubjects 6 and 7 were event-free at 10 years.\nSubjects 2, 9, and 10 had the event before 10 years.\nSubjects 1, 3, 4, 5, and 8 were censored before 10 years, so we don’t know whether they had the event or not at 10 years. But we know something about them - that they were each followed for a certain amount of time without the event of interest prior to being censored.\n\nSurvival analysis techniques provide a way to appropriately account for censored patients in the analysis.\n\n\n2.5 Types of right-censoring\n\nFixed type I censoring occurs when a study is designed to end after C years of follow-up. In this case, everyone who does not have an event observed during the course of the study is censored at C years.\nIn random type I censoring, the study is designed to end after C years, but censored subjects do not all have the same censoring time. This is the main type of right-censoring we will be concerned with.\nIn type II censoring, a study ends when there is a pre- specified number of events.\n\nRegardless of the type of censoring, we must assume that it is non-informative about the event; that is, the censoring is caused by something other than the impending failure.\n\n\n2.6 Terminology and notation\n\n\\(T\\) denotes the response variable, \\(T &gt; 0\\)\nThe survival function is\n\n\\[\nS(t) = \\textrm{Pr}(T &gt; t) = 1− F(t)\n\\]\n\n\nCode\nplot(function(t) exp(-2*t), 0, 2, ylab=\"Survival\", xlab=\"Time (years)\")\n\n\n\n\n\nExample survival function. Survival is 1 at time 0 and decreases over time.\n\n\n\n\n\nAll subjects are at risk (and event free) at time 0. Thus, the survival estimate at 0 is always 1\nSurvival decreases over time. If we observe for a long enough period of time, survival eventually decreases to 0.\nThe survival function gives the probability that a subject will survive past time t.\nSpecifically, as t ranges from 0 to \\(\\infty\\), the survival function has the following properties\n\nIt is non-increasing\nAt time t = 0, S(t) = 1. In other words, the probability of surviving past time 0 is 1.\nAt time \\(t = \\infty\\), \\(S(t) = S(\\infty) = 0\\). As time goes to infinity, the survival curve goes to 0.\n\nIn theory, the survival function is smooth. In practice, we observe events on a discrete time scale (days, weeks, etc.)\n\n\n2.6.1 Hazard, cumulative hazard, and survival\n\nThe hazard function, h(t), is the instantaneous rate at which events occur, given no previous events. \\[\nh(t) = \\lim_{{\\Delta t} \\to 0} \\frac{P(t \\leq T &lt; t + \\Delta t | T \\geq t)}{\\Delta t}\n\\]\n\\(T\\) is a random variable denoting the time until the event (for example, failure or death) occurs. The numerator in the fraction is the conditional probability that the event occurs in the small time interval \\([t, t + Δt]\\) given that it has not occurred before time \\(t\\). The denominator \\(\\Delta t\\) is the length of this time interval. As \\(\\Delta t\\) goes to 0, this ratio gives the instantaneous rate of occurrence of the event at time \\(t\\), which is the hazard function \\(h(t)\\).\nThe cumulative hazard\n\n\\[\nH(t) = \\int_{0}^{t} h(u) du\n\\]\n\nHere, h(u) is the hazard function and u is the variable of integration. This integral represents the accumulated risk of the event (for example, failure or death) over the time interval \\([0, t]\\).\nThe relationship between the survival function \\(S(t)\\) and the cumulative hazard function \\(H(t)\\)\n\n\\[\nS(t) = \\exp(-H(t))\n\\]\n\nKey point: Survival function, cumulative hazard function, and hazard function are interconnected. If we know any one of the functions \\(S(t)\\), \\(H(t)\\), or \\(h(t)\\), we can derive the other two functions.\n\n\n\n2.6.2 Outcome definition\n\nTo analyze survival data, we need to know the observed time \\(Y_i\\) and the event indicator \\(\\delta_i\\). For subject \\(i\\):\nObserved time \\(Y_i = \\min(T_i, C_i)\\) where \\(T_i\\) = event time and \\(C_i\\) = censoring time\nEvent indicator \\(\\delta_i\\) = 1 if event observed (i.e. \\(T_i \\leq C_i\\)), = 0 if censored (i.e. \\(T_i &gt; C_i\\))"
  },
  {
    "objectID": "notes/Lec08.Survival.html#non-parametric-estimation-of-the-survival-curve",
    "href": "notes/Lec08.Survival.html#non-parametric-estimation-of-the-survival-curve",
    "title": "Introduction to Survival Analysis",
    "section": "3 Non-parametric estimation of the survival curve",
    "text": "3 Non-parametric estimation of the survival curve\n\nIn theory the survival function is smooth; in practice we observe events on a discrete time scale.\nThe survival probability at a certain time, \\(S(t)\\), is a conditional probability of surviving beyond that time, given that an individual has survived just prior to that time. The survival probability can be estimated as the number of patients who are alive without loss to follow-up at that time, divided by the number of patients who were alive just prior to that time.\nThe Kaplan-Meier estimate of survival probability at a given time is the product of these conditional probabilities up until that given time\n\n\\[\n\\hat{S}(t) = \\prod_{i: t_i \\leq t} (1 - \\frac{d_i}{n_i})\n\\]\n\n\\(\\hat{S}(t)\\) is the estimated survival function at time (t),\n\\(t_i\\) are the observed event times,\n\\(d_i\\) is the number of events (e.g. deaths) at time (\\(t_i\\)),\n\\(n_i\\) is the number of individuals known to have survived (are at risk) up to time (\\(t_i\\)).\n\n\n3.1 Example: The lung dataset\nThe lung dataset is available from the {survival} package. The data contain subjects with advanced lung cancer from the North Central Cancer Treatment Group. We will focus on the following variables throughout this tutorial:\n\ntime: Observed survival time in days\nstatus: censoring status 1=censored, 2=dead\nsex: 1=Male, 2=Female\n\nNote that the status is coded in a non-standard way in this dataset. Typically you will see 1=event, 0=censored. Let’s recode it to avoid confusion:\n\n\nCode\nlung &lt;- \n  lung %&gt;% \n  mutate(\n    status = recode(status, `1` = 0, `2` = 1)\n  )\n\n\nNow we have:\n\ntime: Observed survival time in days\nstatus: censoring status 0=censored, 1=dead\nsex: 1=Male, 2=Female\n\nHere are the first 6 observations:\n\n\nCode\nhead(lung[, c(\"time\", \"status\", \"sex\")])\n\n\n  time status sex\n1  306      1   1\n2  455      1   1\n3 1010      0   1\n4  210      1   1\n5  883      1   1\n6 1022      0   1\n\n\nNote: the Surv() function in the {survival} package accepts by default TRUE/FALSE, where TRUE is event and FALSE is censored; 1/0 where 1 is event and 0 is censored; or 2/1 where 2 is event and 1 is censored. Please take care to ensure the event indicator is properly formatted.\n\n\n3.2 Creating survival objects and curves\nThe Kaplan-Meier method is the most common way to estimate survival times and probabilities. It is a non-parametric approach that results in a step function, where there is a step down each time an event occurs.\nThe Surv() function from the {survival} package creates a survival object for use as the response in a model formula. There will be one entry for each subject that is the survival time, which is followed by a + if the subject was censored. Let’s look at the first 10 observations:\n\n\nCode\nSurv(lung$time, lung$status)[1:10]\n\n\n [1]  306   455  1010+  210   883  1022+  310   361   218   166 \n\n\nWe see that subject 1 had an event at time 306 days, subject 2 had an event at time 455 days, subject 3 was censored at time 1010 days, etc.\nThe survfit() function creates survival curves using the Kaplan-Meier method based on a formula. Let’s generate the overall survival curve for the entire cohort, assign it to object s1, and look at the structure using str():\n\n\nCode\ns1 &lt;- survfit(Surv(time, status) ~ 1, data = lung)\nstr(s1)\n\n\nList of 16\n $ n        : int 228\n $ time     : num [1:186] 5 11 12 13 15 26 30 31 53 54 ...\n $ n.risk   : num [1:186] 228 227 224 223 221 220 219 218 217 215 ...\n $ n.event  : num [1:186] 1 3 1 2 1 1 1 1 2 1 ...\n $ n.censor : num [1:186] 0 0 0 0 0 0 0 0 0 0 ...\n $ surv     : num [1:186] 0.996 0.982 0.978 0.969 0.965 ...\n $ std.err  : num [1:186] 0.0044 0.00885 0.00992 0.01179 0.01263 ...\n $ cumhaz   : num [1:186] 0.00439 0.0176 0.02207 0.03103 0.03556 ...\n $ std.chaz : num [1:186] 0.00439 0.0088 0.00987 0.01173 0.01257 ...\n $ type     : chr \"right\"\n $ logse    : logi TRUE\n $ conf.int : num 0.95\n $ conf.type: chr \"log\"\n $ lower    : num [1:186] 0.987 0.966 0.959 0.947 0.941 ...\n $ upper    : num [1:186] 1 1 0.997 0.992 0.989 ...\n $ call     : language survfit(formula = Surv(time, status) ~ 1, data = lung)\n - attr(*, \"class\")= chr \"survfit\"\n\n\nSome key components of this survfit object that will be used to create survival curves include:\n\ntime: the timepoints at which the curve has a step, i.e. at least one event occurred\nsurv: the estimate of survival at the corresponding time\n\n\n\n3.3 Kaplan-Meier estimates\n\n\n\nCode\nsummary(s1)\n\n\nCall: survfit(formula = Surv(time, status) ~ 1, data = lung)\n\n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    5    228       1   0.9956 0.00438       0.9871        1.000\n   11    227       3   0.9825 0.00869       0.9656        1.000\n   12    224       1   0.9781 0.00970       0.9592        0.997\n   13    223       2   0.9693 0.01142       0.9472        0.992\n   15    221       1   0.9649 0.01219       0.9413        0.989\n   26    220       1   0.9605 0.01290       0.9356        0.986\n   30    219       1   0.9561 0.01356       0.9299        0.983\n   31    218       1   0.9518 0.01419       0.9243        0.980\n   53    217       2   0.9430 0.01536       0.9134        0.974\n   54    215       1   0.9386 0.01590       0.9079        0.970\n   59    214       1   0.9342 0.01642       0.9026        0.967\n   60    213       2   0.9254 0.01740       0.8920        0.960\n   61    211       1   0.9211 0.01786       0.8867        0.957\n   62    210       1   0.9167 0.01830       0.8815        0.953\n   65    209       2   0.9079 0.01915       0.8711        0.946\n   71    207       1   0.9035 0.01955       0.8660        0.943\n   79    206       1   0.8991 0.01995       0.8609        0.939\n   81    205       2   0.8904 0.02069       0.8507        0.932\n   88    203       2   0.8816 0.02140       0.8406        0.925\n   92    201       1   0.8772 0.02174       0.8356        0.921\n   93    199       1   0.8728 0.02207       0.8306        0.917\n   95    198       2   0.8640 0.02271       0.8206        0.910\n  105    196       1   0.8596 0.02302       0.8156        0.906\n  107    194       2   0.8507 0.02362       0.8056        0.898\n  110    192       1   0.8463 0.02391       0.8007        0.894\n  116    191       1   0.8418 0.02419       0.7957        0.891\n  118    190       1   0.8374 0.02446       0.7908        0.887\n  122    189       1   0.8330 0.02473       0.7859        0.883\n  131    188       1   0.8285 0.02500       0.7810        0.879\n  132    187       2   0.8197 0.02550       0.7712        0.871\n  135    185       1   0.8153 0.02575       0.7663        0.867\n  142    184       1   0.8108 0.02598       0.7615        0.863\n  144    183       1   0.8064 0.02622       0.7566        0.859\n  145    182       2   0.7975 0.02667       0.7469        0.852\n  147    180       1   0.7931 0.02688       0.7421        0.848\n  153    179       1   0.7887 0.02710       0.7373        0.844\n  156    178       2   0.7798 0.02751       0.7277        0.836\n  163    176       3   0.7665 0.02809       0.7134        0.824\n  166    173       2   0.7577 0.02845       0.7039        0.816\n  167    171       1   0.7532 0.02863       0.6991        0.811\n  170    170       1   0.7488 0.02880       0.6944        0.807\n  175    167       1   0.7443 0.02898       0.6896        0.803\n  176    165       1   0.7398 0.02915       0.6848        0.799\n  177    164       1   0.7353 0.02932       0.6800        0.795\n  179    162       2   0.7262 0.02965       0.6704        0.787\n  180    160       1   0.7217 0.02981       0.6655        0.783\n  181    159       2   0.7126 0.03012       0.6559        0.774\n  182    157       1   0.7081 0.03027       0.6511        0.770\n  183    156       1   0.7035 0.03041       0.6464        0.766\n  186    154       1   0.6989 0.03056       0.6416        0.761\n  189    152       1   0.6943 0.03070       0.6367        0.757\n  194    149       1   0.6897 0.03085       0.6318        0.753\n  197    147       1   0.6850 0.03099       0.6269        0.749\n  199    145       1   0.6803 0.03113       0.6219        0.744\n  201    144       2   0.6708 0.03141       0.6120        0.735\n  202    142       1   0.6661 0.03154       0.6071        0.731\n  207    139       1   0.6613 0.03168       0.6020        0.726\n  208    138       1   0.6565 0.03181       0.5970        0.722\n  210    137       1   0.6517 0.03194       0.5920        0.717\n  212    135       1   0.6469 0.03206       0.5870        0.713\n  218    134       1   0.6421 0.03218       0.5820        0.708\n  222    132       1   0.6372 0.03231       0.5769        0.704\n  223    130       1   0.6323 0.03243       0.5718        0.699\n  226    126       1   0.6273 0.03256       0.5666        0.694\n  229    125       1   0.6223 0.03268       0.5614        0.690\n  230    124       1   0.6172 0.03280       0.5562        0.685\n  239    121       2   0.6070 0.03304       0.5456        0.675\n  245    117       1   0.6019 0.03316       0.5402        0.670\n  246    116       1   0.5967 0.03328       0.5349        0.666\n  267    112       1   0.5913 0.03341       0.5294        0.661\n  268    111       1   0.5860 0.03353       0.5239        0.656\n  269    110       1   0.5807 0.03364       0.5184        0.651\n  270    108       1   0.5753 0.03376       0.5128        0.645\n  283    104       1   0.5698 0.03388       0.5071        0.640\n  284    103       1   0.5642 0.03400       0.5014        0.635\n  285    101       2   0.5531 0.03424       0.4899        0.624\n  286     99       1   0.5475 0.03434       0.4841        0.619\n  288     98       1   0.5419 0.03444       0.4784        0.614\n  291     97       1   0.5363 0.03454       0.4727        0.608\n  293     94       1   0.5306 0.03464       0.4669        0.603\n  301     91       1   0.5248 0.03475       0.4609        0.597\n  303     89       1   0.5189 0.03485       0.4549        0.592\n  305     87       1   0.5129 0.03496       0.4488        0.586\n  306     86       1   0.5070 0.03506       0.4427        0.581\n  310     85       2   0.4950 0.03523       0.4306        0.569\n  320     82       1   0.4890 0.03532       0.4244        0.563\n  329     81       1   0.4830 0.03539       0.4183        0.558\n  337     79       1   0.4768 0.03547       0.4121        0.552\n  340     78       1   0.4707 0.03554       0.4060        0.546\n  345     77       1   0.4646 0.03560       0.3998        0.540\n  348     76       1   0.4585 0.03565       0.3937        0.534\n  350     75       1   0.4524 0.03569       0.3876        0.528\n  351     74       1   0.4463 0.03573       0.3815        0.522\n  353     73       2   0.4340 0.03578       0.3693        0.510\n  361     70       1   0.4278 0.03581       0.3631        0.504\n  363     69       2   0.4154 0.03583       0.3508        0.492\n  364     67       1   0.4092 0.03582       0.3447        0.486\n  371     65       2   0.3966 0.03581       0.3323        0.473\n  387     60       1   0.3900 0.03582       0.3258        0.467\n  390     59       1   0.3834 0.03582       0.3193        0.460\n  394     58       1   0.3768 0.03580       0.3128        0.454\n  426     55       1   0.3700 0.03580       0.3060        0.447\n  428     54       1   0.3631 0.03579       0.2993        0.440\n  429     53       1   0.3563 0.03576       0.2926        0.434\n  433     52       1   0.3494 0.03573       0.2860        0.427\n  442     51       1   0.3426 0.03568       0.2793        0.420\n  444     50       1   0.3357 0.03561       0.2727        0.413\n  450     48       1   0.3287 0.03555       0.2659        0.406\n  455     47       1   0.3217 0.03548       0.2592        0.399\n  457     46       1   0.3147 0.03539       0.2525        0.392\n  460     44       1   0.3076 0.03530       0.2456        0.385\n  473     43       1   0.3004 0.03520       0.2388        0.378\n  477     42       1   0.2933 0.03508       0.2320        0.371\n  519     39       1   0.2857 0.03498       0.2248        0.363\n  520     38       1   0.2782 0.03485       0.2177        0.356\n  524     37       2   0.2632 0.03455       0.2035        0.340\n  533     34       1   0.2554 0.03439       0.1962        0.333\n  550     32       1   0.2475 0.03423       0.1887        0.325\n  558     30       1   0.2392 0.03407       0.1810        0.316\n  567     28       1   0.2307 0.03391       0.1729        0.308\n  574     27       1   0.2221 0.03371       0.1650        0.299\n  583     26       1   0.2136 0.03348       0.1571        0.290\n  613     24       1   0.2047 0.03325       0.1489        0.281\n  624     23       1   0.1958 0.03297       0.1407        0.272\n  641     22       1   0.1869 0.03265       0.1327        0.263\n  643     21       1   0.1780 0.03229       0.1247        0.254\n  654     20       1   0.1691 0.03188       0.1169        0.245\n  655     19       1   0.1602 0.03142       0.1091        0.235\n  687     18       1   0.1513 0.03090       0.1014        0.226\n  689     17       1   0.1424 0.03034       0.0938        0.216\n  705     16       1   0.1335 0.02972       0.0863        0.207\n  707     15       1   0.1246 0.02904       0.0789        0.197\n  728     14       1   0.1157 0.02830       0.0716        0.187\n  731     13       1   0.1068 0.02749       0.0645        0.177\n  735     12       1   0.0979 0.02660       0.0575        0.167\n  765     10       1   0.0881 0.02568       0.0498        0.156\n  791      9       1   0.0783 0.02462       0.0423        0.145\n  814      7       1   0.0671 0.02351       0.0338        0.133\n  883      4       1   0.0503 0.02285       0.0207        0.123\n\n\n\n\nApplying the formula\n\n\\[\n\\hat{S}(t) = \\prod_{i: t_i \\leq t} (1 - \\frac{d_i}{n_i})\n\\]\n\n\\(S(0) = 1\\)\n\\(S(5) = 1*(1-1/228) = 0.9956\\)\n\\(S(11) = 1*(1-1/228)*(1-3/227) = 0.9825\\)\netc.\nNumber at risk is decrease by events or censored observations\nMost early times are events, so number at risk is decreasing because of events. – The first censored observation is at 92 days. Here, the risk set decreases by the number of events and number censored (find this time point in the table to verify yourself)\n\n\n\n3.4 Kaplan-Meier plot\n\n\nCode\nggsurvplot(s1, data=lung, risk.table = TRUE)\n\n\n\n\n\n\nPlot gives estimate of survival over time\nConfidence interval is conditional on a given survival time (rather than being a confidence band for the entire survival curve)\nRisk table gives the number of subjects at risk at specified time points\n\n\n\n3.5 Estimating \\(x\\)-year survival\nOne quantity often of interest in a survival analysis is the probability of surviving beyond a certain number of years, \\(x\\).\nFor example, to estimate the probability of surviving to \\(1\\) year, use summary with the times argument (Note: the time variable in the lung data is actually in days, so we need to use times = 365.25)\n\n\nCode\nsummary(survfit(Surv(time, status) ~ 1, data = lung), times = 365.25)\n\n\nCall: survfit(formula = Surv(time, status) ~ 1, data = lung)\n\n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n  365     65     121    0.409  0.0358        0.345        0.486\n\n\nWe find that the \\(1\\)-year probability of survival in this study is 41%.\nThe associated lower and upper bounds of the 95% confidence interval are also displayed.\nThe \\(1\\)-year survival probability is the point on the y-axis that corresponds to \\(1\\) year on the x-axis for the survival curve.\n\n\n\n\n\nWhat happens if you use a “naive” estimate? Here “naive” means that the patients who were censored prior to 1-year are considered event-free and included in the denominator.\n121 of the 228 patients in the lung data died by \\(1\\) year so the “naive” estimate is calculated as:\n\\[\\Big(1 - \\frac{121}{228}\\Big) \\times 100 = 47\\%\\] You get an incorrect estimate of the \\(1\\)-year probability of survival when you ignore the fact that 42 patients were censored before \\(1\\) year.\nRecall the correct estimate of the \\(1\\)-year probability of survival, accounting for censoring using the Kaplan-Meier method, was 41%.\nIgnoring censoring leads to an overestimate of the overall survival probability. Imagine two studies, each with 228 subjects. There are 165 deaths in each study. Censoring is ignored in one (blue line), censoring is accounted for in the other (yellow line). The censored subjects only contribute information for a portion of the follow-up time, and then fall out of the risk set, thus pulling down the cumulative probability of survival. Ignoring censoring erroneously treats patients who are censored as part of the risk set for the entire follow-up period.\n\n\n\n\n\n\n\n3.6 Estimating median survival time\nAnother quantity often of interest in a survival analysis is the average survival time, which we quantify using the median. Survival times are not expected to be normally distributed so the mean is not an appropriate summary.\nWe can obtain the median survival directly from the survfit object:\n\n\nCode\nsurvfit(Surv(time, status) ~ 1, data = lung)\n\n\nCall: survfit(formula = Surv(time, status) ~ 1, data = lung)\n\n       n events median 0.95LCL 0.95UCL\n[1,] 228    165    310     285     363\n\n\nWe see the median survival time is 310 days The lower and upper bounds of the 95% confidence interval are also displayed.\nMedian survival is the time corresponding to a survival probability of \\(0.5\\):\n\n\n\n\n\n\n\n3.7 Non-parametric test for comparing survival times between groups\n\nThe most common test for comparing survival by groups is the log-rank test\n\nAt each event time, constructs a 2x2 table of observed events and number at risk for each group\nCompare observed to expected number of events under the null hypothesis that the event rates are the same in each group\nA Chi-squared test stratifying over all event times\nA test of the entire survival curve\n\nWe get the log-rank p-value using the survdiff function. For example, we can test whether there was a difference in survival time according to sex in the lung data:\n\n\n\nCode\nsurvdiff(Surv(time, status) ~ sex, data = lung)\n\n\nCall:\nsurvdiff(formula = Surv(time, status) ~ sex, data = lung)\n\n        N Observed Expected (O-E)^2/E (O-E)^2/V\nsex=1 138      112     91.6      4.55      10.3\nsex=2  90       53     73.4      5.68      10.3\n\n Chisq= 10.3  on 1 degrees of freedom, p= 0.001 \n\n\n\nWe can also add the p-value to Kaplan-Meier plot by sex\n\n\n\nCode\nfit.sex &lt;- survfit(Surv(time, status) ~ sex, data=lung)\nggsurvplot(fit.sex, data=lung, pval=TRUE)\n\n\n\n\n\nThere was a significant difference in overall survival according to sex in the lung data, with a p-value of p = ‘round(1-pchisq(survdiff(Surv(time, status) ~ sex, data = lung)$chisq, 1),3)’."
  },
  {
    "objectID": "notes/Lec08.Survival.html#semi-parametric-regression-cox-proportional-hazards-model",
    "href": "notes/Lec08.Survival.html#semi-parametric-regression-cox-proportional-hazards-model",
    "title": "Introduction to Survival Analysis",
    "section": "4 Semi-parametric regression (Cox proportional hazards model)",
    "text": "4 Semi-parametric regression (Cox proportional hazards model)\n\nThe preceding approach was very flexible in that it makes no parametric assumptions above the shape of the survival curve\nThis can work for well for categorical predictors and/or descriptive purposes\nIn a clinical trial where we have randomized to treatment groups (and presumably have minimized confounding), the Kaplan-Meier method and log-rank test can be the primary analysis method\nExtensions are needed if we want to consider continuous predictors and multivariable models\nWould be nice to maintain flexibility and have a readily interpret\nStatistical methods\n\nRegression models we have studied to date are generally not valid for survival data\n\nBecause of right censoring, survival time cannot be analyzed as a continuous outcome (e.g. linear regression)\nBecause of unequal length of followup, survival (yes/no) cannot be analyzed using logistic regression\n\nIn the presence of censored data, the usual descriptive statistics are not appropriate\n\nSample mean, sample median, simple proportions, sample standard deviation should not be used\nProper descriptives should be based on the Kaplan Meier estimates\n\nSpecialized regression models are needed with censored data\n\n(Cox) Proportional hazards model\n\nConsiders the instantaneous risk of failure at each time among those subjects who have not failed\nThe term “proportional hazards” assumes that the ratio of these instantaneous failure rates is constant in time between groups\nProportional hazards (Cox) regression treats the survival distribution within a group semi-parametrically\n\nThe baseline hazard is estimated without making any distributional assumptions\nThe hazard ratio is the parameter of interest and is used to compare groups\n\n\n\n\n4.1 Proportional Hazards Regression\n\nLooks at odds of choosing subjects relative to prevalence in the population\n\nCan be derived as estimating the odds ratio of an event at each time that an event occurs\nProportional hazards model averages the odds ratio across all observed event times\nIf the odds ratio is constant over time between two groups, such an average results in a precise estimate of the hazard ratio\n\nBorrowing information\n\nUses other groups to make estimates in groups with sparse data\nBorrows information across predictor groups\n\nIntuitively, 67 and 69 year olds would provide some relevant information about 68 year olds\n\nAlso borrows information over time\n\nRelative risk for an event at each time is presumed to be the same under proportional hazards\n\n\n\n\n\n4.2 The simple PH regression model\n\nModeling the log hazard over time as a function of covariates \\(X\\)\n“Baseline hazard” is unspecified. Baseline hazard is similar to an intercept\n\n\n\n\n\n\n\nModel\n\\(\\textrm{log}(\\lambda(t | X_i)) = \\textrm{log}(\\lambda_{0}(t)) + \\beta_1 \\times X_i\\)\n\n\n\n\n\\(X_i = 0\\)\nlog hazard at \\(t\\) is \\(\\textrm{log}(\\lambda_{0}(t))\\)\n\n\n\\(X_i = x\\)\nlog hazard at \\(t\\) is \\(\\textrm{log}(\\lambda_{0}(t)) + \\beta_1 \\times x\\)\n\n\n\\(X_i = x+1\\)\nlog hazard at \\(t\\) is \\(\\textrm{log}(\\lambda_{0}(t))+ \\beta_1 \\times x + \\beta_1\\)\n\n\n\nModel on the hazard scale is found by exponentiating parameters\n\n\n\n\n\n\n\nModel\n\\(\\lambda(t | X_i) = \\lambda_{0}(t) \\times e^{\\beta_1 \\times X_i}\\)\n\n\n\n\n\\(X_i = 0\\)\nhazard at \\(t\\) is \\(\\lambda_{0}(t)\\)\n\n\n\\(X_i = x\\)\nhazard at \\(t\\) is \\(\\lambda_{0}(t) \\times e^{\\beta_1 \\times x}\\)\n\n\n\\(X_i = x+1\\)\nhazard at \\(t\\) is \\(\\lambda_{0}(t) \\times e^{\\beta_1 \\times x} \\times e^{\\beta_1}\\)\n\n\n\nInterpretation of the model\n\nNo intercept\n\nGenerally ignore the baseline hazard\n\nSlope parameter\n\nHazard ratio found by exponentiating the slope from the PH regression: \\(\\textrm{exp}(\\beta_1)\\)\nHazard ratio compared groups differing in the value of the predictor by 1 unit\n\n\nRelationship to survival\n\nHazard function determines the survival function\n\n\n\n\n\n\n\nHazard\n\\(\\lambda(t | X_i) = \\lambda_{0}(t) \\times e^{\\beta_1 \\times X_i}\\)\n\n\nCumulative Hazard\n\\(\\Lambda(t | X_i) = \\displaystyle\\int^t_0 \\lambda_{0}(u) \\times e^{\\beta_1 \\times X_i} \\ du\\)\n\n\nSurvival Function\n\\(S(t | X_i) = e^{-\\Lambda(t|X_i)} = [S_0(t)]^{e^{\\beta_1 \\times X_i}}\\)\n\n\n\n\n\n\n\n4.3 Descriptive Plots\n\n4.3.1 Hazard\n\n\nCode\nset.seed(55)\nx &lt;- seq(0,2*pi,by=0.1)\nlambda0 &lt;- (2*sin(x) + 4 + rnorm(length(x), 0, .2)) / 100\nlambda0[1] &lt;- 0\nplot(x, lambda0, type='l', xlab=\"Time\", ylab=\"Hazard\", ylim=c(0,.1), main=\"Baseline Hazard Over Time\", lwd=2)\nlegend(\"top\", inset=0.05, lty=1, expression(lambda[0] (t)), lwd=2)\n\n\n\n\n\n\n\n4.3.2 Cumulative hazard\n\n\nCode\nplot(x, cumsum(lambda0), type='l', xlab=\"Time\", ylab=\"Cumulative Hazard\", ylim=c(0,3), main=\"Baseline Cumulative Hazard over Time\", lwd=2)\nlegend(\"top\", inset=0.05, lty=1, expression(Lambda[0] (t) == integral(lambda[0] (u)*du, 0, t)), lwd=2)\n\n\n\n\n\n\n\n4.3.3 Survival\n\n\nCode\nplot(x, exp(-cumsum(lambda0)), type='l', xlab=\"Time\", ylab=\"Survival\", ylim=c(0,1), main=\"Baseline Survival over Time\", lwd=2)\nlegend(\"top\", inset=0.05, lty=1, expression(exp(-Lambda[0] (t))), lwd=2)\n\n\n\n\n\n\n\n4.3.4 Proportional hazards assumption\n\n\nCode\nlambda1 &lt;- lambda0*1.5\nplot(x, lambda0, type='l', xlab=\"Time\", ylab=\"Hazard\", ylim=c(0,.1), main=\"Proportional Hazards for Two Groups\", lwd=2)\nlines(x, lambda1, col=\"Red\", lwd=2)\nlegend(\"topright\", inset=0.05, lty=1, c(expression(lambda[0] (t)), expression(lambda[1] (t) == 1.5*lambda[0] (t))), lwd=2, col=c(\"Black\",\"Red\"))\n\n\n\n\n\n\n\n4.3.5 Proportional hazards assumption on log scale\n\n\nCode\nplot(x, log(lambda0), type='l', xlab=\"Time\", ylab=\"Log Hazard\", ylim=c(-4.5,-2), main=\"Proportional Hazards for Two Groups, log scale\", lwd=2)\nlines(x, log(lambda1), col=\"Red\", lwd=2)\nlegend(\"topright\", inset=0.05, lty=1, c(expression(log(lambda[0] (t))), expression(log(lambda[1] (t)) == log(1.5) + log(lambda[0] (t)))), lwd=2, col=c(\"Black\",\"Red\"))\n\n\n\n\n\n\nComments on plots\n\nBaseline hazard can follow any functional form. This is the “non-parametric” part of the Cox proportional hazards model\nThe cumulative hazard is a non-decreasing function that starts at 0 at time 0. It is bounded by \\(\\infty\\).\nSurvival is a function of the cumulative hazard. Survival is 1 at time 0 and decreases over time. It is bounded by \\(0\\) and \\(1\\).\nFor a dichotomous predictor variable (X is 0 or 1), the proportional hazards assumption is that \\(\\lambda_1 (t) = e^{\\beta_1} \\lambda_0 (t)\\)\n\nIn the plots I illustrated \\(e^{\\beta_1} = 1.5\\) (\\(\\beta_1 = 0.4054\\)).\nOn the hazard scale, this corresponds to \\(\\lambda_1(t)\\) always being \\(1.5\\) times larger than \\(\\lambda_0(t)\\).\nOn the log hazard scale, this corresponds to \\(log(\\lambda_1(t))\\) always being \\(log(1.5) = 0.4054\\) units larger than \\(log(\\lambda_0(t))\\).\nThe proportional hazards assumption is the “parametric” part of the Cox proportional hazards model. Hence, the Cox proportional hazards model is referred to as being “semi-parametric”.\n\n\n\n\n\n\n4.4 Software\n\nStata commands\n\n“stset time event-indicator”\n“stcox predictor, [robust]”\n\nR functions\n\n“Surv(time, event-indicator)”\n“coxph(Surv(time, event-indicator) ~ predictor)”\nThere is a robust option in coxph for calcluating robust standard error estimates. In my opinion the assumption of proportional hazards in the Cox model rarely (never?) holds completely, so the robust option can be a good default.\nLin, D. Y., and L. J. Wei. “The Robust Inference for the Cox Proportional Hazards Model.” Journal of the American Statistical Association 84, no. 408 (1989): 1074–78. https://doi.org/10.2307/2290085.\n\n\n\n\n4.5 Example Analysis\n\n4.5.1 Descriptives\n\nPrognostic values of nadir PSA relative to time in remission\n\nPSA dataset: 50 men who received hormonal treatment for advanced prostate cancer\nFollowed at least 24 months for clinical progression, but exact followup varies from subject to subject\nNadir PSA: lowest level of serum prostate specific antigen achieved post treatment\n\nScatterplots of censored data are not scientifically meaningful\n\nIt is best to not generate them unless you do something to indicate the censored dataset\nWe can label censored data, but have to remember that the true value may be anywhere larger than that\n\n\n\n\nCode\nlibrary(foreign)\nlibrary(rms)\npsa &lt;- read.dta(file=\"data/psa.dta\")\npsa$nadirpsa &lt;- psa$dirpsa\n\nggplot(psa, aes(y=obstime, x=nadirpsa, color=inrem)) + geom_point() + theme_bw()\n\n\n\n\n\n\nCharacterization of plot\n\nOutliers: Can’t tell\nFirst order trends\n\nDownward trending slop\nNo censoring at high nadir PSAs (high nadir PSA are all early events)\n\nSecond order trend: Must be curvilinear (but how much?)\nVariability within groups: Highest variability within lowest PSA groups\n\n\n\n\n4.5.2 Regression model\n\nThe usual coding of the outcome is survival is 1 for an event and 0 for a censored observation.\n\n\n\nCode\n# First, create an indicator variable for those who relapsed at any time point\npsa$relapse &lt;- as.numeric(psa$inrem==\"no\")\nwith(psa, table(relapse, inrem))\n\n\n       inrem\nrelapse no yes\n      0  0  14\n      1 36   0\n\n\n\nCreate a relapse variable to indicate if a subject has relapsed or not\nDefine the outcome using “Surv(obstime, relapse)”\n\nobstime: Time to event (either time to relapse or time to censored)\nrelapse: Failure indicator variable\n\n\n\n\nCode\nm1 &lt;- coxph(Surv(obstime, relapse) ~ nadirpsa, data=psa, robust=TRUE)\nm1\n\n\nCall:\ncoxph(formula = Surv(obstime, relapse) ~ nadirpsa, data = psa, \n    robust = TRUE)\n\n             coef exp(coef) se(coef) robust se     z        p\nnadirpsa 0.015717  1.015841 0.003750  0.003671 4.281 1.86e-05\n\nLikelihood ratio test=11.78  on 1 df, p=0.0005989\nn= 50, number of events= 36 \n\n\nCode\nsummary(m1)\n\n\nCall:\ncoxph(formula = Surv(obstime, relapse) ~ nadirpsa, data = psa, \n    robust = TRUE)\n\n  n= 50, number of events= 36 \n\n             coef exp(coef) se(coef) robust se     z Pr(&gt;|z|)    \nnadirpsa 0.015717  1.015841 0.003750  0.003671 4.281 1.86e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n         exp(coef) exp(-coef) lower .95 upper .95\nnadirpsa     1.016     0.9844     1.009     1.023\n\nConcordance= 0.757  (se = 0.037 )\nLikelihood ratio test= 11.78  on 1 df,   p=6e-04\nWald test            = 18.33  on 1 df,   p=2e-05\nScore (logrank) test = 24.3  on 1 df,   p=8e-07,   Robust = 6.9  p=0.009\n\n  (Note: the likelihood ratio and score tests assume independence of\n     observations within a cluster, the Wald and robust score tests do not).\n\n\nCode\nexp(confint.default(m1))\n\n\n            2.5 %   97.5 %\nnadirpsa 1.008558 1.023177\n\n\nCode\n# 10-unit change in nadir PSA\nexp(coef(m1)*10)\n\n\nnadirpsa \n1.170192 \n\n\nCode\nexp(10*confint.default(m1))\n\n\n            2.5 %   97.5 %\nnadirpsa 1.088947 1.257498\n\n\n\n\n4.5.3 Interpretation of output\n\nBy default, R gives the hazard ratio and the coefficient for comparing groups who differ by one unit in the nadir psa covariate\n\n\\(\\textrm{Hazard ratio} = e^{\\textrm{coeff}}\\)\n\n\\(\\textrm{Hazard ratio} = 1.015^{\\Delta nadir}\\)\nEstimated hazard ratio for two groups differing by 1 in nadir PSA is found by exponentiating the slope\n\nGroups 1 unit higher nadir PSA have instantaneous event rate \\(1.0157\\) fold higher (1.6% higher)\nGroups 10 units higher nadir PSA have instantaneous event rate \\(1.015^{10} = 1.166\\) fold higher (16.6% higher)\n\n\n\n\n4.5.4 Robust standard errors\nR allows for robust standard errors in the coxph function. Note that other functions for fitting Cox models in R may or may not have this functionality.\n“From proportional hazards regression analysis, we estimate that for each 1 ng/ml unit difference in nadir PSA, this risk of relapse is \\(1.6\\%\\) higher in the group with the higher nadir PSA. This estimate is highly statistically significant (\\(p &lt; 0.001\\)). A 95% CI suggests that this observation is not unusual if a group that has a 1 ng/ml higher nadir PSA might have risk of relapse that was anywhere from \\(0.9\\%\\) higher to \\(2.3\\%\\) higher than the group with lower nadir PSA.”\n\n\n\n4.6 Inference with PH Regression\n\nThe ideas of Signal and Noise found in simple linear regression do not translate well to PH regression\n\nWe do not tend to quantify an error distribution with PH regression\n\nValid statistical inference (CIs, p-values) about associations requires three general assumptions\n\nAssumptions about approximately Normal distributions for the parameter estimates\n\nLarge N\n\nNeed for either robust standard errors or classical regression\nDefinition of large depends on the underlying probability distribution\n\n\nAssumptions about independence of observations\n\nClassical regression: Independence of all observation\nRobust standard errors: Correlated observations within identified clusters\n\nAssumptions about variance of observations within groups\n\nClassical regression: Mean-variance relationship for binary data\n\nProportional hazards considers the hazard of event at every time\nHence in order to satisfy this requirement, need proportional hazards and linearity of predictor\n\nRobust standard errors\n\nAllows unequal variance across groups\nHence, do not need linearity of predictor or proportional hazards\n\n\n\nValid statistical inference (CIs, p-values) about hazard of response in specific groups requires a further assumption\n\nAssumption about the adequacy of the linear model\n\nIf we are trying to borrow information about the log hazards from neighboring groups, and we are assuming a straight line relationship, the straight line needs to be true\nNeeded for either classical or robust standard errors\nNote that we can model transformations of the measured predictor\n\n\nWe rarely make inference about within group survival probabilities using the proportional hazards model\n\nSometimes estimated survival curves are used descriptively\n\nUse estimates of the baseline survival function\nExponentiate the baseline survival to find survival curve for specific covariate patterns\n\n\nRelationship to survival\n\nHazard function determines the survival function.\nFor the Cox model\n\n\n\n\n\n\n\nHazard\n\\(\\lambda(t | X_i) = \\lambda_{0}(t) \\times e^{\\beta_1 \\times X_i}\\)\n\n\nCumulative Hazard\n\\(\\Lambda(t | X_i) = \\displaystyle\\int^t_0 \\lambda_{0}(u) \\times e^{\\beta_1 \\times X_i} \\ du\\)\n\n\nSurvival Function\n\\(S(t | X_i) = e^{-\\Lambda(t|X_i)} = [S_0(t)]^{e^{\\beta_1 \\times X_i}}\\)\n\n\n\n\n\n\n4.6.1 Log-transformed nadir PSA\n\nSuppose that you know based on prior experience or consultation with collaborators…\n\nA constant difference in PSA would not be expected to confer the same increased risk\n\nComparing 4 ng/ml to 10 ng/ml is not the same as comparing 104 ng/ml to 110 ng/ml\n\nA multiplicative effect on risk might be better\n\nSame increase in risk for each doubling of nadir PSA\nAchieve this model by using log transformed nadir PSA\n\n\n\n\n\nCode\npsa$lognadirpsa &lt;- log(psa$nadirpsa)\nm2 &lt;- coxph(Surv(obstime, relapse) ~ lognadirpsa, data=psa, robust=TRUE)\nm2\n\n\nCall:\ncoxph(formula = Surv(obstime, relapse) ~ lognadirpsa, data = psa, \n    robust = TRUE)\n\n               coef exp(coef) se(coef) robust se     z        p\nlognadirpsa 0.43724   1.54843  0.08770   0.07286 6.001 1.96e-09\n\nLikelihood ratio test=24.01  on 1 df, p=9.601e-07\nn= 50, number of events= 36 \n\n\nCode\nsummary(m2)\n\n\nCall:\ncoxph(formula = Surv(obstime, relapse) ~ lognadirpsa, data = psa, \n    robust = TRUE)\n\n  n= 50, number of events= 36 \n\n               coef exp(coef) se(coef) robust se     z Pr(&gt;|z|)    \nlognadirpsa 0.43724   1.54843  0.08770   0.07286 6.001 1.96e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n            exp(coef) exp(-coef) lower .95 upper .95\nlognadirpsa     1.548     0.6458     1.342     1.786\n\nConcordance= 0.757  (se = 0.037 )\nLikelihood ratio test= 24.01  on 1 df,   p=1e-06\nWald test            = 36.01  on 1 df,   p=2e-09\nScore (logrank) test = 29.02  on 1 df,   p=7e-08,   Robust = 19.76  p=9e-06\n\n  (Note: the likelihood ratio and score tests assume independence of\n     observations within a cluster, the Wald and robust score tests do not).\n\n\nCode\nexp(confint.default(m2))\n\n\n               2.5 %   97.5 %\nlognadirpsa 1.342372 1.786127\n\n\n\nHazard ratio is \\(1.55\\) for a \\(e\\)-fold difference in nadir PSA (\\(e = 2.7183\\))\nIt is more easy to understand doubling, tripling, 5-fold, 10-fold increases\n\nFor doubling: \\(\\textrm{HR} = 1.54^{log(2)} = 1.35\\)\nFor 5-fold: \\(\\textrm{HR} = 1.54^{log(5)} = 1.99\\)\nCan similarly transform the upper and lower limits of the confidence interval\n\nThe confidence interval and statistical test given in the output is called a Wald test. Other tests (Score, Likelihood Ratio) are also possible.\n\nAll tests are asymptotically equivalent\nThe Wald test is easiest to obtain, but generally performs the poorest in small sample sizes. It is based on the coefficient estimate and standard error.\nThe Likelihood Ratio test performs the best in small samples. We will discuss it later, including how to obtain the test using post-estimation commands.\nThe Score test is not bad in small samples, but is often hard to obtain from software. It is exactly equal to the logrank test for binary outcomes and categorical predictors.\n\n\n\n\n\n4.7 Review: Interpretation of Slopes\n\n4.7.1 Additive models\n\nIdentity link function\n\n\n\n\n\nParameter\nApproach\n\n\n\n\nMeans\nLinear regression\n\n\n\n\n\nInterpretation of non-transformed slope: \\(\\theta_X = \\beta_0 + \\beta_1 \\times X\\)\n\n\\(\\beta_1\\) : (Average) difference in summary measure between groups per 1 unit difference in \\(X\\)\n\\(\\Delta \\times \\beta_1\\) : (Average) difference in summary measure between groups per \\(\\Delta\\) unit difference in \\(X\\)\n\nInterpretation with log transformed slope: \\(\\theta_X = \\beta_0 + \\beta_1 \\times \\textrm{log}(X)\\)\n\n\\(\\textrm{log}(k) \\times \\beta_1\\) : (Average) difference in summary measure between groups per \\(k\\)-fold difference in \\(X\\)\n\n\n\n\n4.7.2 Multiplicative models\n\nLog link function\n\n\n\n\n\nParameter\nApproach\n\n\n\n\nGeometric means\nLinear regression on log scale\n\n\nOdds\nLogistic regression\n\n\nRates\nPoisson regression\n\n\nHazards\nProportional Hazards (Cox) regression\n\n\n\n\n\nInterpretation of non-transformed slope: \\(\\textrm{log}(\\theta_X) = \\beta_0 + \\beta_1 \\times X\\)\n\n\\(e^{\\beta_1}\\) : (Average) ratio of summary measure between groups per 1 unit difference in \\(X\\)\n\\(e^{\\Delta \\times \\beta_1}\\) : (Average) ratio of summary measure between groups per \\(\\Delta\\) unit difference in \\(X\\)\n\nInterpretation with log transformed slope: \\(\\textrm{log}(\\theta_X) = \\beta_0 + \\beta_1 \\times \\textrm{log}(X)\\)\n\n\\(e^{\\textrm{log}(k) \\times \\beta_1} = k^{\\beta_1}\\) : (Average) ratio of summary measure between groups per \\(k\\)-fold difference in \\(X\\)"
  },
  {
    "objectID": "notes/Lec08.Survival.html#parametric-regression",
    "href": "notes/Lec08.Survival.html#parametric-regression",
    "title": "Introduction to Survival Analysis",
    "section": "5 Parametric regression",
    "text": "5 Parametric regression\n\nFully-parametric models are less common than the semi-parametric Cox model because they are less flexible\nHowever, if they are a good fit to a particular dataset, they may\n\nEstimate fewer parameters\nAllow for better predictions (as long as the error distribution is correctly specified!)\nAllow for comparison of survival estimates rather than hazards (accelerated failure time)\n\nTwo fundamental models used to describe the way that some factor might affect time to event\n\nProportional hazards (aka Cox model)\nAccelerated failure time (less popular)\n\nAccelerated Failure Time model\n\nComparisons across groups based on survival times rather than hazards\nAssume that a factor causes some subjects to spend their lifetime too fast\nBasic idea: For every year in a reference group’s lives, the other group ages “k” years\n\ne.g. 1 human year is about 7 dog years, so a 70 year old human is the same age as a 10 year old dog\nIn AFT terminology, the probability that a human survives beyond 70 years is the same as the probability that a dog survives beyond 10 years\nFrom the framework, dogs are accelerating through life 7 times faster than a human. Equivalently, the lifespan of a human is stretched out to be 7 times longer than that of a dog.\n\nAssumes that ratios of quantiles of survival distribution are constant across groups\n\ne.g. report median ratios: It takes 50% longer for half of the treated group to die than half of the control group\n\nAFT models include the parametric exponential, Weibull, and lognormal models\n\n\n\n5.1 Weibull model\n\nThe Weibull model can also be interpreted as either a proportional hazard or accelerated failure time model. It is one of the few models that makes both the proportional hazards and accelerated failure time assumptions.\nThe Weibull model accommodates monotonically increasing or decreasing hazards\nThe following gives the relationship between the hazard function, Survival function, distribution function and cumulative distribution function for four different shape parameters following a Weibull distribution with scale parameter held constant at 2\n\n\n\nCode\nplot.data &lt;- expand.grid(\n  t = seq(0.1, 10, .1),\n  loc = c(.5, 1, 1.5, 2)\n) %&gt;% \n  mutate(\n    `F (CDF)` = pweibull(t, shape = loc, scale = 2),\n    `f (PDF)` = dweibull(t, shape = loc, scale = 2),\n    S = 1 - `F (CDF)`,\n    `h = f / S` = flexsurv::hweibull(t, shape = loc, scale = 2),\n    loc = paste(\"shape:\", loc)\n)\n\nplot.data &lt;- expand.grid(t=seq(0.1, 10, .1),\n                         loc=c(.5, 1, 1.5, 2))\n\nplot.data2 &lt;- with(plot.data, \n                   data.frame(t=rep(t,4),\n                              loc=rep(loc,4),\n                              value=NA,\n                              type=rep(1:4,each=nrow(plot.data))))\n\nplot.data2$value[plot.data2$type==1] &lt;- \n  pweibull(plot.data2$t[plot.data2$type==1], shape = plot.data2$loc[plot.data2$type==1], scale = 2)\n\nplot.data2$value[plot.data2$type==2] &lt;- \n  dweibull(plot.data2$t[plot.data2$type==2], shape = plot.data2$loc[plot.data2$type==2], scale = 2)\n\nplot.data2$value[plot.data2$type==3] &lt;- \n  1-pweibull(plot.data2$t[plot.data2$type==3], shape = plot.data2$loc[plot.data2$type==3], scale = 2)\n\nplot.data2$value[plot.data2$type==4] &lt;- \n  flexsurv::hweibull(plot.data2$t[plot.data2$type==4], shape = plot.data2$loc[plot.data2$type==4], scale = 2)\n\nplot.data2$type &lt;- factor(plot.data2$type, levels=1:4, labels=c(\"F (CDF)\",\"f (PDF)\", \"S = 1 - F\", \"h = f / S\" ))\n\nplot.data2$loc2 &lt;- paste(\"scale =\", plot.data2$loc)\n\nggplot(plot.data2, aes(x=t, y=value)) + geom_line() + facet_grid(type ~ loc2, scales=\"free_y\") + theme_bw()\n\n\n\n\n\n\nWeibull hazard and cumulative hazard at time \\(t\\) with shape \\(p\\) and scale \\(\\lambda\\) \\((\\lambda &gt; 0)\\)\n\n\\[h(t) = λpt^{(p−1)}\\]\n\n\\(p\\) is a shape parameter\n\n\\(p&gt;1\\) is an increasing hazard over time\n\\(p=1\\) is a constant hazard\n\\(p&lt;1\\) is a decreasing hazard\n\nWe can model the scale as a function of covariates\n\n\\[\\textrm{where } \\lambda = \\textrm{exp}(\\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_p X_p)\\]\n\nA special case of the Weibull model is the Exponential model. The Weibull reduces to the Exponential when the shape parameter is 1 (\\(p=1\\)). A shape parameter of 1 assumes the hazard is constant over time, and the cumulative hazard increases linearly with time.\n\nThis provides a method for checking if the Weibull can be reduced to an Exponential model. If we estimate the cumulative hazard without making parametric assumptions and plot versus time, we can see if it is increasing at a constant rate.\n\nNelson-Aalen estimator of the cumulative hazard (non-parametric)\n\n\\[\\hat{H}(t)=\\sum_{i:t_i\\leq t}\\frac{d_i}{n_i}\\]\n\nwhere \\(d_i\\) is the number of events at time \\(t_i\\), and \\(n_i\\) is the number of individuals at risk just before time \\(t_i\\)\nApply this to the lung cancer dataset\n\n\n\nCode\n# Calculate the non-parametric estimate of the cumulative hazard\nn.risk &lt;- summary(s1)$n.risk\nn.evt &lt;- summary(s1)$n.event\ntime &lt;- summary(s1)$time\n\n# Plot cumulative hazard versus time\nna.haz &lt;- n.risk/n.evt\nna.cumhaz &lt;- cumsum(na.haz)\nplot(time, na.cumhaz)\n\n\n\n\n\n\nCumulative hazard is increasing over time, but not at a constant rate. The Weibull may be appropriate, but the Exponential is not appropriate.\n\n\n\n5.2 Weibull example\n\n5.2.1 Compare fit of Weibull (parametric) to Kaplan-Meier (non-parametric)\n\nIs the Weibull model a reasonable fit to the observed survival estimates?\n\n\n\nCode\nfit2 &lt;- survreg(Surv(time, status) ~ sex, data = lung, dist = \"weibull\")\n\npct &lt;- 1:98/100\nptime1 &lt;- predict(fit2, newdata=data.frame(sex=1), type='quantile',\n                 p=pct)\nptime2 &lt;- predict(fit2, newdata=data.frame(sex=2), type='quantile',\n                 p=pct)\n\n# Kaplan-Meier fit\nplot(fit.sex, col=1:2, ylab=\"Survival\")\n\nlines(ptime1, 1-pct, col=1,lty=2)\nlines(ptime2, 1-pct, col=2,lty=2)\nlegend(\"topright\", col=c(1,1,2,2), lty=c(1,2,1,2), \n       c(\"Kaplan-Meier, Male\",\"Weibull, Male\",\"Kaplan-Meier, Female\",\"Weibull, Female\"))\n\n\n\n\n\n\n\n5.2.2 Interpretation of Weibull parameters\n\nThe Weibull model is both a proportional hazards and accelerated failure time model\nHow the model is parameterized will either give a proportional hazards or accelerated failure time interpretation\n\nFor the Weibull proportional hazards model with one covariate ($X$)\n\\[\nh(t) = \\lambda p t^{p-1}\n\\]\n\\[\n\\lambda = \\textrm{exp}(\\beta_1 X)\n\\]\nFor the Weibull accelerated failure time model with one covariate ($X$)\n\\[\nS(t) = exp(-\\lambda t^p)\n\\]\nSolve for \\(t\\)\n\\[\nt = (-\\textrm{ln} S(t))^{1/p} \\times \\frac{1}{\\lambda^{1/p}}\n\\]\n\\[\n\\frac{1}{\\lambda^{1/p}} = \\textrm{exp}(\\alpha_0 + \\alpha_1 X)\n\\]\n\nR default is to give the proportional hazards parameterization. The hazard ratio is found by exponentiation of the slope as we have done with other models\nCompare to females, males are at a 1.48 fold increased risk (hazard) of death.\n\n\n\nCode\nsummary(fit2)\n\n\n\nCall:\nsurvreg(formula = Surv(time, status) ~ sex, data = lung, dist = \"weibull\")\n              Value Std. Error     z       p\n(Intercept)  5.4886     0.1790 30.66 &lt; 2e-16\nsex          0.3956     0.1276  3.10  0.0019\nLog(scale)  -0.2809     0.0619 -4.54 5.7e-06\n\nScale= 0.755 \n\nWeibull distribution\nLoglik(model)= -1148.7   Loglik(intercept only)= -1153.9\n    Chisq= 10.4 on 1 degrees of freedom, p= 0.0013 \nNumber of Newton-Raphson Iterations: 5 \nn= 228 \n\n\nCode\nexp(coef(fit2))\n\n\n(Intercept)         sex \n 241.914353    1.485242 \n\n\nCode\nexp(confint.default(fit2))\n\n\n                 2.5 %     97.5 %\n(Intercept) 170.322286 343.598925\nsex           1.156491   1.907447\n\n\n\nTo obtain the accelerated failure time interpretation, the Weibull model needs to be reparameterized so the output coefficients can be directly interpreted. We also have to exponeniate the coefficient estimates as we are comparing fold differences in survival quantiles.\n\n\n\nCode\nm.aft &lt;- aftreg(formula = Surv(time, status) ~ sex, data = lung,\n                         dist = \"weibull\")\nm.aft\n\n\nCall:\naftreg(formula = Surv(time, status) ~ sex, data = lung, dist = \"weibull\")\n\nCovariate          W.mean      Coef Time-Accn  se(Coef)    Wald p\nsex                 1.438    -0.395     0.673     0.128     0.002 \n\nBaseline parameters:\nlog(scale)                    5.489               0.179     0.000 \nlog(shape)                    0.281               0.062     0.000 \nBaseline life expectancy:  223 \n\nEvents                    165 \nTotal time at risk         69593 \nMax. log. likelihood      -1148.7 \nLR test statistic         10.4 \nDegrees of freedom        1 \nOverall p-value           0.00126068\n\n\nCode\nexp(confint(m.aft))\n\n\n                2.5 %      97.5 %\nsex          0.524437   0.8648488\nlog(scale) 170.416568 343.7261363\nlog(shape)   1.173370   1.4956268\n\n\n\nAcceleration factor (males compared to females) is 0.673\n\nComparison of survival times, with males having shorter survival times at all percentiles\n\n\n\n\nCode\n# 25th, 50th, and 75th percentiles of survival time in males\nptime1[pct%in%c(\"0.25\",\"0.5\",\"0.75\")]\n\n\n[1] 140.2460 272.4383 459.8036\n\n\nCode\n# 25th, 50th, and 75th percentiles of survival time in females\nptime2[pct%in%c(\"0.25\",\"0.5\",\"0.75\")]\n\n\n[1] 208.2993 404.6370 682.9198\n\n\nCode\n# Ratios is constant (accelerated failure-time assumption)\nptime1[pct%in%c(\"0.25\",\"0.5\",\"0.75\")] / ptime2[pct%in%c(\"0.25\",\"0.5\",\"0.75\")]\n\n\n[1] 0.6732908 0.6732908 0.6732908\n\n\n\nSummary interpretation of both PH and AFT\n\nFrom the Weibull regression model, we can estimate both the hazard ratio and acceleration factor.\nComparing males to females, males have a 1.48 fold increased hazard (risk) of death compared to females. We are 95% confident the true hazard ratio is between a 1.16 and 1.91 fold increase. Since the hazard ratio does not contain 1, males are at a significantly increased hazard of death compared to females.\nComparing males to females, males have a median (or any other quantile) survival times that is 0.673 times the median (or other quantile) survival time in females. We are 95% confident that the true acceleration factor is between 0.52 and 0.85.\nThe statements are more awkward when the acceleration factor is less than 1. We could flip the estimates (e.g \\(0.673^{-1}\\)) and say that the median (or other quantile) survival time in females is 1.49 times longer than it is in males to be less awkward.\n\nNote that a hazard ratio greater than one is harmful for survival while an acceleration factor greater than one is beneficial for survival. One is the null values for both interpretations.\n\n\n\n\n5.3 Other parametric models\n\nExample shown here focused on a Weibull model as an introduction. We identified the Exponential distribution (a special case of the Weibull) is not a good fit.\nOther (parametric) distributions are possible. Some have a proportional hazards interpretation (only) or an accelerated failure time interpretation (only). Other lack either of these interpretations, but are more flexible in how they model the hazard function. There is a trade-off between interpretability and flexibility.\nGoal is to pick a distribution with a corresponding hazard (cumulative hazard) that is appropriate model for your data, will be reproducible, and is ultimately interpretable.\nCox model is a good choice if you want to model associations and not concern yourself with the shape of the hazard function. This is a very common situation and why the Cox model is widely used.\nIf goal is prediction, appropriate parametric models (perhaps with additional flexibility) may outperform the Cox model\nBayesian approach to fitting survival models are most often based on parametric approaches\n\nIt is relatively straight forward to fit Bayesian models using parametric distributions like Weibull\nModels that attempt to recreate a Cox-like models are more complex because of the flexibility of the baseline hazard function"
  },
  {
    "objectID": "notes/Lec01.Introduction.html#course-team",
    "href": "notes/Lec01.Introduction.html#course-team",
    "title": "Welcome & Syllabus",
    "section": "Course Team",
    "text": "Course Team\nInstructor\n\nChris Slaughter\n\njames.c.slaughter@vumc.org (best)\njames.c.slaughter@vanderbilt.edu (checked ~ weekly)\nOffice: 2525 West End, 11-127\n\n\n\nTA\n\nBailu (Lucy) Yan\n\nbailu.yan@vanderbilt.edu"
  },
  {
    "objectID": "notes/Lec01.Introduction.html#course-websites",
    "href": "notes/Lec01.Introduction.html#course-websites",
    "title": "Welcome & Syllabus",
    "section": "Course website(s)",
    "text": "Course website(s)\n\nhttps://chrissl789.github.io/modern-regression/\n\nCourse notes\n\nHTML, PDF, Quarto (.qmd)\n\nDaily schedule\nLinks to course tools\n\nBrightspace\n\nAnnouncements\nSubmit homework\nGradebook\nFERPA compliance\n\nThe Family Educational Rights and Privacy Act (FERPA) is a federal law enacted in 1974 that protects the privacy of student education records."
  },
  {
    "objectID": "notes/Lec01.Introduction.html#course-timetable",
    "href": "notes/Lec01.Introduction.html#course-timetable",
    "title": "Welcome & Syllabus",
    "section": "Course Timetable",
    "text": "Course Timetable\n\nLectures (weekly)\n\nTuesdays, 10:30 - 11:50 am - 2525 WEA, Room 11105\nThursdays, 10:30 - 11:50 am - 2525 WEA, Room 11105\n\n\n\n\nLabs (weekly)\n\nMondays, 1:00 to 1:50 pm - 2525 WEA, Room 11105"
  },
  {
    "objectID": "notes/Lec01.Introduction.html#software-r",
    "href": "notes/Lec01.Introduction.html#software-r",
    "title": "Welcome & Syllabus",
    "section": "Software – R",
    "text": "Software – R\n\nPrimary software package will be R\n\nFree\nMost adaptable statistical package\nUsed by the majority of academic Biostatisticians\nRstudio IDE used to work with R\nRstudio includes Quarto, the latest and greatest tool for rendering markdown documents"
  },
  {
    "objectID": "notes/Lec01.Introduction.html#software-rstudio-and-quarto",
    "href": "notes/Lec01.Introduction.html#software-rstudio-and-quarto",
    "title": "Welcome & Syllabus",
    "section": "Software – Rstudio and Quarto",
    "text": "Software – Rstudio and Quarto\n\nCourse notes rendered using Quarto with R code integrated\n\nQuarto allows for creating dynamic content using embedded R (or other software) code\nWill allow you to see all code used to generate notes\n\nI will highlight important R code\n\nI am still learning Quarto, so it has more features than I will likely use\n\nQuarto introduction covered in first lab"
  },
  {
    "objectID": "notes/Lec01.Introduction.html#software-stata",
    "href": "notes/Lec01.Introduction.html#software-stata",
    "title": "Welcome & Syllabus",
    "section": "Software – Stata",
    "text": "Software – Stata\n\nSecondary software package will be Stata\n\nMy second favorite statistical package\nRelatively easy syntax\nDesigned for people who know statistics, but don’t want to write basic functions\nNot integrated with Quarto\n\nI will provide a list of equivalent Stata commands in labs and lectures, but usually not produce the output"
  },
  {
    "objectID": "notes/Lec01.Introduction.html#generative-ai-e.g.-chatgpt",
    "href": "notes/Lec01.Introduction.html#generative-ai-e.g.-chatgpt",
    "title": "Welcome & Syllabus",
    "section": "Generative AI (e.g. ChatGPT)",
    "text": "Generative AI (e.g. ChatGPT)\n\nTwo definitions of generative AI, from ChatGPT 3.5\n\nGenerative AI is a type of artificial intelligence that can create new, original content by learning patterns from existing data and using them to generate new outputs.\nGenerative AI refers to the use of machine learning algorithms to generate new and original content, such as images, text, and music.\n\nI prefer to use the tools available to VUMC\nAs with any tool, use with caution\n\nHelp with generating code, or converting code between languages\nHelp with writing interpretations in more easily understood language\nThe prompt that you use matters. Also, identical prompts at different times can give different answers"
  },
  {
    "objectID": "notes/Lec01.Introduction.html#guiding-principles",
    "href": "notes/Lec01.Introduction.html#guiding-principles",
    "title": "Welcome & Syllabus",
    "section": "Guiding Principles",
    "text": "Guiding Principles\n\nAlternative course title: How to Use Statistics to Answer Scientific Questions (part 2)\n\nPut science before statistics\nEmphasize parameter estimates and confidence intervals (credible intervals, likelihood intervals) over hypothesis testing and p-values\n\nThe End of Statistical Significance (Jonathan Sterne)\nWhat’s Wrong with P-Values (Bland, Altman)\nKey difference between scientific/clinical significance and statistical significance\n\n\nThis is a course in Biostatistics, not coding in R/Stata\n\nI will show you how to get an interpret the key statistics, but not interpret every number on output\nOften there is more than one way to arrive at identical final answers"
  },
  {
    "objectID": "notes/Lec01.Introduction.html#example-clinical-vs-scientific-significance",
    "href": "notes/Lec01.Introduction.html#example-clinical-vs-scientific-significance",
    "title": "Welcome & Syllabus",
    "section": "Example: Clinical vs Scientific significance",
    "text": "Example: Clinical vs Scientific significance\n\n5 clinical trials conducted to determine if drug A, B, or C lowers cholesterol\nAssume that a decrease of 10 mg/dl or more is important to clinicians\nStudy design\n\nCholesterol measured at baseline, subjects take drug for 1 month, Cholesterol measured at 1 month\nChange in cholesterol is the outcome of interest\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrial\nDrug\nPts\nMean diff\nStd dev\nStd error\n95% CI for diff\np-value\n\n\n\n\n1\nA\n30\n-30\n191.7\n49.5\n[-129, 69]\n0.55\n\n\n2\nA\n1000\n-30\n223.6\n10\n[-49.6, -10.4]\n0.002\n\n\n3\nB\n40\n-20\n147.6\n33\n[-85, 45]\n0.55\n\n\n4\nB\n4000\n-2\n147.6\n3.3\n[-8.5, 4.5]\n0.54\n\n\n5\nC\n5000\n-6\n100.0\n2\n[-9.9, -2.1]\n0.002"
  },
  {
    "objectID": "notes/Lec01.Introduction.html#evaluation-components-and-grade-percentages",
    "href": "notes/Lec01.Introduction.html#evaluation-components-and-grade-percentages",
    "title": "Welcome & Syllabus",
    "section": "Evaluation components and grade percentages",
    "text": "Evaluation components and grade percentages\n\nMidterm (25%)\nTake Home Exam (25%)\nFinal Exam (25%)\nHomework (25%)\nClass participation\nThis is a 4-credit course. Your lab and lecture grades will be the same"
  },
  {
    "objectID": "notes/Lec01.Introduction.html#homework",
    "href": "notes/Lec01.Introduction.html#homework",
    "title": "Welcome & Syllabus",
    "section": "Homework",
    "text": "Homework\n\nUp to 1 per week (probably 6 or 7 total)\nWill focus on real data analysis and interpretation with some mathematical derivations of important quantities\nQuestions will focus on specific analyses, with questions stated in as scientific terms as possible\nWork handed in should address the scientific questions\n\nFormat Table and Figures\n\nKeys will be provided shortly after the homework is turned in\n\nNo late homework accepted after the key is posted\n\nAnswers in keys may go beyond what is expected of your homework and present concepts in more detail. You are responsible for any material in the keys for exams.\nYou may discuss the homework with others in the class, but the work you turn in should be your own\nUse Brightspace to turn in homeworks and receive feedback and grade"
  },
  {
    "objectID": "notes/Lec01.Introduction.html#in-class-exams",
    "href": "notes/Lec01.Introduction.html#in-class-exams",
    "title": "Welcome & Syllabus",
    "section": "In Class Exams",
    "text": "In Class Exams\n\nMidterm and Final in class\n\nFocus on understanding concepts, not memorizing formulas\nI will provide an example midterm and final\nFor midterm, you will be allowed 1 page of your own notes\nFor final, you will be allowed 2 pages of your own notes\n\nAll output will be provide for you to interpret"
  },
  {
    "objectID": "notes/Lec01.Introduction.html#take-home-exam",
    "href": "notes/Lec01.Introduction.html#take-home-exam",
    "title": "Welcome & Syllabus",
    "section": "Take Home Exam",
    "text": "Take Home Exam\n\nWill be given approximately mid point between Midterm and Final\nDemonstrate ability to obtain results through software and interpret findings\nOne day to complete and turn in\n\nLikely will be a Monday with no lab scheduled for that day\n\nPractice for applied portion of first year comprehensive exams\nSimilar to Homework, but work should be your own"
  },
  {
    "objectID": "notes/Lec01.Introduction.html#course-notes",
    "href": "notes/Lec01.Introduction.html#course-notes",
    "title": "Welcome & Syllabus",
    "section": "Course notes",
    "text": "Course notes\n\nCourse notes will be the primary source\nAvailable on web page\nDaily class schedule will indicate notes being covered\nNotes will be updated throughout semester"
  },
  {
    "objectID": "notes/Lec01.Introduction.html#recommended-textbook-regression-methods-in-biostatistics",
    "href": "notes/Lec01.Introduction.html#recommended-textbook-regression-methods-in-biostatistics",
    "title": "Welcome & Syllabus",
    "section": "Recommended Textbook: Regression Methods in Biostatistics",
    "text": "Recommended Textbook: Regression Methods in Biostatistics\n\nVittinghoff, Glidden, Shiboski, and McCulloch\nPros\n\nCovers applied linear, logistic, survival, and repeated measures regression\nBiomedical orientation\n\nCons\n\nDoes not discuss robust standard errors, which I will use almost exclusively\nEmphasizes model diagnostics more that I will\nDoes not cover Bayesian approaches\nDoes not cover matrix algebra\nLittle theory and mathematical derivation"
  },
  {
    "objectID": "notes/Lec01.Introduction.html#additional-textbooks",
    "href": "notes/Lec01.Introduction.html#additional-textbooks",
    "title": "Welcome & Syllabus",
    "section": "Additional textbooks",
    "text": "Additional textbooks\n\nWeisberg, Applied Linear Regression\n\nUses matrix notation and appropriate level of statistical theory for this course\nPoor for applied data analysis. Earlier version limited to linear models, no Bayesian approaches. Newer version is more comprehensive.\n\nWakefield, Bayesian and Frequentist Regression Methods\n\nNewer text that I am still evaluating\nProvides a Bayesian approach to fitting the models we discuss\n\nI will give recommended readings throughout the semester\nIf the course notes and book differ, go with the notes"
  },
  {
    "objectID": "notes/Lec01.Introduction.html#supplemental-material",
    "href": "notes/Lec01.Introduction.html#supplemental-material",
    "title": "Welcome & Syllabus",
    "section": "Supplemental Material",
    "text": "Supplemental Material\n\nAs needed, I will post supplemental material on the course web page or on Brightspace\n\nI prefer the course web page, if possible\nBrightspace will be used if necessary\n\nRelevant supplemental material will be noted on course schedule"
  },
  {
    "objectID": "notes/Lec01.Introduction.html#getting-started",
    "href": "notes/Lec01.Introduction.html#getting-started",
    "title": "Welcome & Syllabus",
    "section": "Getting started",
    "text": "Getting started\n\nTo do on your own by Thursday…\n\nDownload and install new versions of R and Rstudio\n\nDetails on course web page, computing\n\n\nI would like everyone (including Stata users) to be able to Render a basic Quarto document\nPlan to start discussing Quarto next Tuesday\n\nIt is difficult to anticipate all issues that might arise, so I would like to resolve any major software issues before then\n\n\n\n\nBios 6312 - Spring 2024"
  },
  {
    "objectID": "notes/Lec06.ConfoundingEffectModificationMediation.html",
    "href": "notes/Lec06.ConfoundingEffectModificationMediation.html",
    "title": "Confouding, Effect Modification, and Mediation",
    "section": "",
    "text": "Code\nlibrary(ggplot2)"
  },
  {
    "objectID": "notes/Lec06.ConfoundingEffectModificationMediation.html#overview",
    "href": "notes/Lec06.ConfoundingEffectModificationMediation.html#overview",
    "title": "Confouding, Effect Modification, and Mediation",
    "section": "1 Overview",
    "text": "1 Overview\n\nMost often scientific questions are translated into comparing the distribution of some response variable across groups of interest\nGroups are defined by the predictor of interest (POI). Regression methods allow for either categorical or continuous predictors of interest\n\nCategorical predictors of interest: Treatment or control, knockout or\nwild type, ethnic group\nContinuous predictors of interest: Age, BMI, cholesterol, blood pressure\n\nOften we need to consider additional variables other than the predictor of interest to answer our scientific question\nCovariates other than the predictor of of interest are included in the regression model as\n\nEffect modifiers\nConfounders\nMediating variables\nPrecision variables\n(Not necessarily mutually exclusive)"
  },
  {
    "objectID": "notes/Lec06.ConfoundingEffectModificationMediation.html#effect-modification",
    "href": "notes/Lec06.ConfoundingEffectModificationMediation.html#effect-modification",
    "title": "Confouding, Effect Modification, and Mediation",
    "section": "2 Effect Modification",
    "text": "2 Effect Modification\n\nThe association between the Response and the Predictor of Interest differs in strata defined by the effect modifier\nStatistical term: “Interaction” between the effect modifier and the POI\n\n\n2.1 Effect modification depends on the measure of effect that you choose\n\nChoice of summary measure: mean, median, geometric mean, odds, hazard\nChoice of comparisons across groups: differences, ratios\n\n\n\n2.2 Examples of Effect Modification\n\n2.2.1 Example 1: Is serum LDL by smoking modified by gender?\n\n\n\n\nMean\nMean\nMedian\nMedian\n\n\n\nWomen\nMen\nWomen\nMen\n\n\nNo Smoke\n120\n122\n120\n115\n\n\nSmoke\n133\n122\n133\n124\n\n\nDiff\n-13\n0\n-13\n-9\n\n\nRatio\n0.90\n1\n0.90\n0.93\n\n\n\n\nEffect modification by gender exists if the effect of smoking in males is different from the effect of smoking in females\n\nCompare the diff (or ratio) for the mean (or median) in women to the diff (or ratio) for the mean (or median) in men\n\nEffect modification for mean, not as much for median\n\nStatement holds for both difference or ratio of mean/median\n\n\n\n\n2.2.2 Example 2: Creatinine by stroke (modified by gender?)\n\n\n\n\nMean\nMean\nMedian\nMedian\n\n\n\nWomen\nMen\nWomen\nMen\n\n\nNo Stroke\n0.72\n1.08\n0.7\n1.1\n\n\nStroke\n1.01\n1.51\n1.0\n1.5\n\n\nDiff\n-0.29\n-0.43\n-0.3\n-0.4\n\n\nRatio\n0.71\n0.72\n0.70\n0.73\n\n\n\n\nEffect modification for difference, not really for ratio\n\nTrue for Mean or median\n\n\n\n\n2.2.3 Example 3: Stroke by smoking (modified by gender?)\n\n\n\n\nProportion\nProportion\nOdds\nOdds\n\n\n\nWomen\nMen\nWomen\nMen\n\n\nNo Smoke\n0.10\n0.16\n0.03\n0.19\n\n\nSmoke\n0.16\n0.26\n0.19\n0.35\n\n\nDiff\n-0.06\n-0.10\n-0.16\n-0.16\n\n\nRatio\n0.62\n0.62\n0.16\n0.54\n\n\n\n\nProportion: No effect modification for ratio, small amount for difference\nOdds: No effect modification for difference, yes for ratio\n\n\n\n2.2.4 Example 4: Stroke by smoking (modified by CVD?)\n\n\n\n\nProportion\nProportion\nOdds\nOdds\n\n\n\nNone\nCVD\nNone\nCVD\n\n\nNo Smoke\n0.02\n0.33\n0.02\n0.50\n\n\nSmoke\n0.04\n0.50\n0.04\n1.00\n\n\nDiff\n-0.02\n-0.17\n-0.02\n-0.50\n\n\nRatio\n0.50\n0.67\n0.50\n0.50\n\n\n\n\nEffect Modficiation?\n\nProportion: Yes for ratio, yes for difference\nOdds: Yes for difference, no for ratio\n\n\n\n\n2.2.5 Example 5: CHD by current smoking (modified by gender?)\n\n\n\n\nProportion\nProportion\nOdds\nOdds\n\n\n\nWomen\nMen\nWomen\nMen\n\n\nNo Smoke\n0.18\n0.26\n0.22\n0.36\n\n\nSmoke\n0.05\n0.24\n0.05\n0.32\n\n\nDiff\n0.13\n0.02\n0.17\n0.03\n\n\nRatio\n3.60\n1.08\n4.17\n1.11\n\n\n\n\nEffect Modfication?\n\nProportion: Yes for ratio, yes for difference\nOdds: Yes for difference, yes for ratio\n\n\n\n\n2.2.6 Example 6: CHD by ever smoke (modified by gender?)\n\n\n\n\nProportion\nProportion\nOdds\nOdds\n\n\n\nWomen\nMen\nWomen\nMen\n\n\nNever Smoke\n0.16\n0.25\n0.19\n0.33\n\n\nEver Smoke\n0.16\n0.26\n0.19\n0.35\n\n\nDiff\n0.00\n-0.01\n0.00\n-0.02\n\n\nRatio\n1.00\n0.96\n1.00\n0.95\n\n\n\n\nEffect Modfication?\n\nProportion: No for ratio, no for difference\nOdds: No for difference, no for ratio\n\n\n\n\n2.2.7 Summary comments on examples\n\nIf there is an effect, will see effect modification on at least one of the difference and ratio scale\nIf there is no effect (example 6), will see no effect modification on both difference and ratio scale\nIn real world, will usually see effect modification on both scales. The real question is “Is the effect modification scientifically meaningful?”\n\nIf we find there is important effect modification, science will go forward estimating effects separately\nModels with interaction terms are useful for testing if effect modification is present (statistically)\n\n\n\n\nAside: Be careful when comparing two ratios\n\nHow close are two ratios?\n\n0.20 and 0.25 VERSUS 5.0 and 4.0?\n0.10 and 0.15 VERSUS 10.0 and 6.7?\n\nCompare the ratio of ratios, not the difference\nWe might consider ratios to be more different when both ratios are \\(&gt;1\\) than when both are \\(&lt;1\\). But, that would be wrong.\n\n\n\n\n\n2.3 Analysis of Effect Modification\n\nWhen the scientific question involves effect modification\n\nConduct analysis within each stratum separately\nIf we want to estimate the degree of effect modification or test its existence, use a regression model including\n\nPredictor of interest (main effect)\nEffect modifying variable (main effect)\nA covariate modeling the interaction (usually a product)\n\n\n2.3.1 Impact of ignoring effect modification\n\nBy design or mistake, we sometimes do not model effect modification\nMight perform\n\nUnadjusted analysis: POI only\nAdjusted analysis: POI and third variable, but no interaction term\n\nIf effect modification exists, an unadjusted analysis will give different results according to the association between the POI and effect modifier in the sample\n\nIf the POI and the effect modifier are not associated\n\nUnadjusted analysis tends toward an (approximate) weighted average of the stratum specific effects\n\nWith means, exactly a weighted average\nWith odds and hazards, an approximate weighted average (because they are non-linear functions of the mean)\n\n\nIf the POI and the effect modifier are associated in the sample\n\nThe “average” effect is confounded and thus unreliable (variables can be both effect modifiers and confounders)\n\nIf effect modification exists, an analysis adjusting only for the third variable (but no interaction) will tend toward a weighted average of the stratum specific effects\n\nHence, an association in one stratum and not the other will make an adjusted analysis look like an association (provide the sample size is large enough)"
  },
  {
    "objectID": "notes/Lec06.ConfoundingEffectModificationMediation.html#confounding",
    "href": "notes/Lec06.ConfoundingEffectModificationMediation.html#confounding",
    "title": "Confouding, Effect Modification, and Mediation",
    "section": "3 Confounding",
    "text": "3 Confounding\n\n3.1 Simpson’s Paradox\n\nConfounding has its roots in Simpson’s Paradox\nGiven binary variables \\(Y\\) (response), \\(X\\) (POI), and \\(Z\\) (strata) it is possible to have …\n\n\\[\\textrm{Pr}(Y=1 | X=1, Z=1) &gt; \\textrm{Pr}(Y=1 | X=0, Z=1)\\] \\[\\textrm{Pr}(Y=1 | X=1, Z=0) &gt; \\textrm{Pr}(Y=1 | X=0, Z=0)\\]\n… but to also have …\n\\[\\textrm{Pr}(Y=1 | X=1) &lt; \\textrm{Pr}(Y=1 | X=0)\\]\n\n3.1.1 Example: Probability of death (Y) at two hospitals (X) stratified by poor patient condition (Z)\n\nQuestion: Which hospital do you want to be treated at?\n\nConsider the results overall (averaging over Z) and conditional on Z below\n\n\n\n\n\nOverall\nDied\nSurvived\nDeath Rate\n\n\n\n\nHospital A\n16\n784\n2.0%\n\n\nHospital B\n63\n2037\n3.0%\n\n\n\n\n\n\nGood Condition\nDied\nSurvived\nDeath Rate\n\n\n\n\nHospital A\n8\n592\n1.3%\n\n\nHospital B\n6\n594\n1.0%\n\n\n\n\n\n\nPoor Condition\nDied\nSurvived\nDeath Rate\n\n\n\n\nHospital A\n8\n192\n4.0%\n\n\nHospital B\n57\n1443\n3.8%\n\n\n\n\nIgnoring condition, Hospital B has a higher death rather. However, within both poor and good condition, Hospital B has a lower death rate.\n\nPoor condition is a confounder. Hospital B has more subjects with poor condition and subjects with poor condition have a higher death rate.\n\n\n\n\n\n3.2 Definition of Confounding\n\nThe association between a predictor of interest and the response is confounded by a third variable if\n\nThe third variable is associated with the predictor of interest in the sample, AND\nThe third variable is associated with the response\n\nCausally (in truth)\nIn groups that are homogeneous with respect to the predictor of interest\nNot in the causal pathway of interest\n\n\nWe must consider our belief about the causal relationships among the measured variables\n\nThere is no statistical test for causality\nInference about causation comes only from the study design\nBUT, consideration of the causal relationships helps us to decide which statistical questions to answer\n\nClassic confounder\n\nA clear case of confounding occurs when some third variable is a “cause” of both the POI and response\nWe generally adjust for such a confounder\n\n\n\n3.2.1 Directed Acyclyic Graph\n\n\n\n\nflowchart LR\n  X[Predictor\\nof interest] -- Causal? --&gt; Y[Outcome]\n  X[Predictor\\nof interest] &lt;-- Association --&gt; Z[Confounder]\n  Z[Confounder] -- Causal --&gt; Y[Outcome]\n\n\n\n\n\n\nExample: Ice cream (POI), murder rate (outcome), and temperature (confounder) in New York City during the summer\n\n\n\n\n\nflowchart LR\n  X[Ice Cream] -- Causal? --&gt; Y[Murder Rate]\n  X[Ice Cream] &lt;-- Association --&gt; Z[Air temperature]\n  Z[Air temperature] -- Causal --&gt; Y[Murder Rate]\n\n\n\n\n\n\n\n3.2.2 Causal pathways\n\nA variable in the causal pathway of interest\n\nNot a confounder, so we would not adjust for such a variable\nIf we did adjust, we would lose ability to detect associations between the POI and the outcome\nExample: Second hand smoke (POI), stunted growth (confounder), FEV1 (outcome)\n\nScientific question is about the impact of smoking on lung function\n\nStunted growth addresses lung anatomy, not lung function, which we don’t care about it\n\n\n\n\n\n\n\nflowchart LR\n  X[Second hand smoke] --&gt; Y[FEV1]\n  X[Second hand smoke] --&gt; Z[Stuntend growth]\n  Z[Stunted growth] --&gt; Y[FEV1]\n\n\n\n\n\n\n\nA variable in the causal pathway not of interest\n\nHowever, we want to adjust for a variable in a causal pathway that is not of interest\n\nExample: Work stress causing ulcers by hormonal effects versus alcoholism\nDirected Acyclyic Graph\n\nWe can adjust for alcoholism to estimate the path through horomonal effects\nAlternatively, we can adjust for hormonal effects to estimate the effect through alcoholism\n\n\n\n\n\n\n\nflowchart LR\n  X[Work stress] --&gt; W[Hormonal Effects]\n  X[Work Stress] --&gt; Z[Alcholism]\n  W[Hormonal Effects] --&gt; Y[Ulcers]\n  Z[Alcholism] --&gt; Y[Ulcers]\n\n\n\n\n\n\n\nSurrogate for response\n\nAdjustment for a surrogate is a bad idea\nAs the name implies, surrogates are a substitute for the response variable\nDirected Acyclyic Graph where forced vital capacity (FVC) is a surrogate for forced exp\n\n\n\n\n\nflowchart LR\n  X[Second hand smoke] --&gt; Z[FVC]\n  Z[FVC] --&gt; Y[FEV1]\n\n\n\n\n\n\n\nMany other (complicated) patterns possible\n\nGreenland, Pearl, and Robins. Causal Diagrams for Epidemiologic Research. Epidemiology. (1999) http://www.jstor.org/stable/3702180\nhttps://www.dagitty.net/\n\n\n\n\n\n3.3 Diagnosing Confounding\n\nConfounding typically produces a difference between unadjusted and adjusted analyses\n\nThis symptom is not proof of confounding\n\nSuch a difference can occur when there is no confounding\nSymptom is more indicative of confounding when modeling means (linear regression) than when modeling odds (logistic regression) or hazards (Cox, proportional hazards regression)\n\n\nEstimates of association from unadjusted analysis are markedly different from estimates of association from adjusted analysis\n\nAssociation within each stratum is similar to each other, but different from the association in the combined data\n\nIn linear regression, differences between adjusted and unadjusted analyses are diagnostic of confounding\n\nPrecision variables tend to change standard errors, but not slope estimates\nEffect modification would show differences between adjusted analysis and unadjusted analysis, but would also show different associations in the strata\n\nMore difficult to diagnosis confounding with non-linear functions of the mean\n\nCommon non-linear functions: Odds (odds ratios), hazards (hazard ratios)\nMay show the symptoms of confounding when confounding is not present\nAdjusting for precision variables can appear to be confounding\nIn logistic and PH regression, difference between adjusted and unadjusted analyses are more difficult to judge\n\nComparison in more homogeneous groups (i.e. after adjustment for a precision variable) will drive slope estimates away from the null\n\n\nExample: Suppose you have a sample where 50% of the subjects die\n\nWhat is the variability? \\(p*(1-p)=0.25\\)\nWe can reduce this variability by changing \\(p\\), the probability of death\nEstimate \\(p\\) in different stratum. One stratum may have a higher \\(p\\), another a lower \\(p\\).\n\ne.g. Stratum 1, 20% death rate and Stratum 2 an 80% death rate.\n\nBy making the estimate more precise, we have also impacted the mean"
  },
  {
    "objectID": "notes/Lec06.ConfoundingEffectModificationMediation.html#mediating-variables",
    "href": "notes/Lec06.ConfoundingEffectModificationMediation.html#mediating-variables",
    "title": "Confouding, Effect Modification, and Mediation",
    "section": "4 Mediating variables",
    "text": "4 Mediating variables\n\nA mediating variable is a predictor hypothesized to lie on the causal pathway between a predictor of interest and the outcome\nWhether or not to adjust for a mediating variable depends on the scientific question of interest\n\nWithout adjustment, we are estimating the total (causal) effect of the predictor of interest via all pathways on the outcome\nIf we adjust for a mediator, we are estimating the direct (causal) effect via other pathways that do not involve the mediator\n\nIf a potential mediator is identified a priori on scientific grounds, we can estimate the direct effect via pathways other than the mediator, the indirect effect through the mediator, and the degree of mediation (e.g. proportion mediated).\nExample: Depression (predictor) and Crohn’s Disease activitity (outcome) with possible mediators being physical activity, smoking, and sleep quality\n\nMediation models were considered using baseline, 6-month, and 12-month follow-up data\nDepression at baseline was the predictor of interest, mediators were assessed at 6 months, and disease activity at 12 months was the outcome.\nTo establish and estimate the mediation effect, we utilized the four steps originally outlined by Barron and Kenny 1\n\nFirst, we tested that there was a significant effect of depression at baseline with disease activity at 12 months to establish there was an effect to be mediated\nSecond, we estimated the correlation between depression at baseline and each of the potential mediators measured at 6 months\nThird, we tested if each of the mediators at 6 months was associated with disease activity at 12 months while controlling for disease activity at baseline.\nFourth, the mediation package in R was used to estimate various quantities for causal mediation analysis, including average causal mediation effects (indirect effect), average direct effects, proportions mediated, and total effect. This step involves combining estimates from the mediator model (mediator at 6 months outcome, depression at baseline the predictor) and estimates from the outcome model (disease activity at 12 months outcome, mediator at 6 months and depression at baseline as predictors of interest, and controlling for disease activity at baseline) efficiently and with appropriate standard error estimates.\n\nResults\n\n\n1 Baron RM, Kenny DA. The moderator-mediator variable distinction in social psychological research: conceptual, strategic, and statistical considerations. J Pers Soc Psychol. 1986 Dec;51(6):1173-82. doi: 10.1037//0022-3514.51.6.1173. PMID: 3806354.\n\n\nMultiple mediation model path diagram and estimates from structural equations model. Part A shows the total effect of depression at baseline on disease activity at 12 months is that, per one unit increase in depression, disease activity increases by 2.90. Part B show the associations of depression at baseline, mediators at 6 months, and disease activity at 12 months. Individual mediation effects are found by multiplying a by b for each mediator. Of the 2.90 total effect, 0.005 went through sleep, 0.138 went through smoking and 0.107 went through activity. Summing these three value, a total of 0.250 (8.6% mediated) when through the three mediators."
  },
  {
    "objectID": "notes/Lec06.ConfoundingEffectModificationMediation.html#precision-variables",
    "href": "notes/Lec06.ConfoundingEffectModificationMediation.html#precision-variables",
    "title": "Confouding, Effect Modification, and Mediation",
    "section": "5 Precision Variables",
    "text": "5 Precision Variables\n\n5.1 Overview\n\nSometimes the scientific question to be answered is chosen based on which questions can be answered most precisely\nIn general, questions can be answered more precisely when the within group distribution is less variable\nComparing groups that are similar with respect to other important risk factors decreases variability\nThe precision variability is independent of the cause of the response\nIf we adjust for such a variable, we tend to gain precision\nDirected Acyclyic Graph:\n\n\n\n\n\nflowchart LR\n  X[Predictor] --&gt; Y[Outcome]\n  Z[Precision] --&gt; Y[Outcome]\n\n\n\n\n\n\n\n5.2 Adjusting for Precision Variables\n\n5.2.1 Precision for Difference of Independent Means\n\nIndependent observations where group 1 has a different mean and variance than group 2\n\n\\(\\textrm{ind } Y_{ij} \\sim (\\mu_j, \\sigma_j^2), j = 1, 2; i = 1, \\ldots, n_j\\)\n\\(n = n_1 + n_2\\); \\(r = n_1 / n_2\\)\n\\(\\theta = \\mu_1 - \\mu_2\\), \\(\\hat{\\theta} = \\overline{Y}_1 - \\overline{Y}_2\\)\n\\(V = (r+1)(\\frac{\\sigma_1^2}{r} + \\sigma_2^2)\\)\n\\(se(\\hat{\\theta}) = \\sqrt{\\frac{V}{n}} = \\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}\\)\n\nMight control for some variable in order to decrease the within group variability\n\nRestrict population sampled\nStandardize ancillary treatments\nStandardize measurement procedure\n\n\n\n\n5.2.2 Precision for Linear Regression\n\nIndependent continuous outcome associated with covariate (\\(X\\))\n\n\\(\\textrm{ind } Y_i | X_i ~ \\sim(\\beta_0 + \\beta_1 X_i, \\sigma^2_{Y|X}), i = 1, \\ldots, n\\)\n\\(\\theta = \\beta_1, \\hat{\\theta} = \\hat{\\beta_1}\\) from LS regression\n\\(V = \\frac{\\sigma^2_{Y|X}}{\\textrm{Var}(X)}\\)\n\\(se(\\hat{\\theta}) = \\sqrt{\\frac{\\hat{\\sigma}^2_{Y|X}}{n \\hat{\\textrm{Var}}(X)}}\\)\n\nAdjusting for covariates (\\(W\\)) decreases the within group standard deviation\n\n\\(\\textrm{Var}(Y | X)\\) versus \\(\\textrm{Var}(Y | X, W)\\)\n\nIndependent continuous outcome associated with covariate (\\(X\\)) and precision variable (\\(W\\))\n\n\\(\\textrm{ind } Y_i | X_i, W_i ~ \\sim(\\beta_0 + \\beta_1 X_i + \\beta_2 W_i, \\sigma^2_{Y|X,W}), i = 1, \\ldots, n\\)\n\\(\\theta = \\beta_1, \\hat{\\theta} = \\hat{\\beta_1}\\) from LS regression\n\\(V = \\frac{\\sigma^2_{Y|X,W}}{\\textrm{Var}(X)(1-r^2_{X,W})}\\)\n\\(se(\\hat{\\theta}) = \\sqrt{\\frac{\\hat{\\sigma}^2_{Y|X,W}}{n \\hat{\\textrm{Var}}(X)(1-r^2_{X,W})}}\\)\n\\(\\sigma^2_{Y|X,W} = \\sigma^2_{Y|X} - \\beta_2^2 \\textrm{Var}(W | X)\\)\n\n\n\n\n5.2.3 Precision for Difference of Proportions\n\nWhen analyzing proportions (means), the mean variance relationship is critical\n\nPrecision is greatest when proportion is close to 0 or 1\nGreater homogeneity of groups makes results more deterministic (this is the goal, at least)\n\nIndependent binary outcomes\n\n\\(\\textrm{ind } Y_{ij} \\sim B(1, p_j), i = 1, \\ldots, n_j; j = 1, 2\\)\n\\(n = n_1 + n_2; r = n_1 / n_2\\)\n\\(\\theta = p_1 - p_2\\), \\(\\hat{\\theta} = \\hat{p}_1 - \\hat{p_2} = \\overline{Y}_1 - \\overline{Y}_2\\)\n\\(\\sigma^2_j = p_j(1-p_j)\\)\n\\(V = (r+1)(\\frac{\\sigma_1^2}{r} + \\sigma_2^2)\\)\n\\(se(\\hat{\\theta}) = \\sqrt{\\frac{V}{n}} = \\sqrt{\\frac{\\sigma^2_1}{n_1} + \\frac{\\sigma^2_2}{n_2}}\\)\n\n\n\n\n5.2.4 Precision for Odds\n\nWhen analyzing odds (a nonlinear function of the mean), adjusting for precision variables results in more extreme estimates\n\\(\\textrm{Odds} = \\frac{p}{1-p}\\)\nOdds using average of stratum specific \\(p\\) is not the average of stratum specific odds\nExample: Stroke by smoking (in CVD strata)\n\nNo association between smoking and CVD in the sample: 10% smokers in subjects with and without CVD\nCVD (precision variable) associated with stroke (outcome)\n\nCVD is not a confounder, but it is clearly a precision variable\n\nWithin each CVD stratum, the (adjusted) odds ratio for stroke by smoking is 2.0.\nOverall, the unadjusted odds ratio is 1.8. That is, the unadjusted odds ratio is attenuated toward the null compared to the adjusted odds ratios\n\n\n\nNo CVD\nN\np\nOdds\n\n\n\n\nSmoke\n1000\n0.04\n0.04\n\n\nNon-Smoke\n10000\n0.02\n0.02\n\n\nRatio\n\n\n2.0\n\n\n\n\n\n\nCVD\nN\np\nOdds\n\n\n\n\nSmoke\n100\n0.50\n1.00\n\n\nNon-Smoke\n1000\n0.33\n0.50\n\n\nRatio\n\n\n2.0\n\n\n\n\n\n\nOverall\nN\np\nOdds\n\n\n\n\nSmoke\n1100\n0.082\n0.09\n\n\nNon-Smoke\n11000\n0.048\n0.05\n\n\nRatio\n\n\n1.8"
  },
  {
    "objectID": "notes/Lec06.ConfoundingEffectModificationMediation.html#diagnosing-confounding-1",
    "href": "notes/Lec06.ConfoundingEffectModificationMediation.html#diagnosing-confounding-1",
    "title": "Confouding, Effect Modification, and Mediation",
    "section": "6 Diagnosing Confounding",
    "text": "6 Diagnosing Confounding\n\n6.1 Adjustment for Covariates\n\nWe include predictors in an analysis for a number of reasons. In order of importance...\n\nScientific question\n\nPredictor of Interest\nEffect Modifiers\n\nAdjust for confounding\nGain precision\n\nAdjustment for covariates changes the question being answered by the statistical analysis\n\nAdjustments can be made to isolate associations that are of particular interest\nWhen consulting with a scientist, it is often difficult to decide whether the interest in an additional covariate is due to confounding, effect modification, or precision\n\nThe distinction is important because I tend to treat these variable differently in the analysis\n\nOften the scientific question dictates inclusion of particular predictors\n\nPredictor of interest: The scientific parameter of interest can be modeled by multiple predictors (e.g. dummy variables, polynomials, splines)\nEffect Modifiers: The scientific question relates to the detection of effect modification\nConfounders: The scientific question may be state in terms of adjusting for known (or suspected) confounders\n\n\n\n\n\n6.2 Confounder Detection\n\nUnanticipated confounding\n\nSome times we must explore our data to assess whether our results were confounded by some variable\nGoal is to assess the “independent effect” of the predictor of interest on the outcome\n\nConfounders\n\nVariables (causally) predictive of the outcome, but not in the causal pathway\n\nBest method: Think about the scientific problem beforehand (perhaps draw DAG)\nUsing data, often assessed in the control group\n\nVariables associated with the predictor of interest in the sample\n\nNote that statistical significance is not relevant because this tells us about associations in the population\n\nDetection of confounding ultimately must rely on our best knowledge about the possible scientific mechanisms\n\nEffect of confounding: A confounder can make the association between the predictor of interest and the response variable look...\n\nStronger than the true association\nWeaker than the true association\nThe complete reverse of the true association (“qualitative confounding”)"
  },
  {
    "objectID": "notes/Lec06.ConfoundingEffectModificationMediation.html#graphical-methods-for-visualizing-effect-modification-confounding-and-precision",
    "href": "notes/Lec06.ConfoundingEffectModificationMediation.html#graphical-methods-for-visualizing-effect-modification-confounding-and-precision",
    "title": "Confouding, Effect Modification, and Mediation",
    "section": "7 Graphical Methods for Visualizing Effect Modification, Confounding, and Precision",
    "text": "7 Graphical Methods for Visualizing Effect Modification, Confounding, and Precision\n\nConduct stratified analysis to distinguish between\n\nEffect modifiers\nConfounders\nPrecision variables\n\nPlots most illustrative for continuous outcomes\n\n\n7.1 Effect Modifiers\n\nEstimates of treatment effect differ among strata\n\nWhen analyzing difference of means of continuous data, stratified smooth curves of the data are non-parallel\nGraphical techniques difficult in other settings\n\n\n\n\nCode\nset.seed(1231)\nn &lt;- 200\ngrp &lt;- rep(c(0,1),each=n/2)\nX &lt;- runif(n)\nemplot &lt;- data.frame(grp=grp,\n                     X=X,\n                     Y=.1*grp + 2*X -4*grp*X + rnorm(n)\n)\nemplot$group &lt;- factor(emplot$grp, levels=0:1, labels=c(\"Trt\",\"Ctrl\"))\nggplot(emplot, aes(x=X, y=Y)) + geom_point() + geom_smooth(se=FALSE) + theme_bw()\n\n\n\n\n\nUnadjusted association between outcome and predictor shows a flat slope.\n\n\n\n\n\n\nCode\nggplot(emplot, aes(x=X, y=Y, color=group, grp=group)) + geom_point() + geom_smooth(se=FALSE,) + theme_bw()\n\n\n\n\n\nAdjusted association between outcome and predictor shows an increasing slope in one group and a decreasing slope in the other. This demonstrates effect modification of group.\n\n\n\n\n\n\n7.2 Confounders\n\nEstimates of treatment effect the same across strata, AND\n\nConfounder is causally associated with the response, AND\nConfounder associated with the POI in the sample\n\nWhen analyzing difference of means of continuous data\n\nStratified smooth curve of data are parallel\nDistribution of POI differs across strata\nUnadjusted and adjusted analyses give different estimates\n\n\n\n\nCode\nset.seed(1231)\nn &lt;- 200\ngrp &lt;- rep(c(0:3), each=n/4)\nX &lt;- c(runif(n/4, 0,.4), runif(n/4, 0.2,.6), runif(n/4, .4,.8), runif(n/4, .6, 1))\nY &lt;- X - grp + rnorm(n)\n\npar(mfrow=c(1,2))\nplot(X,Y, main=\"Unadjusted\")\nlines(lowess(Y~X), lwd=2)\n\nplot(X,Y, pch=(grp+1), main=\"Adjusted Confounding\")\nm1 &lt;- lm(Y~X + grp)\nsegments(x0=0  , x1=.4, y0=coef(m1)[1] + 0*coef(m1)[2],   y1=coef(m1)[1] + 0.4*coef(m1)[2], lwd=2)\nsegments(x0=0.2, x1=.6, y0=coef(m1)[1] + 0.2*coef(m1)[2]+ 1*coef(m1)[3], y1=coef(m1)[1] + 0.6*coef(m1)[2] + 1*coef(m1)[3], lty=2, lwd=2)\nsegments(x0=0.4, x1=.8, y0=coef(m1)[1] + 0.4*coef(m1)[2]+ 2*coef(m1)[3], y1=coef(m1)[1] + 0.8*coef(m1)[2] + 2*coef(m1)[3], lty=3, lwd=2)\nsegments(x0=0.6, x1= 1, y0=coef(m1)[1] + 0.6*coef(m1)[2]+ 3*coef(m1)[3], y1=coef(m1)[1] +   1*coef(m1)[2] + 3*coef(m1)[3], lty=4, lwd=2)\n\n\n\n\n\n\n\n7.3 Precision Variables\n\nEstimates of treatment effect the same across strata, AND\n\nVariable is causally associated with the response, AND\nVariable is not associated with the POI in the sample\n\nWhen analyzing difference of means of continuous data\n\nStratified smooth curve of data are parallel\nDistribution of POI same across strata\nUnadjusted and adjusted analyses give similar estimates but with smaller standard errors\n\n\n\n\nCode\nset.seed(1231)\nn &lt;- 200\ngrp &lt;- rep(c(0:3), each=n/4)\nX &lt;- runif(n)\nY &lt;- 3*X + 5*grp + rnorm(n)\n\npar(mfrow=c(1,2))\nplot(X,Y, main=\"Unadjusted Precision\")\nlines(lowess(Y~X), lwd=2)\nplot(X,Y, main=\"Adjusted Precision\", pch=(grp+1))\nm2 &lt;- lm(Y~X + grp)\nabline(a=coef(m2)[1], b=coef(m2)[2], lwd=2)\nabline(a=coef(m2)[1] + 1*coef(m2)[3], b=(coef(m2)[2]), lty=2, lwd=2)\nabline(a=coef(m2)[1] + 2*coef(m2)[3], b=(coef(m2)[2]), lty=3, lwd=2)\nabline(a=coef(m2)[1] + 3*coef(m2)[3], b=(coef(m2)[2]), lty=4, lwd=2)"
  },
  {
    "objectID": "notes/Lec05.Precision.html",
    "href": "notes/Lec05.Precision.html",
    "title": "Precision of Statistical Intefence",
    "section": "",
    "text": "Goal of statistical inference is to estimate parameters accurately (unbiased) and with high precision\nMeasures of precision\n\nStandard error (not standard deviation)\nWidth of confidence intervals\nPower (equivalently, type II error rate)\n\nScientific hypotheses are typically refined in statistical hypotheses by identifying some parameter, \\(\\theta\\), measuring differences in the distribution of the response variable\nOften we are interested in if \\(\\theta\\) differs across of levels of categorical (e.g. treatment/control) or continuous (e.g. age) predictor variables\n\\(\\theta\\) could be any summary measure such as\n\nDifference/ratio of means\nDifference/ratio of medians\nRatio of geometric means\nDifference/ratio of proportions\nOdds ratio, relative risk, risk difference\nHazard ratio\n\nHow to select \\(\\theta\\)? In order of importance...\n\n\nScientific (clinical) importance. May be based on current state of knowledge\nIs \\(\\theta\\) likely to vary across the predictor of interest? Impacts the ability to detect a difference, if it exists.\nStatistical precision. Only relevant if all other factors are equal.\n\n\nStatistics is concerned with making inference about population parameters, (\\(\\theta\\)), based on a sample of data\n\nFrequentist estimation includes both point estimates (\\(\\hat{\\theta}\\)) and interval estimates (confidence intervals)\nBayesian analysis estimates the posterior distribution of \\(\\theta\\) given the sampled data, \\(p(\\theta | \\textrm{data})\\). The posterior distribution can then be summarized by quantities like the posterior mean and 95% credible interval.\n\n\n\n\nConsider the following results from 5 clinical trials of three drugs (A, B, C) designed to lower cholesterol compared to baseline. Assume a 10 unit drop in cholesterol (relative to baseline) is clinically meaningful.\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrial\nDrug\nPts\nMean diff\nStd dev\nStd error\n95% CI for diff\np-value\n\n\n\n\n1\nA\n30\n-30\n191.7\n49.5\n[-129, 69]\n0.55\n\n\n2\nA\n1000\n-30\n223.6\n10\n[-49.6, -10.4]\n0.002\n\n\n3\nB\n40\n-20\n147.6\n33\n[-85, 45]\n0.55\n\n\n4\nB\n4000\n-2\n147.6\n3.3\n[-8.5, 4.5]\n0.54\n\n\n5\nC\n5000\n-6\n100.0\n2\n[-9.9, -2.1]\n0.002\n\n\n\n\nCompare the results of the different trials with respect to the sample size, mean difference, etc.\n\nWhich drug is effective at reducing cholesterol?\nWhy is study 4 more informative than study 3 (even though the \\(p\\) values are similar)?\nKey points\n\nHypothesis tests and \\(p\\)-values can often be insufficient to make proper decisions. The confidence interval provides more useful information.\nNarrower confidence intervals (more precise estimates) allow for scientific conclusion regardless of the \\(p\\) value"
  },
  {
    "objectID": "notes/Lec05.Precision.html#overview",
    "href": "notes/Lec05.Precision.html#overview",
    "title": "Precision of Statistical Intefence",
    "section": "",
    "text": "Goal of statistical inference is to estimate parameters accurately (unbiased) and with high precision\nMeasures of precision\n\nStandard error (not standard deviation)\nWidth of confidence intervals\nPower (equivalently, type II error rate)\n\nScientific hypotheses are typically refined in statistical hypotheses by identifying some parameter, \\(\\theta\\), measuring differences in the distribution of the response variable\nOften we are interested in if \\(\\theta\\) differs across of levels of categorical (e.g. treatment/control) or continuous (e.g. age) predictor variables\n\\(\\theta\\) could be any summary measure such as\n\nDifference/ratio of means\nDifference/ratio of medians\nRatio of geometric means\nDifference/ratio of proportions\nOdds ratio, relative risk, risk difference\nHazard ratio\n\nHow to select \\(\\theta\\)? In order of importance...\n\n\nScientific (clinical) importance. May be based on current state of knowledge\nIs \\(\\theta\\) likely to vary across the predictor of interest? Impacts the ability to detect a difference, if it exists.\nStatistical precision. Only relevant if all other factors are equal.\n\n\nStatistics is concerned with making inference about population parameters, (\\(\\theta\\)), based on a sample of data\n\nFrequentist estimation includes both point estimates (\\(\\hat{\\theta}\\)) and interval estimates (confidence intervals)\nBayesian analysis estimates the posterior distribution of \\(\\theta\\) given the sampled data, \\(p(\\theta | \\textrm{data})\\). The posterior distribution can then be summarized by quantities like the posterior mean and 95% credible interval.\n\n\n\n\nConsider the following results from 5 clinical trials of three drugs (A, B, C) designed to lower cholesterol compared to baseline. Assume a 10 unit drop in cholesterol (relative to baseline) is clinically meaningful.\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrial\nDrug\nPts\nMean diff\nStd dev\nStd error\n95% CI for diff\np-value\n\n\n\n\n1\nA\n30\n-30\n191.7\n49.5\n[-129, 69]\n0.55\n\n\n2\nA\n1000\n-30\n223.6\n10\n[-49.6, -10.4]\n0.002\n\n\n3\nB\n40\n-20\n147.6\n33\n[-85, 45]\n0.55\n\n\n4\nB\n4000\n-2\n147.6\n3.3\n[-8.5, 4.5]\n0.54\n\n\n5\nC\n5000\n-6\n100.0\n2\n[-9.9, -2.1]\n0.002\n\n\n\n\nCompare the results of the different trials with respect to the sample size, mean difference, etc.\n\nWhich drug is effective at reducing cholesterol?\nWhy is study 4 more informative than study 3 (even though the \\(p\\) values are similar)?\nKey points\n\nHypothesis tests and \\(p\\)-values can often be insufficient to make proper decisions. The confidence interval provides more useful information.\nNarrower confidence intervals (more precise estimates) allow for scientific conclusion regardless of the \\(p\\) value"
  },
  {
    "objectID": "notes/Lec05.Precision.html#precision-variables-and-regression-modeling",
    "href": "notes/Lec05.Precision.html#precision-variables-and-regression-modeling",
    "title": "Precision of Statistical Intefence",
    "section": "2 Precision variables and regression modeling",
    "text": "2 Precision variables and regression modeling\n\n2.1 Definition\n\nPure precision variables are associated with the outcome only\n\nThat is, no association (in the sample) with the predictor of interest\n\nIn a randomized trial where random treatment assignment is the predictor of interest it may be possible to have a pure precision variable by using stratified randomization\n\nStratified randomization can guarantee that there is no correlation between the precision variable and treatment group assignment\n\nIn observational research, there may be some correlation between the precision variable and the predictor of interest\n\nMay act more like a confounder in these cases, depending on the size of the association\n\n\n\n\n2.2 Directed Acyclic Graph\n\n\n\n\nflowchart LR\n  X[Predictor\\nof interest] --&gt; Y[Outcome]\n  W[Precision] --&gt; Y[Outcome]\n\n\n\n\n\n\n\n2.3 Example\n\n2.3.1 Data generation\n\nCategorical precision variable (W), Predictor of Interest (X), Outcome (Y)\n\n\n\nCode\n# Generate some data\nset.seed(1231)\nn &lt;- 200\nprecisiondata &lt;- \n  data.frame(W=rep(0:1, each=n/2),\n             X=runif(n),\n             Y=NA\n)\nprecisiondata$Y &lt;- 10 + 1.5*precisiondata$X + 10*precisiondata$W + rnorm(n,0,4)\nprecisiondata$W &lt;- factor(precisiondata$W)\n\n\n\n\n2.3.2 Bivariate plots\n\nW is uncorrelated with X, and associated with Y\n\n\n\nCode\nlibrary(ggplot2)\nggplot(precisiondata, aes(x=W,y=X)) + geom_boxplot()\n\n\n\n\n\nBox plot showing the (lack of) association between the precision variable (W) and the predictor of interest (X)\n\n\n\n\n\n\nCode\nggplot(precisiondata, aes(x=W,y=Y)) + geom_boxplot()\n\n\n\n\n\nBox plot showing the association between the precision variable (W) and the outcome (Y)\n\n\n\n\n\n\nCode\nggplot(precisiondata, aes(x=X,y=Y)) + geom_point() + geom_smooth(method=\"lm\")\n\n\n\n\n\nAssociation between the predictor of interest (X) and the outcome (Y) ignoring the precision variable (W)\n\n\n\n\n\n\n2.3.3 Bivariate plot by W\n\n\nCode\nggplot(precisiondata, aes(x=X,y=Y,color=W,group=W)) + geom_point() + geom_smooth(method=\"lm\")\n\n\n\n\n\nAssociation between the predictor of interest (X) and the outcome (Y) by the precision variable (W)\n\n\n\n\n\n\n2.3.4 Adjusted and unadjusted regression models\n\nUnadjusted: \\(E[Y|X] = \\beta_0 + \\beta_1*X\\)\nAdjusted: \\(E[Y|X] = \\beta_0 + \\beta_1*X + \\beta_2*W\\)\n\n\n\nCode\nsummary(lm(Y ~ X, data=precisiondata))\n\n\n\nCall:\nlm(formula = Y ~ X, data = precisiondata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.9532  -4.6804  -0.7675   5.5132  13.5264 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  13.8858     0.8822  15.740   &lt;2e-16 ***\nX             3.4619     1.5818   2.189   0.0298 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.227 on 198 degrees of freedom\nMultiple R-squared:  0.02362,   Adjusted R-squared:  0.01869 \nF-statistic:  4.79 on 1 and 198 DF,  p-value: 0.0298\n\n\nCode\nsummary(lm(Y ~ X + W, data=precisiondata))\n\n\n\nCall:\nlm(formula = Y ~ X + W, data = precisiondata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.2400 -2.8797  0.3465  2.3857 11.7172 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   9.1712     0.6417  14.292  &lt; 2e-16 ***\nX             3.4659     1.0294   3.367 0.000915 ***\nW1            9.4252     0.5731  16.446  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.052 on 197 degrees of freedom\nMultiple R-squared:  0.5885,    Adjusted R-squared:  0.5844 \nF-statistic: 140.9 on 2 and 197 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nCode\nmykable = function(x){\n  knitr::kable(x, row.names = FALSE, align = c(\"l\", \"l\", \"r\", \"r\", \"r\", \"r\", \"r\", \"r\", \"r\"),\n               booktabs=TRUE)\n}\n\nlibrary(finalfit)\nlibrary(dplyr)\n\nexplanatory = c(\"X\", \"W\")\ndependent = \"Y\"\n\nprecisiondata %&gt;% \n  finalfit(dependent, explanatory) %&gt;% mykable()\n\n\n\n\n\n\n\n\n\n\n\n\n\nDependent: Y\n\nunit\nvalue\nCoefficient (univariable)\nCoefficient (multivariable)\n\n\n\n\nX\n[0.0,1.0]\nMean (sd)\n15.6 (6.3)\n3.46 (0.34 to 6.58, p=0.030)\n3.47 (1.44 to 5.50, p=0.001)\n\n\nW\n0\nMean (sd)\n10.8 (4.1)\n-\n-\n\n\n\n1\nMean (sd)\n20.3 (4.2)\n9.42 (8.27 to 10.58, p&lt;0.001)\n9.43 (8.30 to 10.56, p&lt;0.001)"
  },
  {
    "objectID": "notes/Lec05.Precision.html#sampling-distributions",
    "href": "notes/Lec05.Precision.html#sampling-distributions",
    "title": "Precision of Statistical Intefence",
    "section": "3 Sampling Distributions",
    "text": "3 Sampling Distributions\n\nThe sampling distribution is the probability distribution of a statistic\n\ne.g. the sampling distribution of the sample mean is \\(N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\\)\n\nMost often we choose estimators that are asymptotically Normally distributed\n\nFor large \\(n\\), \\(\\hat{\\theta} \\sim N\\left(\\theta, \\frac{V}{n} \\right)\\)\n\\(\\hat{\\theta}\\) is our estimate of \\(\\theta\\). The \\(\\hat{ }\\) indicates it is an estimate.\nMean: \\(\\theta\\)\n\nVariance: \\(V\\), which is related to the “average amount of statistical information” available from each observation\n\nOften \\(V\\) depends on \\(\\theta\\)\n\n\n“Large” \\(n\\) depends on the distribution of the underlying data. If \\(n\\) is large enough, approximate Normality of \\(\\hat{\\theta}\\) will hold.\nCalculating \\(100 (1-\\alpha)\\%\\) confidence intervals \\(\\left(\\theta_L, \\theta_U \\right)\\) with approximate Normality\n\n\\(\\theta_L = \\hat{\\theta} - Z_{1-\\alpha/2} \\sqrt{\\frac{V}{n}}\\)\n\\(\\theta_U = \\hat{\\theta} + Z_{1-\\alpha/2} \\sqrt{\\frac{V}{n}}\\)\n(estimate) \\(\\pm\\) (crit val) \\(\\times\\) (std err of estimate)\n\nCan similarly calculate approximate two-sided \\(p\\)-values\n\n\\(Z = \\frac{\\textrm{(estimate)} - \\textrm{(hyp value)}}{\\textrm{(std err of estimate)}}\\)"
  },
  {
    "objectID": "notes/Lec05.Precision.html#precision-and-powersample-size",
    "href": "notes/Lec05.Precision.html#precision-and-powersample-size",
    "title": "Precision of Statistical Intefence",
    "section": "4 Precision and Power/Sample Size",
    "text": "4 Precision and Power/Sample Size\n\nWhat are the measures of (high) precision?\n\nEstimators are less variable across studies, which is often measured by decreased standard error.\nNarrower confidence intervals. Estimators are consistent with fewer hypotheses if the CIs are narrow.\nAble to reject false hypotheses. Z statistic is higher when the alternative hypothesis is true.\n\nTranslation into sample size\n\nBased on the width of the confidence interval\nChoose a sample size such that a 95% CI will not contain both the null and design alternative\nIf both \\(\\theta_0\\) and \\(\\theta_1\\) cannot be in the CI, we have discriminated between those hypotheses\n\nBased on statistical power\n\nWhen the alternative is true, have a high probability of rejecting the null\nIn other words, minimize the type II error rate\n\nStatistical power: Quick review\n\nPower is the probability of rejecting the null hypothesis when the alternative is true\nPr(reject \\(H_0 | \\theta = \\theta_1\\))\nMost often \\(\\hat{\\theta} \\sim N\\left(\\theta, \\frac{V}{n} \\right)\\) so that the test statistic \\(Z = \\frac{\\hat{\\theta} - \\theta_0}{\\sqrt{V/n}}\\) wll follow a Normal distribution\nUnder \\(H_0\\), \\(Z \\sim N(0, 1)\\) so we reject \\(H_0\\) if \\(|Z| &gt; Z_{1-\\alpha/2}\\)\nUnder \\(H_1\\), \\(Z \\sim N\\left(\\frac{\\theta_1 - \\theta_0}{\\sqrt{V/n}}, 1\\right)\\)\n\nPower curves\n\nThe power function (power curve) is a function of the true value of \\(\\theta\\)\nWe can compute power for every value of \\(\\theta\\)\nAs \\(\\theta\\) moves away from \\(\\theta_0\\), power increases (for two-sided alternatives)\nFor any choice of desired power, there is always some \\(\\theta\\) such that the study has that power\n\\(Pwr(\\theta_0) = \\alpha\\), the type I error rate\n\n\n\n\nCode\nmydiffs &lt;- seq(-.8,.8,.05)\n\nmypower &lt;- vector(\"numeric\", length(mydiffs))\nmypower2 &lt;- vector(\"numeric\", length(mydiffs))\n\nfor(i in 1:length(mydiffs)){\n   mypower[i] &lt;- power.t.test(n=100, sd=1, delta=mydiffs[i])$power\n   mypower2[i] &lt;- power.t.test(n=100, sd=1.2, delta=mydiffs[i])$power\n}\n\nplot(mydiffs, mypower, xlab=\"True difference in means (theta)\", ylab=\"Power\", type=\"l\", main=\"\")\nlines(mydiffs, mypower2, lty=2)\nlegend(\"top\", c(\"sigma = 1.0\",\"sigma = 1.2\"), lty=1:2, inset=0.05)\n\n\n\n\n\nPower curves for a two-sample, equal variance, t-test; n=100"
  },
  {
    "objectID": "notes/Lec05.Precision.html#precision-and-standard-errors",
    "href": "notes/Lec05.Precision.html#precision-and-standard-errors",
    "title": "Precision of Statistical Intefence",
    "section": "5 Precision and Standard Errors",
    "text": "5 Precision and Standard Errors\n\nStandard errors are the key to precision\nGreater precision is achieved with smaller standard errors\nStandard errors are decreased by either decreasing \\(V\\) or increasing \\(n\\)\n\nTypically: \\(se(\\hat{\\theta}) = \\sqrt{\\frac{V}{n}}\\)\nWidth of CI: \\(2 \\times (\\textrm{crit value}) \\times se(\\hat{\\theta})\\)\nTest statistic: \\(Z = \\frac{\\hat{\\theta} - \\theta_0}{se(\\hat{\\theta})}\\)\n\n\n\n5.1 Example: One sample mean\n\nObservations are independent and identically distributed (iid)\n\n\\(\\textrm{iid } Y_i \\sim (\\mu, \\sigma^2), i = 1, \\ldots, n\\)\n\\(\\theta = \\mu\\), \\(\\hat{\\theta} = \\frac{1}{n} \\displaystyle \\sum_{i=1}^n Y_i = \\overline{Y}\\)\n\\(V = \\sigma^2\\), \\(se(\\hat{\\theta}) = \\sqrt{\\frac{\\sigma^2}{n}}\\)\n\nNote that we are not assuming a specific distribution for \\(Y_i\\), just that the distribution has a mean and variance\nWe are assuming that \\(n\\) is large so asymptotic results are applicable\n\nThen the distribution \\(Y_i\\) could be binary data, Poisson, exponential, normal, etc. and the results will hold\n\nThere are ways to decrease \\(V\\) including...\n\nRestrict sample by age, gender, etc.\nTake repeated measures on each subject, summarize, and perform test on summary measures\nBetter ideas (this course)\n\nAdjust for age and gender rather than restrict\nUse all repeated observations and modeling correlation\n\n\n\n\n\n5.2 Example: Two sample mean\n\nDifference of independent means\nObservations no longer identically distributed, just independent. Group 1 has a different mean and variance than group 2\n\n\\(\\textrm{ind } Y_{ij} \\sim (\\mu_j, \\sigma_j^2), j = 1, 2; i = 1, \\ldots, n_j\\)\n\\(n = n_1 + n_2\\); \\(r = n_1 / n_2\\)\n\\(\\theta = \\mu_1 - \\mu_2\\), \\(\\hat{\\theta} = \\overline{Y}_1 - \\overline{Y}_2\\)\n\\(V = (r+1)(\\frac{\\sigma_1^2}{r} + \\sigma_2^2)\\)\n\\(se(\\hat{\\theta}) = \\sqrt{\\frac{V}{n}} = \\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}\\)\n\n\n\n\n5.3 Comments on the optimal ratio of sample sizes (\\(r\\))\n\nIf we are constrained by the maximal sample size \\(n = n_1 + n_2\\)\n\nSmallest \\(V\\) when \\(r = \\frac{n_1}{n_2} = \\frac{\\sigma_1}{\\sigma_2}\\)\nIn other words, smaller \\(V\\) if we sample more subjects from the more variable group\n\nIf we are unconstrained by the maximal sample size, there is a point of diminishing returns\nExample: Case-control study where finding cases is difficult/expensive but finding controls is easy/cheap\n\nOften quoted little benefit beyond \\(r = 5\\). That is, little benefit in having more than 5 times as many controls as cases\n\n\n\n\nCode\nvar.fn &lt;- function(r, s1, s2) {\n  (r + 1) * (s1^2/r + s2^2)\n}\n\n# Optimal sample size ratio for fixed sample size\n\nn &lt;- 100\ns2 &lt;- 10\n\nplot(function(r) sqrt(var.fn(r, s1=s2, s2=s2) / n), 0, 20, ylim=c(1,6), xlim=c(0,25), ylab=\"Standard Error\", xlab=\"Sample Size Ratio r = n1/n2\")\nplot(function(r) sqrt(var.fn(r, s1=2*s2, s2=s2) / n), 0, 20, add=TRUE, lty=2)\nplot(function(r) sqrt(var.fn(r, s1=3*s2, s2=s2) / n), 0, 20, add=TRUE, lty=3)\ntext(20,4.7,\"s1 = s2\", pos=4)\ntext(20,5.1,\"s1 = 2*s2\", pos=4)\ntext(20,5.5,\"s1 = 3*s2\", pos=4)\npoints(c(1,2,3), sqrt(var.fn(c(1,2,3), s1=c(1,2,3)*s2, s2=s2)/ n), pch=2)\ntext(1, 1.8, \"r = 1\")\ntext(2, 2.8, \"r = 2\")\ntext(3, 3.8, \"r = 3\")\n\n\n\n\n\nOptimal r for Fixed (n1 + n2). r = s1 / s2, the ratio of standard deviations in the two group.\n\n\n\n\n\n\nCode\nn1 &lt;- 200\n\nplot(function(r) sqrt(var.fn(r, s1=s2, s2=s2) / (n1 + r*n1)), 0, 20, ylim=c(0.5,3), xlim=c(0,25), ylab=\"Standard Error\", xlab=\"Sample Size Ratio r = n1/n2\")\nplot(function(r) sqrt(var.fn(r, s1=2*s2, s2=s2) / (n1 + r*n1)), 0, 20, add=TRUE, lty=2)\nplot(function(r) sqrt(var.fn(r, s1=3*s2, s2=s2) / (n1 + r*n1)), 0, 20, add=TRUE, lty=3)\n\ntext(20,.7,\"s1 = s2\", pos=4)\ntext(20,.8,\"s1 = 2*s2\", pos=4)\ntext(20,.9,\"s1 = 3*s2\", pos=4)\n\n\n\n\n\nDiminishing returns for r &gt; 5. s1 and s2 are the standard deviations is group 1 and group 2, respectively. s1=s2 indicates the two groups have equal variablility. Other conditions represent the cases where the group we are oversampling has more variability.\n\n\n\n\n\n\n5.4 Example: Paired means\n\nDifference of paired means\nNo longer iid. Group 1 has a different mean and variance than group 2, and observations are paired (correlated)\n\n\\(Y_{ij} \\sim (\\mu_j, \\sigma_j^2), j = 1, 2; i = 1, \\ldots, n\\)\n\\(corr(Y_{i1}, Y_{i2}) = \\rho; corr(Y_{ij}, Y_{mk}) = 0 \\textrm{ if } i \\neq m\\)\n\\(\\theta = \\mu_1 - \\mu_2\\), \\(\\hat{\\theta} = \\overline{Y}_1 - \\overline{Y}_2\\)\n\\(V = \\sigma_1^2+ \\sigma_2^2 - 2 \\rho \\sigma_1 \\sigma_2\\)\n\\(se(\\hat{\\theta}) = \\sqrt{\\frac{V}{n}}\\)\n\nPrecision gains are made when matched observations are positively correlated (\\(\\rho &gt; 0\\))\n\nUsually the case, but possible exceptions\n\nSleep on successive nights\nIntrauterine growth of litter-mates\n\n\n\n\n\n5.5 Example: Clustered data\n\nClustered data: Experiment where treatments/interventions are assigned based on the basis of “clusters”\n\nHouseholds\nSchools\nClinics\nCities\n\nMean of clustered data\n\n\\(Y_{ij} \\sim (\\mu, \\sigma^2), i = 1, \\ldots, n; j = 1, \\ldots, m\\)\n\nUp to \\(n\\) clusters, each of which have \\(m\\) subjects\n\n\\(corr(Y_{ij}, Y_{ik}) = \\rho \\textrm{ if } j \\neq k\\)\n\\(corr(Y_{ij}, Y_{mk}) = 0 \\textrm{ if } i \\neq m\\)\n\n\\(\\theta = \\mu\\), \\(\\hat{\\theta} = \\frac{1}{nm} \\displaystyle \\sum_{i=1}^{n} \\sum_{j=1}^m Y_{ij} = \\overline{Y}\\)\n\\(V = \\sigma^2 \\left(\\frac{1 + (m-1)\\rho}{m} \\right)\\)\n\\(se(\\hat{\\theta}) = \\sqrt{\\frac{V}{n}}\\)\n\nWhat is V if ...\n\n\\(\\rho = 0\\) (independent)\n\\(m = 1\\)\n\\(m\\) is large (e.g \\(m = 1000\\)) and \\(\\rho\\) is 0, 1, or 0.01\nWith clustered data, even small correlations can be very important to consider\n\nEqual precision achieved with\n\n\n\n\n\n\nClusters (\\(n\\))\n\\(m\\)\n\\(\\rho\\)\nTotal N\n\n\n\n\n1000\n1\n0.01\n1000\n\n\n650\n2\n0.30\n1300\n\n\n550\n2\n0.10\n1100\n\n\n190\n10\n0.10\n1900\n\n\n109\n10\n0.01\n1090\n\n\n20\n100\n0.01\n2000\n\n\n\n\n\nAlways consider practical issues. Is it easier/cheaper to collect 1 observation on 1000 different subjects (\\(n=1000\\), \\(m=1\\)), or 100 observations on 20 different subjects (\\(n=20\\), \\(m=100\\))? These two designs have equal precision.\n\n\n\n5.6 Example: Independent Odds Ratios\n\nBinary outcomes\n\\(\\textrm{ind } Y_{ij} \\sim B(1, p_j), i = 1, \\ldots, n_j; j = 1, 2\\)\n\\(n = n_1 + n_2; r = n_1 / n_2\\)\n\\(\\theta = \\textrm{log}\\left(\\frac{p_1/(1-p_1)}{p_2/(1-p_2)} \\right)\\); \\(\\hat{\\theta} = \\textrm{log}\\left(\\frac{\\hat{p}_1/(1-\\hat{p}_1)}{\\hat{p}_2/(1-\\hat{p}_2)} \\right)\\)\n\\(\\sigma^2_j = \\frac{1}{p_j(1-p_j)} = \\frac{1}{p_j(q_j)}\\)\n\\(V = (r+1)(\\frac{\\sigma_1^2}{r} + \\sigma_2^2)\\)\n\\(se(\\hat{\\theta}) = \\sqrt{\\frac{V}{n}} = \\sqrt{\\frac{1}{n_1 p_1 q_1} + \\frac{1}{n_2 p_2 q_2}}\\)\nNotes on maximum precision\n\nMax precision is achieved when the underlying odds are near 1 (proportions near 0.5)\nIf we were considering differences in proportions, the max precision is achieved when the underlying proportions are near 0 or 1\n\n\n\n\n5.7 Example: Hazard Ratios\n\nIndependent censored time to event outcomes\n\n\\((T_{ij}, \\delta_{ij}), i = 1, \\ldots, n_j; j = 1, 2\\)\n\\(n = n_1 + n_2; r = n_1 / n_2\\)\n\\(\\theta = \\textrm{log(HR)}\\); \\(\\hat{\\theta} = \\hat{\\beta}\\) from proportional hazards (PH) regression\n\\(V = \\frac{(r+1)(1/r+1)}{\\textrm{Pr}(\\delta_{ij} = 1)}\\)\n\\(se(\\hat{\\theta}) = \\sqrt{\\frac{V}{n}} = \\sqrt{\\frac{(r+1)(1/r+1)}{d}}\\)\n\nIn the PH model, statistical information is roughly proportional to \\(d\\), the number of observed events\n\nPapers always report the number of events\nStudy design must consider how long it will take to observe events (e.g. deaths) starting from randomization\n\n\n\n\n5.8 Example: Linear Regression\n\nIndependent continuous outcomes associated with covariates\n\n\\(\\textrm{ind } Y_i | X_i ~ \\sim(\\beta_0 + \\beta_1 X_i, \\sigma^2_{Y|X}), i = 1, \\ldots, n\\)\n\\(\\theta = \\beta_1, \\hat{\\theta} = \\hat{\\beta_1}\\) from LS regression\n\\(V = \\frac{\\sigma^2_{Y|X}}{\\textrm{Var}(X)}\\)\n\\(se(\\hat{\\theta}) = \\sqrt{\\frac{\\hat{\\sigma}^2_{Y|X}}{n \\hat{\\textrm{Var}}(X)}}\\)\n\nPrecision tends to increases as the predictor (\\(X\\)) is measured over a wider range\nPrecision also related to the within group variance \\(\\sigma^2_{Y|X}\\)\n\nWhat happens to the formulas when \\(X\\) is a binary variable? See two sample mean"
  },
  {
    "objectID": "notes/Lec05.Precision.html#summary-and-concluding-comments",
    "href": "notes/Lec05.Precision.html#summary-and-concluding-comments",
    "title": "Precision of Statistical Intefence",
    "section": "6 Summary and Concluding Comments",
    "text": "6 Summary and Concluding Comments\n\nOptions for increasing precision\n\nIncrease sample size\nDecrease \\(V\\)\n(Decrease confidence level)\n\nCriteria for precision\n\nStandard error\nWidth of confidence intervals\nStatistical power\n\nSelect a suitable, scientifically meaningful alternative\n\nWith proper precision, can distinguish between minor and meaningful effects\n\nSelect desired power\nSample size calculation: The number of sampling units needed to obtain the desired precision\n\nLevel of significance \\(\\alpha\\) when \\(\\theta = \\theta_0\\)\nPower \\(\\beta\\) when \\(\\theta = \\theta_1\\)\nVariability \\(V\\) within one sampling unit\n\\(n = \\frac{(z_{1-\\alpha/2} + z_{1-\\beta})^2 \\times V}{(\\theta_1 - \\theta_0)^2}\\)\n\nWhen sample size is constrained (the usual case) either\n\nCompute power to detect a specified alternative\n\n\\(1 - \\beta = \\phi \\left(\\frac{(\\theta_1-\\theta_0)}{\\sqrt{V/n}} - z_{1-\\alpha/2} \\right)\\)\n\\(\\phi\\) is the standard Normal cdf function\nIn STATA, use normprob for the \\(\\phi\\) function\n\nCompute alternative that can be detected with high power\n\n\\(\\theta_1 = \\theta_0 + (z_{1-\\alpha/2} + z_{1-\\beta}) \\sqrt{V/n}\\)\n\n\n\n\n6.1 General Comments\n\nSample size required behaves like the square of the width of the CI. To cut the width of the CI in half, need to quadruple the sample size.\nPositively correlated observations within the same group provide less precision than the same number of independent observations\nPositively correlated observations across groups provide more precision\nWhat power do you use?\n\nMost popular is 80% (too low) or 90%\nKey is to be able to discriminate between scientifically meaningful hypotheses"
  }
]