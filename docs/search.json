[
  {
    "objectID": "notes/Lec05.Precision.html",
    "href": "notes/Lec05.Precision.html",
    "title": "Precision of Statistical Intefence",
    "section": "",
    "text": "Goal of statistical inference is to estimate parameters accurately (unbiased) and with high precision\nMeasures of precision\n\nStandard error (not standard deviation)\nWidth of confidence intervals\nPower (equivalently, type II error rate)\n\nScientific hypotheses are typically refined in statistical hypotheses by identifying some parameter, \\(\\theta\\), measuring differences in the distribution of the response variable\nOften we are interested in if \\(\\theta\\) differs across of levels of categorical (e.g. treatment/control) or continuous (e.g. age) predictor variables\n\\(\\theta\\) could be any summary measure such as\n\nDifference/ratio of means\nDifference/ratio of medians\nRatio of geometric means\nDifference/ratio of proportions\nOdds ratio, relative risk, risk difference\nHazard ratio\n\nHow to select \\(\\theta\\)? In order of importance...\n\n\nScientific (clinical) importance. May be based on current state of knowledge\nIs \\(\\theta\\) likely to vary across the predictor of interest? Impacts the ability to detect a difference, if it exists.\nStatistical precision. Only relevant if all other factors are equal.\n\n\nStatistics is concerned with making inference about population parameters, (\\(\\theta\\)), based on a sample of data\n\nFrequentist estimation includes both point estimates (\\(\\hat{\\theta}\\)) and interval estimates (confidence intervals)\nBayesian analysis estimates the posterior distribution of \\(\\theta\\) given the sampled data, \\(p(\\theta | \\textrm{data})\\). The posterior distribution can then be summarized by quantities like the posterior mean and 95% credible interval.\n\n\n\n\nConsider the following results from 5 clinical trials of three drugs (A, B, C) designed to lower cholesterol compared to baseline. Assume a 10 unit drop in cholesterol (relative to baseline) is clinically meaningful.\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrial\nDrug\nPts\nMean diff\nStd dev\nStd error\n95% CI for diff\np-value\n\n\n\n\n1\nA\n30\n-30\n191.7\n49.5\n[-129, 69]\n0.55\n\n\n2\nA\n1000\n-30\n223.6\n10\n[-49.6, -10.4]\n0.002\n\n\n3\nB\n40\n-20\n147.6\n33\n[-85, 45]\n0.55\n\n\n4\nB\n4000\n-2\n147.6\n3.3\n[-8.5, 4.5]\n0.54\n\n\n5\nC\n5000\n-6\n100.0\n2\n[-9.9, -2.1]\n0.002\n\n\n\n\nCompare the results of the different trials with respect to the sample size, mean difference, etc.\n\nWhich drug is effective at reducing cholesterol?\nWhy is study 4 more informative than study 3 (even though the \\(p\\) values are similar)?\nKey points\n\nHypothesis tests and \\(p\\)-values can often be insufficient to make proper decisions. The confidence interval provides more useful information.\nNarrower confidence intervals (more precise estimates) allow for scientific conclusion regardless of the \\(p\\) value"
  },
  {
    "objectID": "notes/Lec05.Precision.html#overview",
    "href": "notes/Lec05.Precision.html#overview",
    "title": "Precision of Statistical Intefence",
    "section": "",
    "text": "Goal of statistical inference is to estimate parameters accurately (unbiased) and with high precision\nMeasures of precision\n\nStandard error (not standard deviation)\nWidth of confidence intervals\nPower (equivalently, type II error rate)\n\nScientific hypotheses are typically refined in statistical hypotheses by identifying some parameter, \\(\\theta\\), measuring differences in the distribution of the response variable\nOften we are interested in if \\(\\theta\\) differs across of levels of categorical (e.g. treatment/control) or continuous (e.g. age) predictor variables\n\\(\\theta\\) could be any summary measure such as\n\nDifference/ratio of means\nDifference/ratio of medians\nRatio of geometric means\nDifference/ratio of proportions\nOdds ratio, relative risk, risk difference\nHazard ratio\n\nHow to select \\(\\theta\\)? In order of importance...\n\n\nScientific (clinical) importance. May be based on current state of knowledge\nIs \\(\\theta\\) likely to vary across the predictor of interest? Impacts the ability to detect a difference, if it exists.\nStatistical precision. Only relevant if all other factors are equal.\n\n\nStatistics is concerned with making inference about population parameters, (\\(\\theta\\)), based on a sample of data\n\nFrequentist estimation includes both point estimates (\\(\\hat{\\theta}\\)) and interval estimates (confidence intervals)\nBayesian analysis estimates the posterior distribution of \\(\\theta\\) given the sampled data, \\(p(\\theta | \\textrm{data})\\). The posterior distribution can then be summarized by quantities like the posterior mean and 95% credible interval.\n\n\n\n\nConsider the following results from 5 clinical trials of three drugs (A, B, C) designed to lower cholesterol compared to baseline. Assume a 10 unit drop in cholesterol (relative to baseline) is clinically meaningful.\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrial\nDrug\nPts\nMean diff\nStd dev\nStd error\n95% CI for diff\np-value\n\n\n\n\n1\nA\n30\n-30\n191.7\n49.5\n[-129, 69]\n0.55\n\n\n2\nA\n1000\n-30\n223.6\n10\n[-49.6, -10.4]\n0.002\n\n\n3\nB\n40\n-20\n147.6\n33\n[-85, 45]\n0.55\n\n\n4\nB\n4000\n-2\n147.6\n3.3\n[-8.5, 4.5]\n0.54\n\n\n5\nC\n5000\n-6\n100.0\n2\n[-9.9, -2.1]\n0.002\n\n\n\n\nCompare the results of the different trials with respect to the sample size, mean difference, etc.\n\nWhich drug is effective at reducing cholesterol?\nWhy is study 4 more informative than study 3 (even though the \\(p\\) values are similar)?\nKey points\n\nHypothesis tests and \\(p\\)-values can often be insufficient to make proper decisions. The confidence interval provides more useful information.\nNarrower confidence intervals (more precise estimates) allow for scientific conclusion regardless of the \\(p\\) value"
  },
  {
    "objectID": "notes/Lec05.Precision.html#precision-variables-and-regression-modeling",
    "href": "notes/Lec05.Precision.html#precision-variables-and-regression-modeling",
    "title": "Precision of Statistical Intefence",
    "section": "2 Precision variables and regression modeling",
    "text": "2 Precision variables and regression modeling\n\n2.1 Definition\n\nPure precision variables are associated with the outcome only\n\nThat is, no association (in the sample) with the predictor of interest\n\nIn a randomized trial where random treatment assignment is the predictor of interest it may be possible to have a pure precision variable by using stratified randomization\n\nStratified randomization can guarantee that there is no correlation between the precision variable and treatment group assignment\n\nIn observational research, there may be some correlation between the precision variable and the predictor of interest\n\nMay act more like a confounder in these cases, depending on the size of the association\n\n\n\n\n2.2 Directed Acyclic Graph\n\n\n\n\nflowchart LR\n  X[Predictor\\nof interest] --&gt; Y[Outcome]\n  W[Precision] --&gt; Y[Outcome]\n\n\n\n\n\n\n\n2.3 Example\n\n2.3.1 Data generation\n\nCategorical precision variable (W), Predictor of Interest (X), Outcome (Y)\n\n\n\nCode\n# Generate some data\nset.seed(1231)\nn &lt;- 200\nprecisiondata &lt;- \n  data.frame(W=rep(0:1, each=n/2),\n             X=runif(n),\n             Y=NA\n)\nprecisiondata$Y &lt;- 10 + 1.5*precisiondata$X + 10*precisiondata$W + rnorm(n,0,4)\nprecisiondata$W &lt;- factor(precisiondata$W)\n\n\n\n\n2.3.2 Bivariate plots\n\nW is uncorrelated with X, and associated with Y\n\n\n\nCode\nlibrary(ggplot2)\nggplot(precisiondata, aes(x=W,y=X)) + geom_boxplot()\n\n\n\n\n\nBox plot showing the (lack of) association between the precision variable (W) and the predictor of interest (X)\n\n\n\n\n\n\nCode\nggplot(precisiondata, aes(x=W,y=Y)) + geom_boxplot()\n\n\n\n\n\nBox plot showing the association between the precision variable (W) and the outcome (Y)\n\n\n\n\n\n\nCode\nggplot(precisiondata, aes(x=X,y=Y)) + geom_point() + geom_smooth(method=\"lm\")\n\n\n\n\n\nAssociation between the predictor of interest (X) and the outcome (Y) ignoring the precision variable (W)\n\n\n\n\n\n\n2.3.3 Bivariate plot by W\n\n\nCode\nggplot(precisiondata, aes(x=X,y=Y,color=W,group=W)) + geom_point() + geom_smooth(method=\"lm\")\n\n\n\n\n\nAssociation between the predictor of interest (X) and the outcome (Y) by the precision variable (W)\n\n\n\n\n\n\n2.3.4 Adjusted and unadjusted regression models\n\nUnadjusted: \\(E[Y|X] = \\beta_0 + \\beta_1*X\\)\nAdjusted: \\(E[Y|X] = \\beta_0 + \\beta_1*X + \\beta_2*W\\)\n\n\n\nCode\nsummary(lm(Y ~ X, data=precisiondata))\n\n\n\nCall:\nlm(formula = Y ~ X, data = precisiondata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.9532  -4.6804  -0.7675   5.5132  13.5264 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  13.8858     0.8822  15.740   &lt;2e-16 ***\nX             3.4619     1.5818   2.189   0.0298 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.227 on 198 degrees of freedom\nMultiple R-squared:  0.02362,   Adjusted R-squared:  0.01869 \nF-statistic:  4.79 on 1 and 198 DF,  p-value: 0.0298\n\n\nCode\nsummary(lm(Y ~ X + W, data=precisiondata))\n\n\n\nCall:\nlm(formula = Y ~ X + W, data = precisiondata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.2400 -2.8797  0.3465  2.3857 11.7172 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   9.1712     0.6417  14.292  &lt; 2e-16 ***\nX             3.4659     1.0294   3.367 0.000915 ***\nW1            9.4252     0.5731  16.446  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.052 on 197 degrees of freedom\nMultiple R-squared:  0.5885,    Adjusted R-squared:  0.5844 \nF-statistic: 140.9 on 2 and 197 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nCode\nmykable = function(x){\n  knitr::kable(x, row.names = FALSE, align = c(\"l\", \"l\", \"r\", \"r\", \"r\", \"r\", \"r\", \"r\", \"r\"),\n               booktabs=TRUE)\n}\n\nlibrary(finalfit)\nlibrary(dplyr)\n\nexplanatory = c(\"X\", \"W\")\ndependent = \"Y\"\n\nprecisiondata %&gt;% \n  finalfit(dependent, explanatory) %&gt;% mykable()\n\n\n\n\n\n\n\n\n\n\n\n\n\nDependent: Y\n\nunit\nvalue\nCoefficient (univariable)\nCoefficient (multivariable)\n\n\n\n\nX\n[0.0,1.0]\nMean (sd)\n15.6 (6.3)\n3.46 (0.34 to 6.58, p=0.030)\n3.47 (1.44 to 5.50, p=0.001)\n\n\nW\n0\nMean (sd)\n10.8 (4.1)\n-\n-\n\n\n\n1\nMean (sd)\n20.3 (4.2)\n9.42 (8.27 to 10.58, p&lt;0.001)\n9.43 (8.30 to 10.56, p&lt;0.001)"
  },
  {
    "objectID": "notes/Lec05.Precision.html#sampling-distributions",
    "href": "notes/Lec05.Precision.html#sampling-distributions",
    "title": "Precision of Statistical Intefence",
    "section": "3 Sampling Distributions",
    "text": "3 Sampling Distributions\n\nThe sampling distribution is the probability distribution of a statistic\n\ne.g. the sampling distribution of the sample mean is \\(N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\\)\n\nMost often we choose estimators that are asymptotically Normally distributed\n\nFor large \\(n\\), \\(\\hat{\\theta} \\sim N\\left(\\theta, \\frac{V}{n} \\right)\\)\n\\(\\hat{\\theta}\\) is our estimate of \\(\\theta\\). The \\(\\hat{ }\\) indicates it is an estimate.\nMean: \\(\\theta\\)\n\nVariance: \\(V\\), which is related to the “average amount of statistical information” available from each observation\n\nOften \\(V\\) depends on \\(\\theta\\)\n\n\n“Large” \\(n\\) depends on the distribution of the underlying data. If \\(n\\) is large enough, approximate Normality of \\(\\hat{\\theta}\\) will hold.\nCalculating \\(100 (1-\\alpha)\\%\\) confidence intervals \\(\\left(\\theta_L, \\theta_U \\right)\\) with approximate Normality\n\n\\(\\theta_L = \\hat{\\theta} - Z_{1-\\alpha/2} \\sqrt{\\frac{V}{n}}\\)\n\\(\\theta_U = \\hat{\\theta} + Z_{1-\\alpha/2} \\sqrt{\\frac{V}{n}}\\)\n(estimate) \\(\\pm\\) (crit val) \\(\\times\\) (std err of estimate)\n\nCan similarly calculate approximate two-sided \\(p\\)-values\n\n\\(Z = \\frac{\\textrm{(estimate)} - \\textrm{(hyp value)}}{\\textrm{(std err of estimate)}}\\)"
  },
  {
    "objectID": "notes/Lec05.Precision.html#precision-and-powersample-size",
    "href": "notes/Lec05.Precision.html#precision-and-powersample-size",
    "title": "Precision of Statistical Intefence",
    "section": "4 Precision and Power/Sample Size",
    "text": "4 Precision and Power/Sample Size\n\nWhat are the measures of (high) precision?\n\nEstimators are less variable across studies, which is often measured by decreased standard error.\nNarrower confidence intervals. Estimators are consistent with fewer hypotheses if the CIs are narrow.\nAble to reject false hypotheses. Z statistic is higher when the alternative hypothesis is true.\n\nTranslation into sample size\n\nBased on the width of the confidence interval\nChoose a sample size such that a 95% CI will not contain both the null and design alternative\nIf both \\(\\theta_0\\) and \\(\\theta_1\\) cannot be in the CI, we have discriminated between those hypotheses\n\nBased on statistical power\n\nWhen the alternative is true, have a high probability of rejecting the null\nIn other words, minimize the type II error rate\n\nStatistical power: Quick review\n\nPower is the probability of rejecting the null hypothesis when the alternative is true\nPr(reject \\(H_0 | \\theta = \\theta_1\\))\nMost often \\(\\hat{\\theta} \\sim N\\left(\\theta, \\frac{V}{n} \\right)\\) so that the test statistic \\(Z = \\frac{\\hat{\\theta} - \\theta_0}{\\sqrt{V/n}}\\) wll follow a Normal distribution\nUnder \\(H_0\\), \\(Z \\sim N(0, 1)\\) so we reject \\(H_0\\) if \\(|Z| &gt; Z_{1-\\alpha/2}\\)\nUnder \\(H_1\\), \\(Z \\sim N\\left(\\frac{\\theta_1 - \\theta_0}{\\sqrt{V/n}}, 1\\right)\\)\n\nPower curves\n\nThe power function (power curve) is a function of the true value of \\(\\theta\\)\nWe can compute power for every value of \\(\\theta\\)\nAs \\(\\theta\\) moves away from \\(\\theta_0\\), power increases (for two-sided alternatives)\nFor any choice of desired power, there is always some \\(\\theta\\) such that the study has that power\n\\(Pwr(\\theta_0) = \\alpha\\), the type I error rate\n\n\n\n\nCode\nmydiffs &lt;- seq(-.8,.8,.05)\n\nmypower &lt;- vector(\"numeric\", length(mydiffs))\nmypower2 &lt;- vector(\"numeric\", length(mydiffs))\n\nfor(i in 1:length(mydiffs)){\n   mypower[i] &lt;- power.t.test(n=100, sd=1, delta=mydiffs[i])$power\n   mypower2[i] &lt;- power.t.test(n=100, sd=1.2, delta=mydiffs[i])$power\n}\n\nplot(mydiffs, mypower, xlab=\"True difference in means (theta)\", ylab=\"Power\", type=\"l\", main=\"\")\nlines(mydiffs, mypower2, lty=2)\nlegend(\"top\", c(\"sigma = 1.0\",\"sigma = 1.2\"), lty=1:2, inset=0.05)\n\n\n\n\n\nPower curves for a two-sample, equal variance, t-test; n=100"
  },
  {
    "objectID": "notes/Lec05.Precision.html#precision-and-standard-errors",
    "href": "notes/Lec05.Precision.html#precision-and-standard-errors",
    "title": "Precision of Statistical Intefence",
    "section": "5 Precision and Standard Errors",
    "text": "5 Precision and Standard Errors\n\nStandard errors are the key to precision\nGreater precision is achieved with smaller standard errors\nStandard errors are decreased by either decreasing \\(V\\) or increasing \\(n\\)\n\nTypically: \\(se(\\hat{\\theta}) = \\sqrt{\\frac{V}{n}}\\)\nWidth of CI: \\(2 \\times (\\textrm{crit value}) \\times se(\\hat{\\theta})\\)\nTest statistic: \\(Z = \\frac{\\hat{\\theta} - \\theta_0}{se(\\hat{\\theta})}\\)\n\n\n\n5.1 Example: One sample mean\n\nObservations are independent and identically distributed (iid)\n\n\\(\\textrm{iid } Y_i \\sim (\\mu, \\sigma^2), i = 1, \\ldots, n\\)\n\\(\\theta = \\mu\\), \\(\\hat{\\theta} = \\frac{1}{n} \\displaystyle \\sum_{i=1}^n Y_i = \\overline{Y}\\)\n\\(V = \\sigma^2\\), \\(se(\\hat{\\theta}) = \\sqrt{\\frac{\\sigma^2}{n}}\\)\n\nNote that we are not assuming a specific distribution for \\(Y_i\\), just that the distribution has a mean and variance\nWe are assuming that \\(n\\) is large so asymptotic results are applicable\n\nThen the distribution \\(Y_i\\) could be binary data, Poisson, exponential, normal, etc. and the results will hold\n\nThere are ways to decrease \\(V\\) including...\n\nRestrict sample by age, gender, etc.\nTake repeated measures on each subject, summarize, and perform test on summary measures\nBetter ideas (this course)\n\nAdjust for age and gender rather than restrict\nUse all repeated observations and modeling correlation\n\n\n\n\n\n5.2 Example: Two sample mean\n\nDifference of independent means\nObservations no longer identically distributed, just independent. Group 1 has a different mean and variance than group 2\n\n\\(\\textrm{ind } Y_{ij} \\sim (\\mu_j, \\sigma_j^2), j = 1, 2; i = 1, \\ldots, n_j\\)\n\\(n = n_1 + n_2\\); \\(r = n_1 / n_2\\)\n\\(\\theta = \\mu_1 - \\mu_2\\), \\(\\hat{\\theta} = \\overline{Y}_1 - \\overline{Y}_2\\)\n\\(V = (r+1)(\\frac{\\sigma_1^2}{r} + \\sigma_2^2)\\)\n\\(se(\\hat{\\theta}) = \\sqrt{\\frac{V}{n}} = \\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}\\)\n\n\n\n\n5.3 Comments on the optimal ratio of sample sizes (\\(r\\))\n\nIf we are constrained by the maximal sample size \\(n = n_1 + n_2\\)\n\nSmallest \\(V\\) when \\(r = \\frac{n_1}{n_2} = \\frac{\\sigma_1}{\\sigma_2}\\)\nIn other words, smaller \\(V\\) if we sample more subjects from the more variable group\n\nIf we are unconstrained by the maximal sample size, there is a point of diminishing returns\nExample: Case-control study where finding cases is difficult/expensive but finding controls is easy/cheap\n\nOften quoted little benefit beyond \\(r = 5\\). That is, little benefit in having more than 5 times as many controls as cases\n\n\n\n\nCode\nvar.fn &lt;- function(r, s1, s2) {\n  (r + 1) * (s1^2/r + s2^2)\n}\n\n# Optimal sample size ratio for fixed sample size\n\nn &lt;- 100\ns2 &lt;- 10\n\nplot(function(r) sqrt(var.fn(r, s1=s2, s2=s2) / n), 0, 20, ylim=c(1,6), xlim=c(0,25), ylab=\"Standard Error\", xlab=\"Sample Size Ratio r = n1/n2\")\nplot(function(r) sqrt(var.fn(r, s1=2*s2, s2=s2) / n), 0, 20, add=TRUE, lty=2)\nplot(function(r) sqrt(var.fn(r, s1=3*s2, s2=s2) / n), 0, 20, add=TRUE, lty=3)\ntext(20,4.7,\"s1 = s2\", pos=4)\ntext(20,5.1,\"s1 = 2*s2\", pos=4)\ntext(20,5.5,\"s1 = 3*s2\", pos=4)\npoints(c(1,2,3), sqrt(var.fn(c(1,2,3), s1=c(1,2,3)*s2, s2=s2)/ n), pch=2)\ntext(1, 1.8, \"r = 1\")\ntext(2, 2.8, \"r = 2\")\ntext(3, 3.8, \"r = 3\")\n\n\n\n\n\nOptimal r for Fixed (n1 + n2). r = s1 / s2, the ratio of standard deviations in the two group.\n\n\n\n\n\n\nCode\nn1 &lt;- 200\n\nplot(function(r) sqrt(var.fn(r, s1=s2, s2=s2) / (n1 + r*n1)), 0, 20, ylim=c(0.5,3), xlim=c(0,25), ylab=\"Standard Error\", xlab=\"Sample Size Ratio r = n1/n2\")\nplot(function(r) sqrt(var.fn(r, s1=2*s2, s2=s2) / (n1 + r*n1)), 0, 20, add=TRUE, lty=2)\nplot(function(r) sqrt(var.fn(r, s1=3*s2, s2=s2) / (n1 + r*n1)), 0, 20, add=TRUE, lty=3)\n\ntext(20,.7,\"s1 = s2\", pos=4)\ntext(20,.8,\"s1 = 2*s2\", pos=4)\ntext(20,.9,\"s1 = 3*s2\", pos=4)\n\n\n\n\n\nDiminishing returns for r &gt; 5. s1 and s2 are the standard deviations is group 1 and group 2, respectively. s1=s2 indicates the two groups have equal variablility. Other conditions represent the cases where the group we are oversampling has more variability.\n\n\n\n\n\n\n5.4 Example: Paired means\n\nDifference of paired means\nNo longer iid. Group 1 has a different mean and variance than group 2, and observations are paired (correlated)\n\n\\(Y_{ij} \\sim (\\mu_j, \\sigma_j^2), j = 1, 2; i = 1, \\ldots, n\\)\n\\(corr(Y_{i1}, Y_{i2}) = \\rho; corr(Y_{ij}, Y_{mk}) = 0 \\textrm{ if } i \\neq m\\)\n\\(\\theta = \\mu_1 - \\mu_2\\), \\(\\hat{\\theta} = \\overline{Y}_1 - \\overline{Y}_2\\)\n\\(V = \\sigma_1^2+ \\sigma_2^2 - 2 \\rho \\sigma_1 \\sigma_2\\)\n\\(se(\\hat{\\theta}) = \\sqrt{\\frac{V}{n}}\\)\n\nPrecision gains are made when matched observations are positively correlated (\\(\\rho &gt; 0\\))\n\nUsually the case, but possible exceptions\n\nSleep on successive nights\nIntrauterine growth of litter-mates\n\n\n\n\n\n5.5 Example: Clustered data\n\nClustered data: Experiment where treatments/interventions are assigned based on the basis of “clusters”\n\nHouseholds\nSchools\nClinics\nCities\n\nMean of clustered data\n\n\\(Y_{ij} \\sim (\\mu, \\sigma^2), i = 1, \\ldots, n; j = 1, \\ldots, m\\)\n\nUp to \\(n\\) clusters, each of which have \\(m\\) subjects\n\n\\(corr(Y_{ij}, Y_{ik}) = \\rho \\textrm{ if } j \\neq k\\)\n\\(corr(Y_{ij}, Y_{mk}) = 0 \\textrm{ if } i \\neq m\\)\n\n\\(\\theta = \\mu\\), \\(\\hat{\\theta} = \\frac{1}{nm} \\displaystyle \\sum_{i=1}^{n} \\sum_{j=1}^m Y_{ij} = \\overline{Y}\\)\n\\(V = \\sigma^2 \\left(\\frac{1 + (m-1)\\rho}{m} \\right)\\)\n\\(se(\\hat{\\theta}) = \\sqrt{\\frac{V}{n}}\\)\n\nWhat is V if ...\n\n\\(\\rho = 0\\) (independent)\n\\(m = 1\\)\n\\(m\\) is large (e.g \\(m = 1000\\)) and \\(\\rho\\) is 0, 1, or 0.01\nWith clustered data, even small correlations can be very important to consider\n\nEqual precision achieved with\n\n\n\n\n\n\nClusters (\\(n\\))\n\\(m\\)\n\\(\\rho\\)\nTotal N\n\n\n\n\n1000\n1\n0.01\n1000\n\n\n650\n2\n0.30\n1300\n\n\n550\n2\n0.10\n1100\n\n\n190\n10\n0.10\n1900\n\n\n109\n10\n0.01\n1090\n\n\n20\n100\n0.01\n2000\n\n\n\n\n\nAlways consider practical issues. Is it easier/cheaper to collect 1 observation on 1000 different subjects (\\(n=1000\\), \\(m=1\\)), or 100 observations on 20 different subjects (\\(n=20\\), \\(m=100\\))? These two designs have equal precision.\n\n\n\n5.6 Example: Independent Odds Ratios\n\nBinary outcomes\n\\(\\textrm{ind } Y_{ij} \\sim B(1, p_j), i = 1, \\ldots, n_j; j = 1, 2\\)\n\\(n = n_1 + n_2; r = n_1 / n_2\\)\n\\(\\theta = \\textrm{log}\\left(\\frac{p_1/(1-p_1)}{p_2/(1-p_2)} \\right)\\); \\(\\hat{\\theta} = \\textrm{log}\\left(\\frac{\\hat{p}_1/(1-\\hat{p}_1)}{\\hat{p}_2/(1-\\hat{p}_2)} \\right)\\)\n\\(\\sigma^2_j = \\frac{1}{p_j(1-p_j)} = \\frac{1}{p_j(q_j)}\\)\n\\(V = (r+1)(\\frac{\\sigma_1^2}{r} + \\sigma_2^2)\\)\n\\(se(\\hat{\\theta}) = \\sqrt{\\frac{V}{n}} = \\sqrt{\\frac{1}{n_1 p_1 q_1} + \\frac{1}{n_2 p_2 q_2}}\\)\nNotes on maximum precision\n\nMax precision is achieved when the underlying odds are near 1 (proportions near 0.5)\nIf we were considering differences in proportions, the max precision is achieved when the underlying proportions are near 0 or 1\n\n\n\n\n5.7 Example: Hazard Ratios\n\nIndependent censored time to event outcomes\n\n\\((T_{ij}, \\delta_{ij}), i = 1, \\ldots, n_j; j = 1, 2\\)\n\\(n = n_1 + n_2; r = n_1 / n_2\\)\n\\(\\theta = \\textrm{log(HR)}\\); \\(\\hat{\\theta} = \\hat{\\beta}\\) from proportional hazards (PH) regression\n\\(V = \\frac{(r+1)(1/r+1)}{\\textrm{Pr}(\\delta_{ij} = 1)}\\)\n\\(se(\\hat{\\theta}) = \\sqrt{\\frac{V}{n}} = \\sqrt{\\frac{(r+1)(1/r+1)}{d}}\\)\n\nIn the PH model, statistical information is roughly proportional to \\(d\\), the number of observed events\n\nPapers always report the number of events\nStudy design must consider how long it will take to observe events (e.g. deaths) starting from randomization\n\n\n\n\n5.8 Example: Linear Regression\n\nIndependent continuous outcomes associated with covariates\n\n\\(\\textrm{ind } Y_i | X_i ~ \\sim(\\beta_0 + \\beta_1 X_i, \\sigma^2_{Y|X}), i = 1, \\ldots, n\\)\n\\(\\theta = \\beta_1, \\hat{\\theta} = \\hat{\\beta_1}\\) from LS regression\n\\(V = \\frac{\\sigma^2_{Y|X}}{\\textrm{Var}(X)}\\)\n\\(se(\\hat{\\theta}) = \\sqrt{\\frac{\\hat{\\sigma}^2_{Y|X}}{n \\hat{\\textrm{Var}}(X)}}\\)\n\nPrecision tends to increases as the predictor (\\(X\\)) is measured over a wider range\nPrecision also related to the within group variance \\(\\sigma^2_{Y|X}\\)\n\nWhat happens to the formulas when \\(X\\) is a binary variable? See two sample mean"
  },
  {
    "objectID": "notes/Lec05.Precision.html#summary-and-concluding-comments",
    "href": "notes/Lec05.Precision.html#summary-and-concluding-comments",
    "title": "Precision of Statistical Intefence",
    "section": "6 Summary and Concluding Comments",
    "text": "6 Summary and Concluding Comments\n\nOptions for increasing precision\n\nIncrease sample size\nDecrease \\(V\\)\n(Decrease confidence level)\n\nCriteria for precision\n\nStandard error\nWidth of confidence intervals\nStatistical power\n\nSelect a suitable, scientifically meaningful alternative\n\nWith proper precision, can distinguish between minor and meaningful effects\n\nSelect desired power\nSample size calculation: The number of sampling units needed to obtain the desired precision\n\nLevel of significance \\(\\alpha\\) when \\(\\theta = \\theta_0\\)\nPower \\(\\beta\\) when \\(\\theta = \\theta_1\\)\nVariability \\(V\\) within one sampling unit\n\\(n = \\frac{(z_{1-\\alpha/2} + z_\\beta)^2 \\times V}{(\\theta_1 - \\theta_0)^2}\\)\n\nWhen sample size is constrained (the usual case) either\n\nCompute power to detect a specified alternative\n\n\\(1 - \\beta = \\phi \\left(\\frac{(\\theta_1-\\theta_0)}{\\sqrt{V/n}} - z_{1-\\alpha/2} \\right)\\)\n\\(\\phi\\) is the standard Normal cdf function\nIn STATA, use normprob for the \\(\\phi\\) function\n\nCompute alternative that can be detected with high power\n\n\\(\\theta_1 = \\theta_0 + (z_{1-\\alpha/2} + z_\\beta) \\sqrt{V/n}\\)\n\n\n\n\n6.1 General Comments\n\nSample size required behaves like the square of the width of the CI. To cut the width of the CI in half, need to quadruple the sample size.\nPositively correlated observations within the same group provide less precision than the same number of independent observations\nPositively correlated observations across groups provide more precision\nWhat power do you use?\n\nMost popular is 80% (too low) or 90%\nKey is to be able to discriminate between scientifically meaningful hypotheses"
  },
  {
    "objectID": "notes/Lec01.Introduction.html#course-team",
    "href": "notes/Lec01.Introduction.html#course-team",
    "title": "Welcome & Syllabus",
    "section": "Course Team",
    "text": "Course Team\nInstructor\n\nChris Slaughter\n\njames.c.slaughter@vumc.org (best)\njames.c.slaughter@vanderbilt.edu (checked ~ weekly)\nOffice: 2525 West End, 11-127\n\n\n\nTA\n\nBailu (Lucy) Yan\n\nbailu.yan@vanderbilt.edu"
  },
  {
    "objectID": "notes/Lec01.Introduction.html#course-websites",
    "href": "notes/Lec01.Introduction.html#course-websites",
    "title": "Welcome & Syllabus",
    "section": "Course website(s)",
    "text": "Course website(s)\n\nhttps://chrissl789.github.io/modern-regression/\n\nCourse notes\n\nHTML, PDF, Quarto (.qmd)\n\nDaily schedule\nLinks to course tools\n\nBrightspace\n\nAnnouncements\nSubmit homework\nGradebook\nFERPA compliance\n\nThe Family Educational Rights and Privacy Act (FERPA) is a federal law enacted in 1974 that protects the privacy of student education records."
  },
  {
    "objectID": "notes/Lec01.Introduction.html#course-timetable",
    "href": "notes/Lec01.Introduction.html#course-timetable",
    "title": "Welcome & Syllabus",
    "section": "Course Timetable",
    "text": "Course Timetable\n\nLectures (weekly)\n\nTuesdays, 10:30 - 11:50 am - 2525 WEA, Room 11105\nThursdays, 10:30 - 11:50 am - 2525 WEA, Room 11105\n\n\n\n\nLabs (weekly)\n\nMondays, 1:00 to 1:50 pm - 2525 WEA, Room 11105"
  },
  {
    "objectID": "notes/Lec01.Introduction.html#software-r",
    "href": "notes/Lec01.Introduction.html#software-r",
    "title": "Welcome & Syllabus",
    "section": "Software – R",
    "text": "Software – R\n\nPrimary software package will be R\n\nFree\nMost adaptable statistical package\nUsed by the majority of academic Biostatisticians\nRstudio IDE used to work with R\nRstudio includes Quarto, the latest and greatest tool for rendering markdown documents"
  },
  {
    "objectID": "notes/Lec01.Introduction.html#software-rstudio-and-quarto",
    "href": "notes/Lec01.Introduction.html#software-rstudio-and-quarto",
    "title": "Welcome & Syllabus",
    "section": "Software – Rstudio and Quarto",
    "text": "Software – Rstudio and Quarto\n\nCourse notes rendered using Quarto with R code integrated\n\nQuarto allows for creating dynamic content using embedded R (or other software) code\nWill allow you to see all code used to generate notes\n\nI will highlight important R code\n\nI am still learning Quarto, so it has more features than I will likely use\n\nQuarto introduction covered in first lab"
  },
  {
    "objectID": "notes/Lec01.Introduction.html#software-stata",
    "href": "notes/Lec01.Introduction.html#software-stata",
    "title": "Welcome & Syllabus",
    "section": "Software – Stata",
    "text": "Software – Stata\n\nSecondary software package will be Stata\n\nMy second favorite statistical package\nRelatively easy syntax\nDesigned for people who know statistics, but don’t want to write basic functions\nNot integrated with Quarto\n\nI will provide a list of equivalent Stata commands in labs and lectures, but usually not produce the output"
  },
  {
    "objectID": "notes/Lec01.Introduction.html#generative-ai-e.g.-chatgpt",
    "href": "notes/Lec01.Introduction.html#generative-ai-e.g.-chatgpt",
    "title": "Welcome & Syllabus",
    "section": "Generative AI (e.g. ChatGPT)",
    "text": "Generative AI (e.g. ChatGPT)\n\nTwo definitions of generative AI, from ChatGPT 3.5\n\nGenerative AI is a type of artificial intelligence that can create new, original content by learning patterns from existing data and using them to generate new outputs.\nGenerative AI refers to the use of machine learning algorithms to generate new and original content, such as images, text, and music.\n\nI prefer to use the tools available to VUMC\nAs with any tool, use with caution\n\nHelp with generating code, or converting code between languages\nHelp with writing interpretations in more easily understood language\nThe prompt that you use matters. Also, identical prompts at different times can give different answers"
  },
  {
    "objectID": "notes/Lec01.Introduction.html#guiding-principles",
    "href": "notes/Lec01.Introduction.html#guiding-principles",
    "title": "Welcome & Syllabus",
    "section": "Guiding Principles",
    "text": "Guiding Principles\n\nAlternative course title: How to Use Statistics to Answer Scientific Questions (part 2)\n\nPut science before statistics\nEmphasize parameter estimates and confidence intervals (credible intervals, likelihood intervals) over hypothesis testing and p-values\n\nThe End of Statistical Significance (Jonathan Sterne)\nWhat’s Wrong with P-Values (Bland, Altman)\nKey difference between scientific/clinical significance and statistical significance\n\n\nThis is a course in Biostatistics, not coding in R/Stata\n\nI will show you how to get an interpret the key statistics, but not interpret every number on output\nOften there is more than one way to arrive at identical final answers"
  },
  {
    "objectID": "notes/Lec01.Introduction.html#example-clinical-vs-scientific-significance",
    "href": "notes/Lec01.Introduction.html#example-clinical-vs-scientific-significance",
    "title": "Welcome & Syllabus",
    "section": "Example: Clinical vs Scientific significance",
    "text": "Example: Clinical vs Scientific significance\n\n5 clinical trials conducted to determine if drug A, B, or C lowers cholesterol\nAssume that a decrease of 10 mg/dl or more is important to clinicians\nStudy design\n\nCholesterol measured at baseline, subjects take drug for 1 month, Cholesterol measured at 1 month\nChange in cholesterol is the outcome of interest\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrial\nDrug\nPts\nMean diff\nStd dev\nStd error\n95% CI for diff\np-value\n\n\n\n\n1\nA\n30\n-30\n191.7\n49.5\n[-129, 69]\n0.55\n\n\n2\nA\n1000\n-30\n223.6\n10\n[-49.6, -10.4]\n0.002\n\n\n3\nB\n40\n-20\n147.6\n33\n[-85, 45]\n0.55\n\n\n4\nB\n4000\n-2\n147.6\n3.3\n[-8.5, 4.5]\n0.54\n\n\n5\nC\n5000\n-6\n100.0\n2\n[-9.9, -2.1]\n0.002"
  },
  {
    "objectID": "notes/Lec01.Introduction.html#evaluation-components-and-grade-percentages",
    "href": "notes/Lec01.Introduction.html#evaluation-components-and-grade-percentages",
    "title": "Welcome & Syllabus",
    "section": "Evaluation components and grade percentages",
    "text": "Evaluation components and grade percentages\n\nMidterm (25%)\nTake Home Exam (25%)\nFinal Exam (25%)\nHomework (25%)\nClass participation\nThis is a 4-credit course. Your lab and lecture grades will be the same"
  },
  {
    "objectID": "notes/Lec01.Introduction.html#homework",
    "href": "notes/Lec01.Introduction.html#homework",
    "title": "Welcome & Syllabus",
    "section": "Homework",
    "text": "Homework\n\nUp to 1 per week (probably 6 or 7 total)\nWill focus on real data analysis and interpretation with some mathematical derivations of important quantities\nQuestions will focus on specific analyses, with questions stated in as scientific terms as possible\nWork handed in should address the scientific questions\n\nFormat Table and Figures\n\nKeys will be provided shortly after the homework is turned in\n\nNo late homework accepted after the key is posted\n\nAnswers in keys may go beyond what is expected of your homework and present concepts in more detail. You are responsible for any material in the keys for exams.\nYou may discuss the homework with others in the class, but the work you turn in should be your own\nUse Brightspace to turn in homeworks and receive feedback and grade"
  },
  {
    "objectID": "notes/Lec01.Introduction.html#in-class-exams",
    "href": "notes/Lec01.Introduction.html#in-class-exams",
    "title": "Welcome & Syllabus",
    "section": "In Class Exams",
    "text": "In Class Exams\n\nMidterm and Final in class\n\nFocus on understanding concepts, not memorizing formulas\nI will provide an example midterm and final\nFor midterm, you will be allowed 1 page of your own notes\nFor final, you will be allowed 2 pages of your own notes\n\nAll output will be provide for you to interpret"
  },
  {
    "objectID": "notes/Lec01.Introduction.html#take-home-exam",
    "href": "notes/Lec01.Introduction.html#take-home-exam",
    "title": "Welcome & Syllabus",
    "section": "Take Home Exam",
    "text": "Take Home Exam\n\nWill be given approximately mid point between Midterm and Final\nDemonstrate ability to obtain results through software and interpret findings\nOne day to complete and turn in\n\nLikely will be a Monday with no lab scheduled for that day\n\nPractice for applied portion of first year comprehensive exams\nSimilar to Homework, but work should be your own"
  },
  {
    "objectID": "notes/Lec01.Introduction.html#course-notes",
    "href": "notes/Lec01.Introduction.html#course-notes",
    "title": "Welcome & Syllabus",
    "section": "Course notes",
    "text": "Course notes\n\nCourse notes will be the primary source\nAvailable on web page\nDaily class schedule will indicate notes being covered\nNotes will be updated throughout semester"
  },
  {
    "objectID": "notes/Lec01.Introduction.html#recommended-textbook-regression-methods-in-biostatistics",
    "href": "notes/Lec01.Introduction.html#recommended-textbook-regression-methods-in-biostatistics",
    "title": "Welcome & Syllabus",
    "section": "Recommended Textbook: Regression Methods in Biostatistics",
    "text": "Recommended Textbook: Regression Methods in Biostatistics\n\nVittinghoff, Glidden, Shiboski, and McCulloch\nPros\n\nCovers applied linear, logistic, survival, and repeated measures regression\nBiomedical orientation\n\nCons\n\nDoes not discuss robust standard errors, which I will use almost exclusively\nEmphasizes model diagnostics more that I will\nDoes not cover Bayesian approaches\nDoes not cover matrix algebra\nLittle theory and mathematical derivation"
  },
  {
    "objectID": "notes/Lec01.Introduction.html#additional-textbooks",
    "href": "notes/Lec01.Introduction.html#additional-textbooks",
    "title": "Welcome & Syllabus",
    "section": "Additional textbooks",
    "text": "Additional textbooks\n\nWeisberg, Applied Linear Regression\n\nUses matrix notation and appropriate level of statistical theory for this course\nPoor for applied data analysis. Earlier version limited to linear models, no Bayesian approaches. Newer version is more comprehensive.\n\nWakefield, Bayesian and Frequentist Regression Methods\n\nNewer text that I am still evaluating\nProvides a Bayesian approach to fitting the models we discuss\n\nI will give recommended readings throughout the semester\nIf the course notes and book differ, go with the notes"
  },
  {
    "objectID": "notes/Lec01.Introduction.html#supplemental-material",
    "href": "notes/Lec01.Introduction.html#supplemental-material",
    "title": "Welcome & Syllabus",
    "section": "Supplemental Material",
    "text": "Supplemental Material\n\nAs needed, I will post supplemental material on the course web page or on Brightspace\n\nI prefer the course web page, if possible\nBrightspace will be used if necessary\n\nRelevant supplemental material will be noted on course schedule"
  },
  {
    "objectID": "notes/Lec01.Introduction.html#getting-started",
    "href": "notes/Lec01.Introduction.html#getting-started",
    "title": "Welcome & Syllabus",
    "section": "Getting started",
    "text": "Getting started\n\nTo do on your own by Thursday…\n\nDownload and install new versions of R and Rstudio\n\nDetails on course web page, computing\n\n\nI would like everyone (including Stata users) to be able to Render a basic Quarto document\nPlan to start discussing Quarto next Tuesday\n\nIt is difficult to anticipate all issues that might arise, so I would like to resolve any major software issues before then\n\n\n\n\nBios 6312 - Spring 2024"
  },
  {
    "objectID": "notes/Lec03.SimpleLinearRegression.html",
    "href": "notes/Lec03.SimpleLinearRegression.html",
    "title": "Simple Linear Regression",
    "section": "",
    "text": "Code\nlibrary(rstanarm)\nlibrary(bayesplot)\nlibrary(finalfit)\nlibrary(dplyr)\nlibrary(rms)\nlibrary(ggplot2)\nlibrary(gglm)"
  },
  {
    "objectID": "notes/Lec03.SimpleLinearRegression.html#general-regression-setting",
    "href": "notes/Lec03.SimpleLinearRegression.html#general-regression-setting",
    "title": "Simple Linear Regression",
    "section": "1 General Regression Setting",
    "text": "1 General Regression Setting\n\n1.1 Two variable setting\n\nMany statistical problems examine the association between two variables\n\nOutcome variable (response variable, dependent variable)\nGrouping variable (covariate, predictor variable, independent variable)\n\nGeneral goal is to compare distribution of the outcome variable across levels of the grouping variable\nGroups are defined by the grouping variable\n\nWithin each group, the grouping variable is constant\nTreatment and control groups\nAge: Continuous (days), broadly categorized (years), or wide categories (e.g. 0-18, 18-65, 65+)\n\nIn intro course, statistical analysis is characterized by two factors\n\nNumber of groups (samples)\nIf subjects in groups are independent\n\nIn the two variable setting, statistical analysis is more generally characterized by the grouping variable. If the grouping variable is\n\nConstant: One sample problem\nBinary: Two sample problem\nCategorical: \\(k\\) sample problem (e.g. ANOVA)\nContinuous: Infinite sample problem (analyzed with regression)\n\nRegression thus extends the one- and two-sample problems up to infinite sample problems\n\nOf course, in reality we never have infinite samples, but models that can handle this case are the ultimate generalization\nContinuous predictors of interest\nContinuous adjustment variables\n\n\n\n\nCode\n  ### Make a cholesterol and age dataframe.  Set the random number seed so everything is reproducible\nset.seed(19)\n\nplotdata &lt;- data.frame(age=c(63, 63, rep(65:80,15), rep(81:85,10), 86,86,86, 87,89, 90, 93, 95, 100),\n                       chol=NA)\nplotdata$chol &lt;- 190 + .5*plotdata$age + rnorm(length(plotdata$age), 0, 15)\n\nggplot(plotdata, aes(x=age, y=chol)) + geom_point() + ylab(\"Cholesterol\") + xlab(\"Age (years)\") + theme_bw()\n\n\n\n\n\nFigure 1: Scatterplot of cholesterol and age. Age is a continuous grouping variable.\n\n\n\n\n\n\n1.2 Regression versus two sample approaches\n\nWith a binary grouping variable, regression models reduce to the corresponding two variable methods\nLinear regression with a binary predictor\n\nt-test, equal variance: Classic linear regression\nt-test, unequal variance: Linear regression with robust standard errors (approximately)\n\nLogistic regression with a binary predictor\n\n(Pearson) Chi-squared test: Score test from logistic regression\n\nProportional odds regression with a binary predictor\n\nWilcoxon Rank Sum Test\n\nCox (proportional hazards) regression with a binary predictor\n\nLog-rank test: Score test from Cox regression\n\n\n\n\n1.3 Guiding principle\n\nEverything can be recast as a regression problem."
  },
  {
    "objectID": "notes/Lec03.SimpleLinearRegression.html#motivating-problem-cholesterol-and-age",
    "href": "notes/Lec03.SimpleLinearRegression.html#motivating-problem-cholesterol-and-age",
    "title": "Simple Linear Regression",
    "section": "2 Motivating Problem: Cholesterol and Age",
    "text": "2 Motivating Problem: Cholesterol and Age\n\n2.1 Definitions\n\nIs there an association between cholesterol and age?\nScientific question: Does aging effect cholesterol?\nStatistical question: Does the distribution of cholesterol differ across age groups?\n\nAcknowledges variability in the response (cholesterol)\nAcknowledges cause-effect relationship is uncertain\nAssociation does not imply causation\n\nAny observed differences could be due to other factors. For example, the association could be with year of birth rather than age.\n\n\nContinuous response variable: Cholesterol\nContinuous grouping variable (predictor of interest): Age\n\nAn infinite number of ages are possible\nWe will not sample every possible age, and may round for simplicity\n\n\n\n\n2.2 Simple Regression Model\n\nAttempt to answer scientific question by assessing linear trends in average cholesterol\nEstimate the best fitting line to average cholesterol within age groups\n\n\\[\nE[\\textrm{Chol} | \\textrm{Age}] = \\beta_0 + \\beta_1 \\times \\textrm{Age}\n\\]\n\nThe expected value of cholesterol given age is modeled using an intercept (\\(\\beta_0\\)) and slope (\\(\\beta_1\\))\n\nAn association exists if the slope is nonzero\nA non-zero slope indicates that the average cholesterol will be different across different age groups\n\n\n\n2.2.1 Descriptive plots of cholesterol and age\n\n\nCode\nggplot(plotdata, aes(x=age, y=chol)) + geom_point() + ylab(\"Cholesterol\") + xlab(\"Age (years)\") + theme_bw() + geom_smooth()\n\n\n\n\n\nFigure 2: Scatterplot of cholesterol and age. Association is described using a lowess smooth line.\n\n\n\n\n\n\nCode\nggplot(plotdata, aes(x=age, y=chol)) + geom_point() + ylab(\"Cholesterol\") + xlab(\"Age (years)\") + theme_bw() + geom_smooth(method=\"lm\")\n\n\n\n\n\nFigure 3: Scatterplot of cholesterol and age. Association is described using a linear trend line.\n\n\n\n\n\n\n2.2.2 Simple linear regression model\n\nThe simple regression model produces an easy to remember (but approximate) rule of thumb.\n\n“Normal cholesterol is 190 plus half your age”\n\\(E[\\textrm{Chol} | \\textrm{Age}] = 190 + 0.50 \\times \\textrm{Age}\\)\nNote that data were generated using this model. Estimates, below, will be different.\n\n\n\n\n2.2.3 Frequentist Estimates and Interpretation\n\nRaw, unformatted estimates, model summary, and confidence intervals for age\n\n\n\nCode\nm.chol &lt;- lm(chol ~ age, data=plotdata)\nsummary(m.chol)\n\n\n\nCall:\nlm(formula = chol ~ age, data = plotdata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-39.591 -10.524  -0.234  11.154  41.819 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 203.2259    10.3138  19.704   &lt;2e-16 ***\nage           0.3209     0.1375   2.333   0.0203 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15.34 on 299 degrees of freedom\nMultiple R-squared:  0.01788,   Adjusted R-squared:  0.0146 \nF-statistic: 5.444 on 1 and 299 DF,  p-value: 0.0203\n\n\nCode\nconfint.default(m.chol, \"age\") # Based on asymptotic Normality\n\n\n       2.5 %    97.5 %\nage 0.051334 0.5904842\n\n\n\\(E[\\textrm{Chol} | \\textrm{Age}] = 203.2 + 0.32 \\times \\textrm{Age}\\)\n\nUsing finalfit to create a nicer output table of the slope coefficient for age and confidence intervals\n\n\n\nCode\nmykable = function(x){\n  knitr::kable(x, row.names = FALSE, align = c(\"l\", \"l\", \"r\", \"r\", \"r\", \"r\", \"r\", \"r\", \"r\"),\n               booktabs=TRUE)\n}\n\n\nexplanatory = c(\"age\")\ndependent = 'chol'\n\nlabel(plotdata$chol) &lt;- \"Cholesterol\"\nlabel(plotdata$age) &lt;- \"Age (years)\"\nplotdata %&gt;% \n  finalfit(dependent, explanatory) %&gt;% mykable()\n\n\n\n\n\n\n\n\n\n\n\n\n\nDependent: Cholesterol\n\nunit\nvalue\nCoefficient (univariable)\nCoefficient (multivariable)\n\n\n\n\nAge (years)\n[63.0,100.0]\nMean (sd)\n227.2 (15.5)\n0.32 (0.05 to 0.59, p=0.020)\n0.32 (0.05 to 0.59, p=0.020)\n\n\n\n\n\n\n\n2.2.4 Bayesian Estimates and Interpretation\n\nBayesian approach to the linear model requires specifying\n\nThe model, e.g. a linear model with intercept and slope for age, Normally distributed errors and constant variance\nPrior distributions on parameters\n\nFor the simple linear regression model, we have parameters \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\sigma\\).\nFor now, we will use default prior distributions that are are intended to be weakly informative in that they provide moderate regularization and help stabilize computation. See the STAN documentation for more details\nAppropriate priors can be based on scientific considerations\nSensitivity analyses can evaluate the the robustness of finding to different prior assumptions\n\n\nOutput from Bayesian linear regression\n\n\n\nCode\nfit1 &lt;- stan_glm(chol ~ age,\n                 data=plotdata, family=gaussian(),\n                 seed=1234,\n                 refresh=0)\nsummary(fit1, digits=2, prob=c(.025, .5, .975))\n\n\n\nModel Info:\n function:     stan_glm\n family:       gaussian [identity]\n formula:      chol ~ age\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 301\n predictors:   2\n\nEstimates:\n              mean   sd     2.5%   50%    97.5%\n(Intercept) 203.28  10.69 182.56 203.18 224.80 \nage           0.32   0.14   0.04   0.32   0.60 \nsigma        15.39   0.63  14.25  15.37  16.74 \n\nFit Diagnostics:\n           mean   sd     2.5%   50%    97.5%\nmean_PPD 227.19   1.24 224.79 227.19 229.63 \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.18 1.00 3721 \nage           0.00 1.00 3735 \nsigma         0.01 1.00 3469 \nmean_PPD      0.02 1.00 3911 \nlog-posterior 0.03 1.00 1638 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\n\n\nAnd a summary of the prior distributions used\n\n\n\nCode\nprior_summary(fit1, digits = 2)\n\n\nPriors for model 'fit1' \n------\nIntercept (after predictors centered)\n  Specified prior:\n    ~ normal(location = 227, scale = 2.5)\n  Adjusted prior:\n    ~ normal(location = 227, scale = 39)\n\nCoefficients\n  Specified prior:\n    ~ normal(location = 0, scale = 2.5)\n  Adjusted prior:\n    ~ normal(location = 0, scale = 6)\n\nAuxiliary (sigma)\n  Specified prior:\n    ~ exponential(rate = 1)\n  Adjusted prior:\n    ~ exponential(rate = 0.065)\n------\nSee help('prior_summary.stanreg') for more details\n\n\n\nInterpretation\n\nSlope for age is of primary scientific importance\nA priori we assume that no association between age and cholesterol. Specifically, we assumed a Normal prior with location (mean) of 0 and scale (standard devation) of 6 for \\(\\beta_1\\).\nConditional on the data, we estimate that for every 1 year increase in age, cholesterol increases by 0.32 mg/dl (95% credible interval 0.06 to 0.58).\n\n\n\n\n2.2.5 Comparison of Frequentist and Bayesian interpretation of the slope\n\nFollowing was given by gpt-4.0 1\n\n1 Query “Explain how a slope from simple linear regression is interpreted using language a college graduate could understand. Provide a Bayesian and Frequentist interpretation.” VUMC aiChat tool, GPT-4. Retrieved 12/28/2023.\nIn simple linear regression, the slope (often denoted as “b” or “beta”) is interpreted as the expected change in the dependent (or response) variable for a one-unit increase in the independent (or predictor) variable, assuming all other variables are held constant.\nFrom a Frequentist perspective, the slope is considered a fixed but unknown value which is estimated from the sample data. If the slope is found to be 2, for instance, it would suggest that for every one-unit increase in our predictor variable, we expect, on average, a 2-unit increase in the response variable. This is based on the data at hand and is subject to sampling variability. Confidence intervals can be constructed around this estimate to provide a range of plausible values for this unknown fixed parameter. If a 95% confidence interval for the slope is (1.5, 2.5), it would suggest that we are 95% confident that the true slope falls within this range.\nIn contrast, a Bayesian approach treats the slope as a random variable. Rather than a single fixed value, the slope has a distribution of possible values. This distribution combines our prior beliefs about what the slope might be (prior distribution) and the observed data (likelihood). The resulting distribution (posterior distribution) reflects our updated belief about the slope after seeing the data. If the mean of the posterior distribution is 2, it suggests that, given the prior and the data, our best guess is that a one-unit increase in the predictor variable corresponds to a 2-unit increase in the response variable on average. A credible interval, say a 95% credible interval of (1.5, 2.5), suggests that there is a 95% probability that the true value of the slope falls within this range, given our prior and the observed data.\n\n\nWhen to use Bayesian or frequentist approaches to estimation2\n\nOn many occasions, if one is careful in execution, both approaches to analysis will yield essentially equivalent inference\nFor small sample sizes, Bayesian approaches with carefully considered priors are often the only way to go because it is difficult to obtain well-calibrated frequentist intervals\nFor medium to large samples, unless there is strong prior information that one wants to incorporate, robust frequentist estimation using sandwich estimation is very appealing because its consistency is guaranteed under mild conditions\nFor highly complex models, a Bayesian approach is often the most convenient way to formulate the model, and computation under the Bayesian approach is the most straightforward\nIn summary, in many instances carefully considered Bayesian and frequentist approach will lead to similar scientific conclusions. My goal is describe the advantages and shortcoming of each approach, but a strong recommendation of one over the other is not given as there is often no reason for stating a preference.\n\n\n2 Wakefield, Jon. Bayesian and Frequentist Regression Methods. Section 1.6, Executive Summary\n\n\n2.3 Uses of Regression\n\nBorrowing information\n\nUse other groups to make estimates in groups with sparse data\nIntuitively, 67 and 69 year olds would provide some relevant information about 68 year olds\nAssuming a straight line relationship tells us about other, even more distant, individuals\nIf we do not want to assume a straight line, we may only want to borrow information from nearby groups\nLocally weighted scatterplot smooth line (lowess) added to the previous figures\nSplines discussed in future lectures\nMay not want to borrow too much information\n\nLinear relationship is an assumption, with often low power to detect departures from linearity\nAlways avoid extrapolating beyond the range of the data (e.g. ages under 65 or over 100)\nDefining “Contrasts”\n\nDefine a comparison across groups to use when answering scientific questions\nIf the straight line relationship holds, the slope is the difference in mean cholesterol levels between groups differing by 1 year in age\nDo we want to assume that comparisons of 65 to 66 year old subjects are the same as comparisons of 95 to 96 year old subjects?\nIf a non-linear relationship, the slope is still the average difference in mean cholesterol levels between groups differing by 1 year in age\nSlope is a (first order or linear) test for trend\n\n\n\n\n2.4 Linear Regression Inference\n\nRegression output provides\n\nEstimates\n\nIntercept: Estimated mean cholesterol when age is 0\nSlope: Estimated average difference in average cholesterol for two groups differing by 1 year in age\nStandard errors\n\nConfidence intervals\nP-values for testing\n\nIntercept is zero (usually unimportant)\nSlope is zero (test for linear trend in means)\n\n\n(Frequentist) Interpretation\n\nFrom linear regression analysis, we estimate that for each year difference in age, the difference in mean cholesterol is 0.32 mg/dL. A 95% confidence interval (CI) suggests that this observation is not unusual if the true difference in mean cholesterol per year difference in age were between 0.05 and 0.59 mg/dL. Because \\(p = 0.02\\), we reject the null hypothesis that there is no linear trend in the average cholesterol across age groups using a significance level, \\(\\alpha\\), of \\(0.05\\)."
  },
  {
    "objectID": "notes/Lec03.SimpleLinearRegression.html#simple-linear-regression",
    "href": "notes/Lec03.SimpleLinearRegression.html#simple-linear-regression",
    "title": "Simple Linear Regression",
    "section": "3 Simple Linear Regression",
    "text": "3 Simple Linear Regression\n\n3.1 Ingredients\n\nResponse\n\nThe distribution of this variable will be compared across groups\nLinear regression models the mean of the response variable\nLog transformation of the response corresponds to modeling the geometric mean\nNotation: It is extremely common to use \\(Y\\) to denote the response variable when discussing general methods\n\nPredictor\n\nGroup membership is measured by this variable\nNotation\n\nWhen not using mnemonics, will be referred to as the \\(X\\) variable in simple linear regression (linear regression with one predictor)\nLater, when we discuss multiple regression, will refer to \\(X_1, X_2, \\ldots, X_p\\) when there are up to \\(p\\) predictors\n\n\nRegression Model\n\nWe typically consider a “linear predictor function” that is linear in the modeled predictors\nExpected value (i.e. mean) of \\(Y\\) for a particular value of \\(X\\)\n\\(E[Y | X] = \\beta_0 + \\beta_1 \\times X\\)\nIn a deterministic world, a line is of the form \\(y = mx + b\\)\n\nWith no variation in the data, each value of \\(y\\) would like exactly on a straight line\nIntercept \\(b\\) is values of \\(y\\) when \\(x = 0\\)\nSlope \\(m\\) is the difference in \\(y\\) for a one unit difference in \\(x\\)\n\nStatistics in not completely deterministic. The real world has variability\n\nResponse within groups is variable (people born on the same day will have different cholesterol levels!)\n\nRandomness due to other variables impacting cholesterol\nInherent randomness\n\n\n\nThe regression line thus describes the central tendency of the data in a scatterplot of the response versus the predictor\nInterpretation of regression parameters\n\nIntercept \\(\\beta_0\\): Mean \\(Y\\) for a group with \\(X=0\\)\n\nOften \\(\\beta_0\\) is not of scientific interest\nMay be out of the range of data, or even impossible to observe \\(X=0\\)\n\nSlope \\(\\beta_1\\): Difference in mean \\(Y\\) across groups differing in \\(X\\) by 1 unit\n\nUsually measures association between \\(Y\\) and \\(X\\)\n\\(E[Y | X] = \\beta_0 + \\beta_1 \\times X\\)\n\n\nDerivation of interpretation\n\nSimple linear regression of response \\(Y\\) on predictor \\(X\\)\nMean of any arbitrary group can be derived from the equation \\[\nY_i = \\beta_0 + \\beta_1 X_i\n\\]\nInterpretation determined by considering possible values of \\(X\\)\nModel: \\(E[Y_i | X_i] = \\beta_0 + \\beta_1 \\times X_i\\)\n\nWhen \\(X_i = 0\\), \\(E[Y_i | X_i = 0 ] = \\beta_0\\)\nWhen \\(X_i = x\\), \\(E[Y_i | X_i = x ] = \\beta_0 + \\beta_1 x\\)\nWhen \\(X_i = x + 1\\), \\(E[Y_i | X_i = x + 1 ] = \\beta_0 + \\beta_1 x + \\beta_1\\)\nWe can use the above to get an equation for \\(\\beta_1\\)\n\n\\[\nE[Y_i | X_i = x + 1 ] - E[Y_i | X_i = x ] = \\\\\n(\\beta_0 + \\beta_1 x + \\beta_1) - (\\beta_0 + \\beta_1 x) \\\\\n= \\beta_1\n\\]\n\nThus, \\(\\beta_1\\) is the expected change in \\(Y\\) per one unit increase in \\(x\\)\n\n\n\n\n\n3.2 Matrix Notation\n\nUsing scalars, the simple linear regression model can be written as\n\n\\(Y_i = \\beta_0 + \\beta_1 \\times X_i + \\epsilon_i\\)\n\\(i = 1, \\ldots, n\\)\n\\(i\\) indexes the independent sampling units (e.g. subjects)\n\\(n\\) is the total number of independent sampling units\nThis formulauation drops the expected value notation, add in \\(\\epsilon_i\\)\n\n\\(\\epsilon_i\\) are the “Residuals” or “Errors”\n\\(E[\\epsilon_i] = 0\\)\n\\(V[\\epsilon_i] = \\sigma^2\\) (constant variance assumption)\n\n\n\n\nMatrix notation: \\({\\boldsymbol{Y}}= {\\boldsymbol{X}}{\\boldsymbol{\\beta}}+ {\\boldsymbol{\\epsilon}}\\)\n\n\\({\\boldsymbol{Y}}= \\left( \\begin{array}{c} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{array} \\right)_{n\\times1}\\) \\({\\boldsymbol{X}}= \\left( \\begin{array}{cc} 1 & x_1 \\\\ 1 & x_2 \\\\ \\vdots & \\vdots \\\\ 1 & x_n \\end{array} \\right)_{n\\times2}\\) \\({\\boldsymbol{\\beta}}= \\left( \\begin{array}{c} \\beta_0 \\\\ \\beta_1 \\\\ \\end{array} \\right)_{2\\times1}\\) \\({\\boldsymbol{\\epsilon}}= \\left( \\begin{array}{c} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{array} \\right)_{n\\times1}\\)\n\\(E[{\\boldsymbol{\\epsilon}}] = {\\boldsymbol{0}}\\), where \\({\\boldsymbol{0}}= \\left( \\begin{array}{c} 0 \\\\ 0 \\\\ \\vdots \\\\0 \\end{array} \\right)_{n\\times1}\\)\n\\(V[{\\boldsymbol{\\epsilon}}] = \\sigma^2 {\\boldsymbol{I}}\\), where \\({\\boldsymbol{I}}= \\left( \\begin{array}{cccc} 1 & 0 & \\ldots & 0 \\\\ 0 & 1 & \\ldots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\ldots & 1 \\end{array} \\right)_{n\\times n}\\)\n\\(E[{\\boldsymbol{Y}}] = {\\boldsymbol{X}}{\\boldsymbol{\\beta}}\\)\n\nI am using standard notation to indicate matrices/vectors and scalars\nBoldface indicates a vector or matrix (\\({\\boldsymbol{Y}}\\), \\({\\boldsymbol{X}}\\), \\({\\boldsymbol{\\beta}}\\), \\({\\boldsymbol{\\epsilon}}\\), \\({\\boldsymbol{0}}\\), \\({\\boldsymbol{I}}\\))\nNormal typeface indicates a scalar (\\(Y_i\\), \\(x_i\\), \\(\\beta_0\\), \\(\\beta_1\\), \\(\\epsilon_i\\), \\(0\\), \\(1\\))"
  },
  {
    "objectID": "notes/Lec03.SimpleLinearRegression.html#example-analysis-of-bmi-part-1",
    "href": "notes/Lec03.SimpleLinearRegression.html#example-analysis-of-bmi-part-1",
    "title": "Simple Linear Regression",
    "section": "4 Example Analysis of BMI (Part 1)",
    "text": "4 Example Analysis of BMI (Part 1)\n\nExample analysis conducted in class involving BMI (response) with gender (predictor 1) and age (predictor 2)\nThis is “Lab 1” and will serve as an example of how future labs will proceed"
  },
  {
    "objectID": "notes/Lec03.SimpleLinearRegression.html#residuals",
    "href": "notes/Lec03.SimpleLinearRegression.html#residuals",
    "title": "Simple Linear Regression",
    "section": "5 Residuals",
    "text": "5 Residuals\n\nOften linear regression models are specified in terms of the response instead of the mean response\n\nInclude an error term in the model, \\(\\epsilon_i\\)\nModel \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\)\n\nThe linear regression model is divided into two parts\n\nThe mean, or systematic, part (the “signal”)\nThe error, or random, part (the “noise”)\n\nResiduals\n\n\\(\\hat{\\epsilon}_i = Y_i - \\left(\\hat{\\beta_0} + \\hat{\\beta_1} X_i\\right)\\)\n\\(\\hat{{\\boldsymbol{\\epsilon}}} = {\\boldsymbol{Y}}- {\\boldsymbol{X}}\\hat{{\\boldsymbol{\\beta}}}\\)\n\\(\\hat{{\\boldsymbol{\\beta}}} = \\left({\\boldsymbol{X}}'{\\boldsymbol{X}}\\right)^{-1} \\left({\\boldsymbol{X}}'{\\boldsymbol{Y}}\\right)\\)\n\nThe mean of the residuals is \\(0\\)\nThe standard deviation of the residuals is the “Root Mean Square Error”\n\nIn our example analysis of BMI and gender, the RMSE is exactly equal to the pooled estimate of the standard deviation from a two-sample, equal variance t-test\nIn our example analysis of BMI and age, the RMSE is the square root of the average variances across the age groups\n\nIn many textbooks, \\(\\epsilon_i \\sim N(0, \\sigma^2)\\)\n\nA common \\(\\sigma^2\\) implies constant variance across all levels of the grouping variable, “homoscedasticity”\nNormality of the residuals is a nice property, but it is not necessary (and rarely observed in practice)\nWe will discuss how lack of Normality and heteroscedasticity impact statistical inference"
  },
  {
    "objectID": "notes/Lec03.SimpleLinearRegression.html#assumptions-and-inference-with-linear-regression",
    "href": "notes/Lec03.SimpleLinearRegression.html#assumptions-and-inference-with-linear-regression",
    "title": "Simple Linear Regression",
    "section": "6 Assumptions and Inference with Linear Regression",
    "text": "6 Assumptions and Inference with Linear Regression\n\n6.1 The Big Three Assumptions\n\nMost common uses of regression\n\nPrediction: Estimating what a future value of \\(Y\\) will be based on observed \\(X\\)\nComparisons within groups: Describing the distribution of \\(Y\\) across levels of the grouping variable \\(X\\) by estimating the mean \\(E[Y | X]\\)\nComparisons across groups: Differences appear across groups if the regression parameter slope estimate \\(\\beta_1\\) is non-zero\n\nValid statistical inference (CIs, p-values) about associations requires three general assumptions\nAssumption 1: Approximately Normal distributions for the parameter estimates\n\nNormal data or “large” N\nIt is often surprising how small “large” can be\n\nDefinition of large depends on the error distribution and relative sample sizes within each group\nWith exactly Normally distributed errors, only need one observation (or two to estimate a slope)\nWith very heavy tails, “large” can be very large\nSee Lumley, et al., Ann Rev Pub Hlth, 2002\n\n\nAssumption 2: Independence of observations\n\nClassic regression: Independence of all observation (now)\nRobust standard errors: Correlated observations within identified clusters (later)\n\nAssumption 3: Assumption about the variance of observations within groups\n\nClassic regression: Homoscedasticity (equal variance across groups)\nRobust standard errors: Allows for unequal variance across groups\n\n\n\n\n6.2 Additional Assumptions for Additional Inference\n\nNote that some textbooks will claim there are more than three assumptions. In truth, additional assumptions are not needed to make the aforementioned statistical inference about associations. However ...\nValid statistical inference (CIs, p-values) about mean responses in specific groups requires a further assumption\nAssumption 4: Adequacy of the linear model\n\nIf we are trying to borrow information about the mean from neighboring groups, and we are assuming a straight line relationship, the straight line needs to be true\nNo longer saying there is just a linear trend in the means, but now need to believe that all the means lie on a straight line\nNote that we can model transformations of the measured predictor\n\nFor inference about individual observations (prediction intervals, P-values) in specific groups requires another assumption\nAssumption 5: Assumptions about the distribution of the errors within each group (a very strong assumption)\n\nClassically: Errors have the same Normal distribution within each grouping variable\nRobust standard error will not help\nPrediction intervals assume a common error distribution across groups (homoscedasticity)\n\nPossible extension: Errors have the same distribution, but not necessarily Normal (rarely implemented in frequentist software)\n\nBootstrapping\nBayesian analysis\nOther flexible approaches\n\n\n\n6.2.1 Scenario 1: All model assumptions hold\n\n\nCode\nset.seed(1234)\nn &lt;- 200\nregassumptions &lt;- data.frame(x=seq(from=0, to=100, length=n))\n# Linear model correct, Normal errors\nregassumptions$y1 &lt;- 5 + 0.2*regassumptions$x + rnorm(n,0,5)\n# Linear model correct, Skewed errors\nregassumptions$y2 &lt;- 5 + 0.2*regassumptions$x + rlnorm(n,0,1)\n# Linear model correct, errors increasing with predictor (so increasing with Y too)\nregassumptions$y3 &lt;- 5 + 0.2*regassumptions$x + rnorm(n,0,1+regassumptions$x*.1)\n# Linear model incorrect, Normal error\nregassumptions$y4&lt;- 5 + 0.2*regassumptions$x + 0.02*(regassumptions$x)^2 + rnorm(n,0,5)\nggplot(regassumptions, aes(x=x, y=y1)) + geom_point() + geom_smooth(method=\"lm\")\n\n\n\n\n\nFigure 4: Scatterplot with best fit line where assumptions of Normality, linearity, homoskedasticity all hold\n\n\n\n\n\n\nCode\nmodel.y1 &lt;- lm(y1 ~ x, data = regassumptions)\ngglm(model.y1)\n\n\n\n\n\nFigure 5: Diagnostic plots where assumptions of Normality, linearity, homoskedasticity all hold\n\n\n\n\n\nCan estimate the mean of Y at a given value of X\nCan make inference about the association between X and Y (based on the slope)\nCan predict new values of Y given X\n\n\n\n6.2.2 Scenario 2: Error distribution is not Normally distributed\n\n\nCode\nggplot(regassumptions, aes(x=x, y=y2)) + geom_point() + geom_smooth(method=\"lm\")\n\n\n\n\n\nFigure 6: Scatterplot with best fit line where the error distribution is skewed to the right\n\n\n\n\n\n\nCode\nmodel.y2 &lt;- lm(y2 ~ x, data = regassumptions)\ngglm(model.y2)\n\n\n\n\n\nFigure 7: Diagnostic plots where the error distribution is skewed to the right\n\n\n\n\n\nIf the sample size is large enough, we can estimate the mean of Y at a given value of X (Central Limit Theorem)\nIf we use “robust” standard errors, we can make inference about the association between X and Y (based on the slope)\n\nRobust standard error relax the assumption of homoskedasticity/common variance\n\nWe cannot predict new values of Y given X. This requires the correct error distribution.\n\n\n\n6.2.3 Scenario 3: Variance increasing with predictor and expected value of Y (heteroskedasticity)\n\n\nCode\nggplot(regassumptions, aes(x=x, y=y3)) + geom_point() + geom_smooth(method=\"lm\")\n\n\n\n\n\nFigure 8: Scatterplot with best fit line where the error distribution variance is increasing\n\n\n\n\n\n\nCode\nmodel.y3 &lt;- lm(y3 ~ x, data = regassumptions)\ngglm(model.y3)\n\n\n\n\n\nFigure 9: Diagnostic plot where the error distribution variance is increasing\n\n\n\n\n\nIf the sample size is large enough, we can estimate the mean of Y at a given value of X (Central Limit Theorem)\nIf we use “robust” standard errors, we can make inference about the association between X and Y (based on the slope)\n\nRobust standard error relax the assumption of homoskedasticity/common variance\n\nWe cannot predict new values of Y given X. This requires the correct error distribution (and mean model, but that is correct)\n\n\n\n6.2.4 Scenario 4: Mean model incorrectly specified\n\n\nCode\nggplot(regassumptions, aes(x=x, y=y4)) + geom_point() + geom_smooth(method=\"lm\")\n\n\n\n\n\nFigure 10: Scatterplot with best fit line where the mean model is not correctly specified\n\n\n\n\n\n\nCode\nmodel.y4 &lt;- lm(y4 ~ x, data = regassumptions)\ngglm(model.y4)\n\n\n\n\n\nFigure 11: Diagnostic plots where the mean model is not correctly specified\n\n\n\n\n\nBecause our model is wrong, we cannot estimate the mean of Y at a given value of X. It will be biased, even if the sample size is large, the model will still be incorrect\n\nThe good news is that we can fix our mean model to more flexibly model the association.\n\nHere, we have just included a single slope term to model the association with \\(X\\), \\(E[Y|X] = \\beta_0 + \\beta_1*X\\).\nThe correct model in this simple example is \\(E[Y|X] = \\beta_0 + \\beta_1*X + \\beta_2*X^2\\). Note that this model is still a “linear model”. A “linear model” is linear in the parameters (\\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\)). We can included the quadratic predictor \\(X^2\\) in the model and still have a model that is linear in the parameters.\n\n\nIf we use “robust” standard errors, we can make inference about the association between X and Y (based on the slope, \\(\\beta_1\\))\n\nRobust standard error relax the assumption of homoskedasticity\nWhile the model is not a great fit, \\(\\beta_1\\) may summarize a first-order trend\n\nWe cannot predict new values of Y given X. This requires the correct mean model and error distribution.\n\n\n\n6.2.5 Summary\n\nRegression based inference about associations is far more trustworthy than estimation of group means or individual predictions.\nThere is much to be gained by using robust variance estimates\n\n\n\nTable 1: Summary of what is required for valid estimates of \\(\\beta\\) and the variance of beta using the classical, non-robust (NR) and robust (R) approaches. The classic (non-robust) variance is \\(\\hat{\\textrm{Var}}_{NR}(\\hat{\\beta})\\) and the robust variance is \\(\\hat{\\textrm{Var}}_R(\\hat{\\beta})\\)\n\n\n\n\n(a) Assumption Holds (Yes/No)\n\n\nLinearity\nHomoskedasticity\nNormality\n\n\n\n\nY\nY\nY\n\n\nY\nY\nN\n\n\nY\nN\nY\n\n\nY\nN\nN\n\n\nN\nY\nY\n\n\nN\nY\nN\n\n\nN\nN\nY\n\n\nN\nN\nN\n\n\n\n\n\n\n(b) Valid Inference (Yes/No/Maybe)\n\n\n\n\n\n\n\n\\(\\hat{\\beta}\\)\n\\(\\hat{\\textrm {Var}}_{NR}(\\hat{\\beta})\\)\n\\(\\hat{\\textrm{ Var}}_R(\\hat{\\beta})\\)\n\n\n\n\nY\nY\nY\n\n\nY\nM2\nM2\n\n\nY\nN\nY\n\n\nY\nN\nM2\n\n\nM1\nN\nM3\n\n\nM1\nN\nM2,3\n\n\nM1\nN\nM2\n\n\nM1\nN\nM2,3\n\n\n\n\n\n\n\n1 First-order trend, if appropriate\n2 Approximately valid under large sample sizes\n3 If \\(X\\) is random, but not if \\(X\\) is fixed\n\n\n\n\n6.3 Interpreting “Positive” Results\n\nSlope is statistically different from 0 using robust standard errors\nObserved data is atypical of a setting with no linear trend in mean response across groups\nData suggests evidence of a trend toward larger (or smaller) means in groups having larger values of the predictor\n(To the extent the data appears linear, estimates of the group means will be reliable)\n\n\n\n6.4 Interpreting “Negative” Results\n\nMany possible reasons why the slope is not statistically different from 0 using robust standard errors\nThere may be no association between the response and predictor\nThere may be an association, but not in the parameter considered (the mean response)\nThere may be an association in the parameter considered, but the best fitting line has zero slope\nThere may be a first order trend in the parameter considered, but we lacked the precision to be confident that it truly exists (a type II error)"
  },
  {
    "objectID": "notes/Lec03.SimpleLinearRegression.html#model-checking",
    "href": "notes/Lec03.SimpleLinearRegression.html#model-checking",
    "title": "Simple Linear Regression",
    "section": "7 Model Checking",
    "text": "7 Model Checking\n\nMuch statistical literature has been devoted to methods for checking the assumptions for regression models\nMy philosophy: Model checking is generally fraught with peril as it necessarily involves multiple comparisons\n\nWe cannot reliably use the sampled data to assess whether it accurately portrays the population\nWe are more worried about the data from the population that we might not have sampled\nIt is not so much the abnormal points that we see, but the ones that are hiding in the population that will make our model perform badly\nBut, do tend to worry more when we see a tendency to extreme outliers in the sample or clear departures from model assumptions\nIf we over-check our model and make adjustments to the model based on our observed data, we will inflate the type I error rate (i.e. will be more likely to claim statistical significance when it doesn’t really exist). We run the risk of creating a model that fits our data well but does not generalize.\n\nEstimates are biased away from the null\nReported standard errors are too small\nIf we fish through the data, we will always find significant results\nIn clinical trials, often Phase II results are not able to be replicated in Phase III trials\n\n\nInstead of extensive model checking, go back to our choices of inference when planning our analysis\n\nBest to plan for unusual data\nThere is often little to be lost and much to be gained by using the robust standard error estimates\nBy using robust errors, avoids much of the need for model checking\nModel checking is almost entirely data driven\nRobust standard errors is a more logical scientific approach\n\nMinimize the need to presume more knowledge than the question we are trying to answer\nE.g., if we don’t know how the means might differ, why should we presume to know how the variances or the shape of the distribution might behave?\n\n\n\n\n7.1 Residuals versus fitted values\n\nPlot of \\(\\hat{\\epsilon_i}\\) versus \\(\\hat{y_i}\\)\nIf assumptions hold, should be a random pattern about zero. See Figure 5 for example.\nIf you have a priori concerns about non-constant variance, this is one potential check\n\nE.g., it is fairly common for uncertainty to increase with outcome. See Figure 9 for the resulting “fan” pattern that would appear\n\n\n\n\n7.2 Scale-location plot\n\nThe scale-location plot is very similar to residuals vs fitted values, and is used to evaluate the homoskedasticity assumption\nIt uses the square root of the absolute value of standardized residuals instead of plotting the residuals themselves\nWe want to check two things\n\nThat the best fit line is approximately horizontal. If it is, then the average magnitude of the standardized residuals isn’t changing much as a function of the fitted values.\nThat the spread around the fit line doesn’t vary with the fitted values. If so, then the variability of magnitudes doesn’t vary much as a function of the fitted values.\n\n\n\n\n7.3 QQ plot\n\nUsed to evaluate Normality of the residuals\n\nIf assumptions hold, points should lie on a 1:1 line\n\nPlot of standardized residuals versus theoretical quantities from a N(0,1) distribution\n\nIf assumptions hold, the standardized residuals should be a random sample from a N(0,1) distribution\nWe can use quantiles of the Normal distribution to check how closely the observed matches the expected\n\nThe 50th percentile of the residuals should be 0\nThe 75th percentile of the residuals should be 0.67\nThe 90th percentile of the residuals should be 1.28\nThe 95th percentile of the residuals should be 1.64\nThe 97.5th percentil of the residuals should be 1.96\n\n\nSimilar idea would be a histogram or density plot of the (standardized) residuals that could be visually evaluated for Normality\n\n\n\n7.4 Residuals versus leverage\n\nUsed to check for outliers\nPlot can help us to identify influential observations, if there are any\nInfluence differs from leverage. Not all outliers influence the regression coefficient estimates"
  },
  {
    "objectID": "notes/Lec03.SimpleLinearRegression.html#linear-regression-with-robust-standard-error-estimates",
    "href": "notes/Lec03.SimpleLinearRegression.html#linear-regression-with-robust-standard-error-estimates",
    "title": "Simple Linear Regression",
    "section": "8 Linear Regression with Robust Standard Error estimates",
    "text": "8 Linear Regression with Robust Standard Error estimates\n\nThere are several libraries available in R for fitting models with robust error estimates\nThere are also several different flavors of robust estimators\n\nFor now, we are going to consider the default “Huber-White sandwich estimator”\nIn Stata, the Huber-White robust estimate of the standard error can be obtained using the ‘robust’ option\nregress chol age, robust\n\nIn the following examples I provide estimates using classical linear regression and linear regression estimate using robust standard errors. For each of these example compare\n\nEstimates of the intercept, \\(\\hat{\\beta_0}\\)\nEstimates of the slope, \\(\\hat{\\beta_1}\\)\nEstimates of the standard errors, \\(\\hat{\\textrm{se}}(\\hat{\\beta_0})\\) and \\(\\hat{\\textrm{se}}(\\hat{\\beta_1})\\)\n\n\n\n8.1 Comparison when Normality, constant variance holds\n\n8.1.1 Age and Cholesterol\n\n\nCode\nfit.ols &lt;- ols(chol ~ age, data=plotdata, x=TRUE)\nfit.ols\n\n\nLinear Regression Model\n \n ols(formula = chol ~ age, data = plotdata, x = TRUE)\n \n                  Model Likelihood    Discrimination    \n                        Ratio Test           Indexes    \n Obs      301    LR chi2      5.43    R2       0.018    \n sigma15.3405    d.f.            1    R2 adj   0.015    \n d.f.     299    Pr(&gt; chi2) 0.0198    g        2.344    \n \n Residuals\n \n     Min      1Q  Median      3Q     Max \n -39.591 -10.524  -0.234  11.154  41.819 \n \n \n           Coef     S.E.    t     Pr(&gt;|t|)\n Intercept 203.2259 10.3138 19.70 &lt;0.0001 \n age         0.3209  0.1375  2.33 0.0203  \n \n\n\n\n\nCode\nrobcov(fit.ols)\n\n\nLinear Regression Model\n \n ols(formula = chol ~ age, data = plotdata, x = TRUE)\n \n                  Model Likelihood    Discrimination    \n                        Ratio Test           Indexes    \n Obs      301    LR chi2      5.43    R2       0.018    \n sigma15.3405    d.f.            1    R2 adj   0.015    \n d.f.     299    Pr(&gt; chi2) 0.0198    g        2.344    \n \n Residuals\n \n     Min      1Q  Median      3Q     Max \n -39.591 -10.524  -0.234  11.154  41.819 \n \n \n           Coef     S.E.    t     Pr(&gt;|t|)\n Intercept 203.2259 10.3790 19.58 &lt;0.0001 \n age         0.3209  0.1378  2.33 0.0205  \n \n\n\n\n\n8.1.2 Scenario 1 dataset\n\n\nCode\nols(y1 ~ x, data=regassumptions, x=TRUE)\n\n\nLinear Regression Model\n \n ols(formula = y1 ~ x, data = regassumptions, x = TRUE)\n \n                 Model Likelihood    Discrimination    \n                       Ratio Test           Indexes    \n Obs     200    LR chi2    200.34    R2       0.633    \n sigma5.0531    d.f.            1    R2 adj   0.631    \n d.f.    198    Pr(&gt; chi2) 0.0000    g        7.659    \n \n Residuals\n \n      Min       1Q   Median       3Q      Max \n -15.2533  -3.2042  -0.4963   3.0693  14.4377 \n \n \n           Coef   S.E.   t     Pr(&gt;|t|)\n Intercept 3.3375 0.7119  4.69 &lt;0.0001 \n x         0.2275 0.0123 18.47 &lt;0.0001 \n \n\n\nCode\nrobcov(ols(y1 ~ x, data=regassumptions, x=TRUE))\n\n\nLinear Regression Model\n \n ols(formula = y1 ~ x, data = regassumptions, x = TRUE)\n \n                 Model Likelihood    Discrimination    \n                       Ratio Test           Indexes    \n Obs     200    LR chi2    200.34    R2       0.633    \n sigma5.0531    d.f.            1    R2 adj   0.631    \n d.f.    198    Pr(&gt; chi2) 0.0000    g        7.659    \n \n Residuals\n \n      Min       1Q   Median       3Q      Max \n -15.2533  -3.2042  -0.4963   3.0693  14.4377 \n \n \n           Coef   S.E.   t     Pr(&gt;|t|)\n Intercept 3.3375 0.7002  4.77 &lt;0.0001 \n x         0.2275 0.0132 17.20 &lt;0.0001 \n \n\n\n\n\n\n8.2 Comparison when variance is increasing with predictor (Scenario 3)\n\n\nCode\nols(y3 ~ x, data=regassumptions, x=TRUE)\n\n\nLinear Regression Model\n \n ols(formula = y3 ~ x, data = regassumptions, x = TRUE)\n \n                 Model Likelihood    Discrimination    \n                       Ratio Test           Indexes    \n Obs     200    LR chi2     95.93    R2       0.381    \n sigma6.5232    d.f.            1    R2 adj   0.378    \n d.f.    198    Pr(&gt; chi2) 0.0000    g        5.909    \n \n Residuals\n \n      Min       1Q   Median       3Q      Max \n -24.1947  -3.7861  -0.1145   4.1816  17.2286 \n \n \n           Coef   S.E.   t     Pr(&gt;|t|)\n Intercept 5.6640 0.9191  6.16 &lt;0.0001 \n x         0.1755 0.0159 11.04 &lt;0.0001 \n \n\n\nCode\nrobcov(ols(y3 ~ x, data=regassumptions, x=TRUE))\n\n\nLinear Regression Model\n \n ols(formula = y3 ~ x, data = regassumptions, x = TRUE)\n \n                 Model Likelihood    Discrimination    \n                       Ratio Test           Indexes    \n Obs     200    LR chi2     95.93    R2       0.381    \n sigma6.5232    d.f.            1    R2 adj   0.378    \n d.f.    198    Pr(&gt; chi2) 0.0000    g        5.909    \n \n Residuals\n \n      Min       1Q   Median       3Q      Max \n -24.1947  -3.7861  -0.1145   4.1816  17.2286 \n \n \n           Coef   S.E.   t     Pr(&gt;|t|)\n Intercept 5.6640 0.5955  9.51 &lt;0.0001 \n x         0.1755 0.0153 11.49 &lt;0.0001 \n \n\n\n\n\n8.3 Comparison when error variance is skewed to the right (Scenario 2)\n\n\nCode\nols(y2 ~ x, data=regassumptions, x=TRUE)\n\n\nLinear Regression Model\n \n ols(formula = y2 ~ x, data = regassumptions, x = TRUE)\n \n                 Model Likelihood    Discrimination    \n                       Ratio Test           Indexes    \n Obs     200    LR chi2    431.83    R2       0.885    \n sigma2.0635    d.f.            1    R2 adj   0.884    \n d.f.    198    Pr(&gt; chi2) 0.0000    g        6.596    \n \n Residuals\n \n     Min      1Q  Median      3Q     Max \n -1.8206 -1.1341 -0.6103  0.1731 16.6448 \n \n \n           Coef   S.E.   t     Pr(&gt;|t|)\n Intercept 6.9339 0.2907 23.85 &lt;0.0001 \n x         0.1959 0.0050 38.95 &lt;0.0001 \n \n\n\nCode\nrobcov(ols(y2 ~ x, data=regassumptions, x=TRUE))\n\n\nLinear Regression Model\n \n ols(formula = y2 ~ x, data = regassumptions, x = TRUE)\n \n                 Model Likelihood    Discrimination    \n                       Ratio Test           Indexes    \n Obs     200    LR chi2    431.83    R2       0.885    \n sigma2.0635    d.f.            1    R2 adj   0.884    \n d.f.    198    Pr(&gt; chi2) 0.0000    g        6.596    \n \n Residuals\n \n     Min      1Q  Median      3Q     Max \n -1.8206 -1.1341 -0.6103  0.1731 16.6448 \n \n \n           Coef   S.E.   t     Pr(&gt;|t|)\n Intercept 6.9339 0.3526 19.67 &lt;0.0001 \n x         0.1959 0.0056 35.18 &lt;0.0001 \n \n\n\n\n\n8.4 Comments\n\nComparison of standard error estimates\n\n\n\n\n\n\n\n\n\nModel\nClassical Intercept\nRobust Intercept\nClassical Slope\nRobust Slope\n\n\n\n\nAge-chol All Assump. hold\n\n\n\n\n\n\nScenario 1 Assump hold\n\n\n\n\n\n\nScenario 3 Increase Variance\n\n\n\n\n\n\nScenario 2 Skewed Variance\n\n\n\n\n\n\n\n\nPoint estimates \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) were the same for robust and classical standard error\nPoint estimates \\(\\hat{\\textrm{se}}(\\hat{\\beta_0})\\) and \\(\\hat{\\textrm{se}}(\\hat{\\beta_1})\\) were different for robust and classical standard error\n\nIn general, it is hard to predict the effect of using robust standard errors\nThese were just a a couple of examples and do not constitute a thorough examination of the impact of using robust standard errors\n\nWhen the model assumptions hold, for these two scenarios, robust standard error estimates were larger than classical standard error estimates\n\nWhile this is not unexpected, to formally evaluate the amount of increase would require a statistical simulation\n\nSimulate many datasets where the Normality and constant variance assumption holds\nFor each dataset, estimate \\(\\hat{\\textrm{se}}(\\hat{\\beta_0})\\) and \\(\\hat{\\textrm{se}}(\\hat{\\beta_1})\\)\nSummarize the results\nApproach will be covered in a future lab"
  },
  {
    "objectID": "notes/Lec03.SimpleLinearRegression.html#linear-regression-and-correlation",
    "href": "notes/Lec03.SimpleLinearRegression.html#linear-regression-and-correlation",
    "title": "Simple Linear Regression",
    "section": "9 Linear Regression and Correlation",
    "text": "9 Linear Regression and Correlation\n\n9.1 Pearson’s Correlation\n\nNotation\n\n\\(\\rho\\) signifies the population value\n\\(r\\) (or \\(\\hat{\\rho}\\)) is the estimated correlation from data\n\nFormula\n\n\\(r = \\frac{\\Sigma(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\Sigma(x_i - \\bar{x})^2\\Sigma(y_i - \\bar{y})^2}}\\)\nRange: \\(-1 \\leq r \\leq 1\\)\n\nInterpretation\n\nMeasures the linear relationship between \\(X\\) and \\(Y\\)\nCorrelation coefficient is a unitless index of strength of association between two variables (+ = positive association, - = negative, 0 = no association)\nCan test for significant association by testing whether the population correlation is zero t = which is identical to the \\(t\\)-test used to test whether the population \\(r\\) is zero; \\(\\textrm{d.f.} = n-2\\)\nUse probability calculator for \\(t\\) distribution to get a 2-tailed \\(P\\)-value\nConfidence intervals for population \\(r\\) calculated using Fisher’s \\(Z\\) transformation\n\n\\[Z = \\frac{1}{2} \\textrm{log}_\\textrm{e} \\left( \\frac{1+r}{1-r} \\right)\\]\n\nFor large \\(n\\), \\(Z\\) follows a Normal distribution with standard error \\(\\frac{1}{\\sqrt{n-3}}\\)\nTo calculate a confidence interval for \\(r\\), first find the confidence interval for \\(Z\\) then transform back to the \\(r\\) scale\n\n\n\\[\\begin{aligned}\nZ & = & \\frac{1}{2} \\textrm{log}_\\textrm{e} \\left( \\frac{1+r}{1-r} \\right) \\\\\n2*Z & = & \\textrm{log}_\\textrm{e} \\left( \\frac{1+r}{1-r} \\right) \\\\\n\\textrm{exp}(2*Z) & = & \\left( \\frac{1+r}{1-r} \\right) \\\\\n\\textrm{exp}(2*Z) * (1-r) & = & 1 + r \\\\\n\\textrm{exp}(2*Z) - r * \\textrm{exp}(2*Z) & = & 1 + r \\\\\n\\textrm{exp}(2*Z) - 1 & = & r * \\textrm{exp}(2*Z) + r \\\\\n\\textrm{exp}(2*Z) - 1 & = & r \\left(\\textrm{exp}(2*Z) + 1\\right) \\\\\n\\frac{\\textrm{exp}(2*Z) - 1}{\\textrm{exp}(2*Z) + 1} & = & r \\\\\n\\end{aligned}\\]\n\nExample (Altman 89-90): Pearson’s \\(r\\) for a study investigating the association of basal metabolic rate with total energy expenditure was calculated to be \\(0.7283\\) in a study of \\(13\\) women. Derive a 95% confidence interval for \\(r\\)\n\\[Z = \\frac{1}{2} \\textrm{log}_\\textrm{e} \\left( \\frac{1+r}{1-r} \\right) = 0.9251\\]\nThe lower limit of a 95% CI for \\(Z\\) is given by \\(0.9251 + 1.96*\\frac{1}{\\sqrt{n-3}} = 0.3053\\)\nThe upper limit is \\(0.9251 + 1.96*\\frac{1}{\\sqrt{n-3}} = 1.545\\)\nA 95% CI for the population correlation coefficient is given by transforming these limits from the \\(Z\\) scale back to the \\(r\\) scale.\n\n\\[\\frac{\\textrm{exp}(2*0.3053) - 1}{\\textrm{exp}(2*0.3053) + 1} \\hspace{.5cm} \\textrm{to} \\hspace{.5cm}  \\frac{\\textrm{exp}(2*1.545) - 1}{\\textrm{exp}(2*1.545) + 1}\\]\n\nWhich gives a 95% CI from 0.30 to 0.91 for the population correlation\n\n\n\nCode\nfisher.z &lt;- function(r) {.5 * log((1+r)/(1-r))}\nfisher.z.inv &lt;- function(z) {(exp(2*z)-1) / (exp(2*z)+1)}\n\nfisher.z.se &lt;- function(n) {1/sqrt(n-3)}\nfisher.z.inv(fisher.z(0.7283)) #Should be original value\n\n\n[1] 0.7283\n\n\nCode\nfisher.z(0.7283)\n\n\n[1] 0.9250975\n\n\nCode\nfisher.z(0.7283)-1.96*fisher.z.se(13)\n\n\n[1] 0.3052911\n\n\nCode\nfisher.z(0.7283)+1.96*fisher.z.se(13)\n\n\n[1] 1.544904\n\n\nCode\nfisher.z.inv(fisher.z(0.7283)+c(-1.96, 1.96)*fisher.z.se(13))\n\n\n[1] 0.2961472 0.9129407\n\n\n\n\n9.2 Relationship to regression\n\nPearson’s correlation (\\(\\rho\\)) is directly related to linear regression\nCorrelation treats \\(Y\\) and \\(X\\) symmetrically, but we can relate\n\n\\(E[Y | X]\\) as a function of \\(X\\)\n\\(E[Y | X] = \\beta_0 + \\beta_1 X\\)\n\\(\\beta_1 = \\rho \\frac{\\sigma_Y}{\\sigma_X}\\)\n\\(E[Y | X]\\): mean \\(Y\\) withing groups having equal \\(X\\)\n\\(\\beta_1\\): difference in mean \\(Y\\) per 1 unit difference in \\(X\\)\n\\(\\rho\\): true correlation between \\(Y\\) and \\(X\\)\n\\(\\sigma_Y\\): standard deviation of \\(Y\\)\n\\(\\sigma_X\\): standard deviation of \\(X\\)\n\nMore interpretable formulation of \\(\\rho\\)\n\n\\(\\rho \\approx \\beta \\sqrt{\\frac{\\textrm{Var}(X)}{\\beta^2\\textrm{Var}(X) + \\textrm{Var}(Y | X = x)}}\\)\n\\(\\beta\\): slope between \\(Y\\) and \\(X\\)\n\\(\\textrm{Var}(X)\\): variance of \\(X\\) in the sample\n\\(\\textrm{Var}(Y | X = x)\\): variance of \\(Y\\) in groups having the same value of \\(X\\) (the vertical spread of data)\n\nCorrelation tends to increase in absolute value as\n\nThe absolute value of the slope of the line increases\nThe variance of data decreases within groups that share a common value of \\(X\\)\nThe variance of \\(X\\) increases\n\nScientific uses of correlation\n\nThe slope between \\(X\\) and \\(Y\\) is of scientific interest\nThe variance of \\(Y\\) given \\(X=x\\) is partly of scientific interest, but can also be impacted by restricting sampling to certain values of another variable\n\nThe variance of height given age is less if sampling just males than if sampling both sexes\nThe variance of \\(X\\) is often set by study design (which is often not of scientific interest)\n\nHypothesis tests for a nonzero correlation are exactly the same as a test for a nonzero slope in classical linear regression\nThe statistical significance of a given value of \\(\\rho\\) depends only on the sample size\n\nCorrelation is more of a statistical measure than a scientific measure"
  },
  {
    "objectID": "notes/Lec03.SimpleLinearRegression.html#regression-on-geometric-means-log-transformations",
    "href": "notes/Lec03.SimpleLinearRegression.html#regression-on-geometric-means-log-transformations",
    "title": "Simple Linear Regression",
    "section": "10 Regression on Geometric Means: Log transformations",
    "text": "10 Regression on Geometric Means: Log transformations\n\nGeometric means of distributions are typically analyzed by using linear regression on the log transformed outcome\nUsed for inference under the following circumstances\n\nWe are (scientifically) interested in multiplicative rather than additive models\nWe want to down-weight the impact of outliers\nThe standard deviation of the response in a groups is proportional to the mean\n\n“Error is \\(\\pm 10\\%\\)” instead of “Error is \\(\\pm 10\\)”\n\n\n\n\n10.1 Interpretation of Parameters\n\nLinear regression on the log transformed \\(Y\\)\nNote that in statistics \\(\\textrm{log}\\) almost always refers the the natural log (\\(\\textrm{log}_e\\))\nModel: \\(E[\\textrm{log} Y_i | X_i] = \\beta_0 + \\beta_1 \\times X_i\\)\n\nWhen \\(X_i = 0\\): \\(E[\\textrm{log} Y_i | X_i = 0] = \\beta_0\\)\nWhen \\(X_i = x\\): \\(E[\\textrm{log} Y_i | X_i = x] = \\beta_0 + \\beta_1 \\times x\\)\nWhen \\(X_i = x+1\\): \\(E[\\textrm{log} Y_i | X_i = x + 1] = \\beta_0 + \\beta_1 \\times x + \\beta_1\\)\nRestate the model as log link for geometric mean\nModel: \\(\\textrm{log GM}[Y_i | X_i] = \\beta_0 + \\beta_1 \\times X_i\\)\nWhen \\(X_i = 0\\): \\(\\textrm{log GM}[Y_i | X_i = 0] = \\beta_0\\)\nWhen \\(X_i = x\\): \\(\\textrm{log GM}[Y_i | X_i = x] = \\beta_0 + \\beta_1 \\times x\\)\nWhen \\(X_i = x+1\\): \\(\\textrm{log GM}[Y_i | X_i = x + 1] = \\beta_0 + \\beta_1 \\times x + \\beta_1\\)\n\nInterpretation of parameters by back-transforming model\n\nModel: \\(\\textrm{GM}[Y_i | X_i] = e^{\\beta_0} \\times e^{\\beta_1 \\times X_i}\\)\nWhen \\(X_i = 0\\): \\(\\textrm{GM}[Y_i | X_i = 0] = e^{\\beta_0}\\)\nWhen \\(X_i = x\\): \\(\\textrm{GM}[Y_i | X_i = x] = e^{\\beta_0} \\times e^{\\beta_1 \\times x}\\)\nWhen \\(X_i = x+1\\): \\(\\textrm{GM}[Y_i | X_i = x + 1] = e^{\\beta_0} \\times e^{\\beta_1 \\times x} \\times e^{\\beta_1}\\)\n\n\n\n\n10.2 Example: FEV with Height\n\nQuestion: How does FEV differ across height groups?\nScientific justification for summarizing with geometric mean\n\nFEV is a volume\nHeight is a linear dimension\nEach dimension of lung size is proportional to height\nStandard deviation is likely proportional to height\nScience: \\(FEV \\propto Height^3\\) or \\(\\sqrt[3]{FEV} \\propto Height\\)\n\nStatistics: \\(\\textrm{log}(FEV) \\propto 3 \\textrm{ log}(Height)\\)\n\nScience dictates model choice\n\nStatistical preference for (any) transformations of response\n\nMay transform to equal variance across groups\nHomoscedasticity allows for easier inference\n\nStatistical preference for a log transformation\n\nEasier interpretation: multiplicative model\nCompare groups using ratios\n\n\n\n\n10.3 Log Transformed Predictors\n\nInterpretation of log predictor and log link for response\nLog link used to model the geometric mean of \\(Y\\)\nExponentiated slope estimates ratio of geometric means across groups\nLog transformation of predictor compares groups with a \\(k\\)-fold difference in there measured predictors\nEstimated ratio of geometric means\n\n\\(\\exp (\\textrm{log}(k) \\times \\beta_1) = k^{\\beta_1}\\)\n\nInterpretation of model\n\n\\(\\textrm{log GM}[FEV_i | log (height_i)] = -11.9 + 3.12 \\times log(height_i)\\)\n\nEstimate ratio of geometric mean FEV for two groups differing by \\(10\\%\\) in height (a 1.1 fold difference in height)\nTo find: Exponentiate \\(1.1\\) to the slope: \\(1.1^{3.12} = 1.35\\)\n\n“Group that is 10% taller is estimated to have a geometric mean FEV that is 1.35 times higher (35% higher)”\n\nWhy transform the predictor?\n\nTypically chosen according to whether the data are likely to follow a straight line relationship\nLinearity necessary to predict the values of the parameter in individual groups\nLinearity is not necessary to estimate existence of an association\nLinearity is not necessary to estimate a first order trend in the parameter across groups having the sampled distribution of the predictor\n(Inference about these two questions will tend to be conservative if linearity does not hold)\nRarely do we know which transformation of the predictor provides the best linear fit\nAs always, there is significant danger in using the data to estimate the best transformation to use\nIf there is no association of any kind between the response and the predictor, a linear fit (with a zero slope) is correct\nTrying to detect a transformation is thus an informal test for an association (multiple testing, which inflates type I error rate)\nIt is best to choose transformations of the predictor on scientific grounds\nIn practice, it is often true that mean functions are well approximated by a straight line over a small range of the data\n\nExample: In the FEV dataset, the logarithm of height is approximately linear over the range of heights sampled (look at the plot)\nWe can often choose untransformed predictors when science would suggest a non-linear association\nCan have major advantages in interpreting the results of the analysis\nIn the FEV example, it is far more natural to compare heights by difference than by ratios\n\nWe would rather characterize two children as differing by 4 inches in height rather than as the 44 inch child being 10% taller than the 40 inch child\n\n\n\n\n10.4 Looking Ahead to Multivariable Models\n\nThe relative importance of having the “true” transformation for a predictor depends on the statistical role\n\nPredictor of Interest\nEffect Modifiers\nConfounders\nPrecision variables\n\nWe treat these variables different when planning our analysis\n\nPredictor of Interest (should be pre-specified)\n\nIn general, don’t worry about modeling the exact association until you have even established an association (binary search)\nNeed to have a prespecified model for the POI\nSearching for the best fit can inflate the type I error\nMake most accurate, precise inference about the presence of the association first\nExploratory models can suggest models for future analyses\n\nEffect Modifiers (difficult to do right anyway)\n\nModeling of effect modifiers is invariably just a test for the existence of an interaction\nWe rarely have a lot of precision to answer questions in subgroups of the data\nPatterns of interaction can be so complex that it is unlikely that we will really capture the interactions across all subgroups in a single model\nTypically, we would restrict future studies to analyses treating subgroups separately\n\nConfounders (matters the most)\n\nIt is important to have an appropriate model of the association between the confounder and the response\nFailure to accurately model the confounder means that some residual confounding will exist\nHowever, searching for the best model may inflate the type I error for inference about the predictor of interest by overstating the precision of the study\nLuckily, we rarely care about inference for the confounder, so we are free to use inefficient means of adjustment (e.g. a stratified analysis)\n\nPrecision Variables\n\nWhen modeling precision variables, it is rarely worth the effort to use the “best” transformation\nWe usually capture the largest part of the added precision using crude models\nWe generally do not care about estimating association between the response and the precision variable\n\nMost often, precision variables represent known effects on the response"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bios 6312: Modern Regression",
    "section": "",
    "text": "This page contains an outline of the topics, content, and assignments for the semester. Note that this schedule will be updated as the semester progresses, with all changes documented here.\n\nNotes and Lab refer to course notes\nRMB is Regression Methods in Biostatistics\n\nSchedule updated Mon Jan 22 11:24:18 AM 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek\nDay\nDate\nTopic\nNotes.or.Lab\nSupplemental\nHomework\nReading\n\n\n\n\n1\nM\n2024/01/08\nIntroduction\nWelcome and Introduction\n\nInstall R and Rstudio\n\n\n\n1\nTu\n2024/01/09\nReview of Key Concepts\nLec 2\n\n\n\n\n\n1\nTh\n2024/01/11\nSimple Linear Regression\nLec 3\n\n\nRMB 3.3\n\n\n2\nM\n2024/01/15\nMartin Luther King, Jr Holiday\nno class\n\n\n\n\n\n2\nTu\n2024/01/16\nClass cancelled (weather)\nno class\n\n\n\n\n\n2\nTh\n2024/01/18\nSimple Linear Regression\nLab 1\n\n\nRMB 3.2\n\n\n3\nM\n2024/01/22\nSimple Linear Regression\nLab 1 and Lec 3\nMatrix Algebra Review\nHW 1 assigned\n\n\n\n3\nTu\n2024/01/23\nSimple Linear Regression\nChap 3\nLog Transformations\n\n\n\n\n3\nTh\n2024/01/25\nLinear Regression model assumptions\nLab 2\n\n\n\n\n\n4\nM\n2024/01/29\nSimple Logistic Regression\nLec 4\n\n\nRMB 3.4, 6.1\n\n\n4\nTu\n2024/01/30\nSimple Logistic Regression\nLec 4/Lab 3\n\nHW 1 due; HW 2 assigned\n\n\n\n4\nTh\n2024/02/01\nPrecision\nLec 5\n\n\n\n\n\n5\nM\n2024/02/05\nPrecision and adjustment for baseline\nLab 3\n\n\n\n\n\n5\nTu\n2024/02/06\nConfounding and Effect Modification\nLec 6\n\nHW 2 due; HW 3 assigned\nRMB 5.1\n\n\n5\nTh\n2024/02/08\nConfounding and Effect Modification\nLec 6\n\n\n\n\n\n6\nM\n2024/02/12\nMultivariable Models\nLec 7\n\n\n\n\n\n6\nTu\n2024/02/13\nMultivariable Models\nLec 7\n\nHW 3 due; HW 4 assigned\n\n\n\n6\nTh\n2024/02/15\nMultivariable Models\nLab 4\n\n\n\n\n\n7\nM\n2024/02/19\nLab: Multivariable models\n\n\n\n\n\n\n7\nTu\n2024/02/20\nTime to event models\n\n\nHW 4 due\n\n\n\n7\nTh\n2024/02/22\nModel estimation\n\n\n\n\n\n\n8\nM\n2024/02/26\nReview\n\n\n\n\n\n\n8\nTu\n2024/02/27\nMidterm Exam\n\n\nExam\n\n\n\n8\nTh\n2024/02/29\nModel estimation\n\n\n\n\n\n\n9\nM\n2024/03/04\nLab\n\n\n\n\n\n\n9\nTu\n2024/03/05\nModeling effect modificiaton\n\n\n\n\n\n\n9\nTh\n2024/03/07\nRegression models for nominal, ordinal, and count outcomes\n\n\n\n\n\n\n10\nM\n2024/03/11\nSpring Break\n\n\n\n\n\n\n10\nTu\n2024/03/12\nSpring Break\n\n\n\n\n\n\n10\nTh\n2024/03/14\nSpring Break\n\n\n\n\n\n\n11\nM\n2024/03/18\nLab\n\n\n\n\n\n\n11\nTu\n2024/03/19\nANOVA and Model Choice\n\n\n\n\n\n\n11\nTh\n2024/03/21\nANOVA and Model Choice\n\n\n\n\n\n\n12\nM\n2024/03/25\nNo Lab\n\n\nTake Home Exam\n\n\n\n12\nTu\n2024/03/26\nRegression Based Prediction\n\n\n\n\n\n\n12\nTh\n2024/03/28\nRegression Based Prediction\n\n\n\n\n\n\n13\nM\n2024/04/01\nLab\n\n\n\n\n\n\n13\nTu\n2024/04/02\nCorrelated data\n\n\n\n\n\n\n13\nTh\n2024/04/04\nCorrelated data\n\n\n\n\n\n\n14\nM\n2024/04/08\nLab\n\n\n\n\n\n\n14\nTu\n2024/04/09\nMissing data\n\n\n\n\n\n\n14\nTh\n2024/04/11\nFlexible models for dose-response\n\n\n\n\n\n\n15\nM\n2024/04/15\nLab\n\n\n\n\n\n\n15\nTu\n2024/04/16\nReview\n\n\n\n\n\n\n15\nTh\n2024/04/18\nFinal Exam (in class)\n\n\nExam\n\n\n\n16\nM\n2024/04/22\nGo over final exam"
  },
  {
    "objectID": "labs/lab1-linearregression.html",
    "href": "labs/lab1-linearregression.html",
    "title": "Lab 1: Linear regression",
    "section": "",
    "text": "Labs will begin with loading necessary packages (for R users). If this is the first time using these packages, they will need to be installed prior to being loaded.\nLab instructions follow in numbered steps. In general, you will need to add code (Insert… Code Cell… R), run it to obtain output, and provide an interpretation of the results. It is not enough to provide output without interpretation.\nWe will cover Parts 1 and 2 in one day. Part 3 will be revisited after we have covered more of the simple linear regression notes."
  },
  {
    "objectID": "labs/lab1-linearregression.html#general-instructions",
    "href": "labs/lab1-linearregression.html#general-instructions",
    "title": "Lab 1: Linear regression",
    "section": "",
    "text": "Labs will begin with loading necessary packages (for R users). If this is the first time using these packages, they will need to be installed prior to being loaded.\nLab instructions follow in numbered steps. In general, you will need to add code (Insert… Code Cell… R), run it to obtain output, and provide an interpretation of the results. It is not enough to provide output without interpretation.\nWe will cover Parts 1 and 2 in one day. Part 3 will be revisited after we have covered more of the simple linear regression notes."
  },
  {
    "objectID": "labs/lab1-linearregression.html#learning-objectives",
    "href": "labs/lab1-linearregression.html#learning-objectives",
    "title": "Lab 1: Linear regression",
    "section": "Learning objectives",
    "text": "Learning objectives\n\nBe able to fit and interpret frequentist and Bayesian simple linear regression models\nCompare the results to those that would be obtained using a 2-sample t-test\nCompare the results of robust versus classical standard error estimates for the frequentist approach"
  },
  {
    "objectID": "labs/lab1-linearregression.html#load-r-packages",
    "href": "labs/lab1-linearregression.html#load-r-packages",
    "title": "Lab 1: Linear regression",
    "section": "Load R packages",
    "text": "Load R packages\nLoad the following packages for use in this lab.\n\nlibrary(rms)\n\nLoading required package: Hmisc\n\n\nLoading required package: lattice\n\n\nLoading required package: survival\n\n\nLoading required package: Formula\n\n\nLoading required package: ggplot2\n\n\n\nAttaching package: 'Hmisc'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    knitrSet\n\n\nThe following objects are masked from 'package:base':\n\n    format.pval, units\n\n\nLoading required package: SparseM\n\n\n\nAttaching package: 'SparseM'\n\n\nThe following object is masked from 'package:base':\n\n    backsolve\n\nlibrary(rstanarm)\n\nLoading required package: Rcpp\n\n\nThis is rstanarm version 2.26.1\n\n\n- See https://mc-stan.org/rstanarm/articles/priors for changes to default priors!\n\n\n- Default priors may change, so it's safest to specify priors, even if equivalent to the defaults.\n\n\n- For execution on a local, multicore CPU with excess RAM we recommend calling\n\n\n  options(mc.cores = parallel::detectCores())\n\n\n\n1. Read in dataset\nWe will be using the inflammation dataset. Full documentation is available\nThe data is available as tab delimited or Stata format\n\n\n2. Perform basic descriptive statistics for variables of interest\nWe will only be considering the three variables: age, male, and bmi\n\nMale is coded as 0 for female and 1 for male. Create a factor variable (R) or label (Stata) this variable accordingly"
  },
  {
    "objectID": "labs/lab1-linearregression.html#part-1-bmi-and-gender-classical-standard-error-estimate",
    "href": "labs/lab1-linearregression.html#part-1-bmi-and-gender-classical-standard-error-estimate",
    "title": "Lab 1: Linear regression",
    "section": "Part 1: BMI and gender (classical standard error estimate)",
    "text": "Part 1: BMI and gender (classical standard error estimate)\n\n1. Create and describe a plot of BMI by gender\n\n\n2. Fit a simple linear regression model of BMI (outcome) on gender (predictor) using a frequentist approach.\n\n\n3. Conduct an (equal-variance) t-test of BMI by gender. Compare to the output of the linear regression model\n\n\n4. Fit a Bayesian linear regression model of BMI (outcome) on gender (predictor). Use the default priors. Compare the results."
  },
  {
    "objectID": "labs/lab1-linearregression.html#part-2-bmi-and-age",
    "href": "labs/lab1-linearregression.html#part-2-bmi-and-age",
    "title": "Lab 1: Linear regression",
    "section": "Part 2: BMI and age",
    "text": "Part 2: BMI and age\n\n1. Create and describe a plot of BMI by age\n\n\n2. Fit a simple linear regression model of BMI (outcome) on age (predictor) using a frequentist approach\n\n\n3. Interpret the slope and corresponding 95% confidence interval\n\n\n4. What is the estimate association for a 5-year increase in age? A 10-year increase in age? Give the confidence interval for each.\n\n\n5. Fit a similar Bayesian linear regression model and interpret the results."
  },
  {
    "objectID": "labs/lab1-linearregression.html#part-3-bmi-and-gender-robust-standard-error",
    "href": "labs/lab1-linearregression.html#part-3-bmi-and-gender-robust-standard-error",
    "title": "Lab 1: Linear regression",
    "section": "Part 3: BMI and gender, robust standard error",
    "text": "Part 3: BMI and gender, robust standard error\nWe will complete this part after covering robust standard errors in the notes\n\n1. Fit a regression using the robust standard error estimate. Compare the results from this regression model to a t-test assuming unequal variance between group."
  },
  {
    "objectID": "LICENSE.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "href": "LICENSE.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "title": "Attribution-ShareAlike 4.0 International",
    "section": "Creative Commons Attribution-ShareAlike 4.0 International Public License",
    "text": "Creative Commons Attribution-ShareAlike 4.0 International Public License\nBy exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License (“Public License”). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\n\nSection 1 – Definitions.\n\nAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\nAdapter’s License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.\nBY-SA Compatible License means a license listed at creativecommons.org/compatiblelicenses, approved by Creative Commons as essentially the equivalent of this Public License.\nCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\nEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\nExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\nLicense Elements means the license attributes listed in the name of a Creative Commons Public License. The License Elements of this Public License are Attribution and ShareAlike.\nLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\nLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\nLicensor means the individual(s) or entity(ies) granting rights under this Public License.\nShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\nSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\nYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\n\n\nSection 2 – Scope.\n\nLicense grant.\n\nSubject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\nA. reproduce and Share the Licensed Material, in whole or in part; and\nB. produce, reproduce, and Share Adapted Material.\nExceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\nTerm. The term of this Public License is specified in Section 6(a).\nMedia and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material.\nDownstream recipients.\nA. Offer from the Licensor – Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\nB. Additional offer from the Licensor – Adapted Material. Every recipient of Adapted Material from You automatically receives an offer from the Licensor to exercise the Licensed Rights in the Adapted Material under the conditions of the Adapter’s License You apply.\nC. No downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\nNo endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nOther rights.\n\nMoral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\nPatent and trademark rights are not licensed under this Public License.\nTo the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties.\n\n\n\n\nSection 3 – License Conditions.\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\nAttribution.\n\nIf You Share the Licensed Material (including in modified form), You must:\nA. retain the following if it is supplied by the Licensor with the Licensed Material:\n\nidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\na copyright notice;\na notice that refers to this Public License;\na notice that refers to the disclaimer of warranties;\na URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\nB. indicate if You modified the Licensed Material and retain an indication of any previous modifications; and\nC. indicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\nYou may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\nIf requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\n\nShareAlike.\n\nIn addition to the conditions in Section 3(a), if You Share Adapted Material You produce, the following conditions also apply.\n\nThe Adapter’s License You apply must be a Creative Commons license with the same License Elements, this version or later, or a BY-SA Compatible License.\nYou must include the text of, or the URI or hyperlink to, the Adapter’s License You apply. You may satisfy this condition in any reasonable manner based on the medium, means, and context in which You Share Adapted Material.\nYou may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, Adapted Material that restrict exercise of the rights granted under the Adapter’s License You apply.\n\n\n\nSection 4 – Sui Generis Database Rights.\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\nfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database;\nif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material, including for purposes of Section 3(b); and\nYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\n\n\nSection 5 – Disclaimer of Warranties and Limitation of Liability.\n\nUnless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You.\nTo the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You.\nThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\n\n\nSection 6 – Term and Termination.\n\nThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\nWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\nautomatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\nupon express reinstatement by the Licensor.\n\nFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\nFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\nSections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\n\n\nSection 7 – Other Terms and Conditions.\n\nThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\nAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.\n\n\n\nSection 8 – Interpretation.\n\nFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\nTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\nNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\nNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n\n\nCreative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the “Licensor.” The text of the Creative Commons public licenses is dedicated to the public domain under the CC0 Public Domain Dedication. Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark “Creative Commons” or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\nCreative Commons may be contacted at creativecommons.org."
  },
  {
    "objectID": "officehours.html",
    "href": "officehours.html",
    "title": "Office Hours",
    "section": "",
    "text": "We are available to discuss any questions that you might have at the following times\nChris Slaughter\n\n2525 West End, 11-107\n\nMonday 12 - 12:45\nMonday 2 - 3\n\nBy appointment\n\nIn person or virtual\nEmail to arrange\n\n\nLucy Yan\n\nFridays 12:30 - 1:30\n2525 West End, 11-105"
  },
  {
    "objectID": "supplemental/slr-derivations.html",
    "href": "supplemental/slr-derivations.html",
    "title": "Deriving the Least-Squares Estimates for Simple Linear Regression",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr. Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\n\n\nThis document contains the mathematical details for deriving the least-squares estimates for slope (\\(\\beta_1\\)) and intercept (\\(\\beta_0\\)). We obtain the estimates, \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\) by finding the values that minimize the sum of squared residuals, as shown in Equation 1.\n\\[\nSSR = \\sum\\limits_{i=1}^{n}[y_i - \\hat{y}_i]^2 = [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)]^2 = [y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i]^2\n\\tag{1}\\]\nRecall that we can find the values of \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\) that minimize /eq-ssr by taking the partial derivatives of Equation 1 and setting them to 0. Thus, the values of \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\) that minimize the respective partial derivative also minimize the sum of squared residuals. The partial derivatives are shown in Equation 2.\n\\[\n\\begin{aligned}\n\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_1} &= -2 \\sum\\limits_{i=1}^{n}x_i(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)  \\\\\n\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_0} &= -2 \\sum\\limits_{i=1}^{n}(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)\n\\end{aligned}\n\\tag{2}\\]\nThe derivation of deriving \\(\\hat{\\beta}_0\\) is shown in Equation 3.\n\\[\n\\begin{aligned}\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_0} &= -2 \\sum\\limits_{i=1}^{n}(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0 \\\\&\\Rightarrow -\\sum\\limits_{i=1}^{n}(y_i + \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i) = 0 \\\\&\\Rightarrow - \\sum\\limits_{i=1}^{n}y_i + n\\hat{\\beta}_0 + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i = 0 \\\\&\\Rightarrow n\\hat{\\beta}_0  = \\sum\\limits_{i=1}^{n}y_i - \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i \\\\&\\Rightarrow \\hat{\\beta}_0  = \\frac{1}{n}\\Big(\\sum\\limits_{i=1}^{n}y_i - \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i\\Big)\\\\&\\Rightarrow \\hat{\\beta}_0  = \\bar{y} - \\hat{\\beta}_1 \\bar{x} \\\\\\end{aligned}\n\\tag{3}\\]\nThe derivation of \\(\\hat{\\beta}_1\\) using the \\(\\hat{\\beta}_0\\) we just derived is shown in Equation 4.\n\\[\n\\begin{aligned}&\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_1} = -2 \\sum\\limits_{i=1}^{n}x_i(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0  \\\\&\\Rightarrow -\\sum\\limits_{i=1}^{n}x_iy_i + \\hat{\\beta}_0\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = 0 \\\\\\text{(Fill in }\\hat{\\beta}_0\\text{)}&\\Rightarrow -\\sum\\limits_{i=1}^{n}x_iy_i + (\\bar{y} - \\hat{\\beta}_1\\bar{x})\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = 0 \\\\&\\Rightarrow  (\\bar{y} - \\hat{\\beta}_1\\bar{x})\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = \\sum\\limits_{i=1}^{n}x_iy_i \\\\&\\Rightarrow \\bar{y}\\sum\\limits_{i=1}^{n}x_i - \\hat{\\beta}_1\\bar{x}\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = \\sum\\limits_{i=1}^{n}x_iy_i \\\\&\\Rightarrow n\\bar{y}\\bar{x} - \\hat{\\beta}_1n\\bar{x}^2 + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = \\sum\\limits_{i=1}^{n}x_iy_i \\\\&\\Rightarrow \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 - \\hat{\\beta}_1n\\bar{x}^2  = \\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x} \\\\&\\Rightarrow \\hat{\\beta}_1\\Big(\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2\\Big)  = \\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x} \\\\ &\\hat{\\beta}_1 = \\frac{\\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x}}{\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2}\\end{aligned}\n\\tag{4}\\]\nTo write \\(\\hat{\\beta}_1\\) in a form that’s more recognizable, we will use the following:\n\\[\n\\sum x_iy_i - n\\bar{y}\\bar{x} = \\sum(x - \\bar{x})(y - \\bar{y}) = (n-1)\\text{Cov}(x,y)\n\\tag{5}\\]\n\\[\n\\sum x_i^2 - n\\bar{x}^2 - \\sum(x - \\bar{x})^2 = (n-1)s_x^2\n\\tag{6}\\]\nwhere \\(\\text{Cov}(x,y)\\) is the covariance of \\(x\\) and \\(y\\), and \\(s_x^2\\) is the sample variance of \\(x\\) (\\(s_x\\) is the sample standard deviation).\nThus, applying Equation 5 and Equation 6, we have\n\\[\n\\begin{aligned}\\hat{\\beta}_1 &= \\frac{\\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x}}{\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2} \\\\&= \\frac{\\sum\\limits_{i=1}^{n}(x-\\bar{x})(y-\\bar{y})}{\\sum\\limits_{i=1}^{n}(x-\\bar{x})^2}\\\\&= \\frac{(n-1)\\text{Cov}(x,y)}{(n-1)s_x^2}\\\\&= \\frac{\\text{Cov}(x,y)}{s_x^2}\\end{aligned}\n\\tag{7}\\]\nThe correlation between \\(x\\) and \\(y\\) is \\(r = \\frac{\\text{Cov}(x,y)}{s_x s_y}\\). Thus, \\(\\text{Cov}(x,y) = r s_xs_y\\). Plugging this into Equation 7, we have\n\\[\n\\hat{\\beta}_1 = \\frac{\\text{Cov}(x,y)}{s_x^2} = r\\frac{s_ys_x}{s_x^2} = r\\frac{s_y}{s_x}\n\\tag{8}\\]"
  },
  {
    "objectID": "supplemental/log-transformations.html",
    "href": "supplemental/log-transformations.html",
    "title": "Log Transformations in Linear Regression",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr. Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document provides details about the model interpretation when the predictor and/or response variables are log-transformed. For simplicity, we will discuss transformations for the simple linear regression model as shown in Equation 1.\n\\[\n\\label{orig}\ny = \\beta_0 + \\beta_1 x\n\\tag{1}\\]\nAll results and interpretations can be easily extended to transformations in multiple regression models.\nNote: log refers to the natural logarithm."
  },
  {
    "objectID": "supplemental/log-transformations.html#log-transformation-on-the-response-variable",
    "href": "supplemental/log-transformations.html#log-transformation-on-the-response-variable",
    "title": "Log Transformations in Linear Regression",
    "section": "Log-transformation on the response variable",
    "text": "Log-transformation on the response variable\nSuppose we fit a linear regression model with \\(\\log(y)\\), the log-transformed \\(y\\), as the response variable. Under this model, we assume a linear relationship exists between \\(x\\) and \\(\\log(y)\\), such that \\(\\log(y) \\sim N(\\beta_0 + \\beta_1 x, \\sigma^2)\\) for some \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\). In other words, we can model the relationship between \\(x\\) and \\(\\log(y)\\) using the model in Equation 2.\n\\[\n\\log(y) = \\beta_0 + \\beta_1 x\n\\tag{2}\\]\nIf we interpret the model in terms of \\(\\log(y)\\), then we can use the usual interpretations for slope and intercept. When reporting results, however, it is best to give all interpretations in terms of the original response variable \\(y\\), since interpretations using log-transformed variables are often more difficult to truly understand.\nIn order to get back on the original scale, we need to use the exponential function (also known as the anti-log), \\(\\exp\\{x\\} = e^x\\). Therefore, we use the model in Equation 2 for interpretations and predictions, we will use Equation 3 to state our conclusions in terms of \\(y\\).\n\\[\n\\begin{aligned}\n&\\exp\\{\\log(y)\\} = \\exp\\{\\beta_0 + \\beta_1 x\\} \\\\[10pt]\n\\Rightarrow &y = \\exp\\{\\beta_0 + \\beta_1 x\\} \\\\[10pt]\n\\Rightarrow &y = \\exp\\{\\beta_0\\}\\exp\\{\\beta_1 x\\}\n\\end{aligned}\n\\tag{3}\\]\nIn order to interpret the slope and intercept, we need to first understand the relationship between the mean, median and log transformations.\n\nMean, Median, and Log Transformations\nSuppose we have a dataset y that contains the following observations:\n\n\n[1] 3 5 6 7 8\n\n\nIf we log-transform the values of y then calculate the mean and median, we have\n\n\n\n\n\nmean_log_y\nmedian_log_y\n\n\n\n\n1.70503\n1.79176\n\n\n\n\n\nIf we calculate the mean and median of y, then log-transform the mean and median, we have\n\n\n\n\n\nlog_mean\nlog_median\n\n\n\n\n1.75786\n1.79176\n\n\n\n\n\nThis is a simple illustration to show\n\n\\(\\text{Mean}[{\\log(y)}] \\neq \\log[\\text{Mean}(y)]\\) - the mean and log are not commutable\n\\(\\text{Median}[\\log(y)] = \\log[\\text{Median}(y)]\\) - the median and log are commutable\n\n\n\nInterpretaton of model coefficients\nUsing Equation 2, the mean \\(\\log(y)\\) for any given value of \\(x\\) is \\(\\beta_0 + \\beta_1 x\\); however, this does not indicate that the mean of \\(y = \\exp\\{\\beta_0 + \\beta_1 x\\}\\) (see previous section). From the assumptions of linear regression, we assume that for any given value of \\(x\\), the distribution of \\(\\log(y)\\) is Normal, and therefore symmetric. Thus the median of \\(\\log(y)\\) is equal to the mean of \\(\\log(y)\\), i.e \\(\\text{Median}(\\log(y)) = \\beta_0 + \\beta_1 x\\).\nSince the log and the median are commutable, \\(\\text{Median}(\\log(y)) = \\beta_0 + \\beta_1 x \\Rightarrow \\text{Median}(y) = \\exp\\{\\beta_0 + \\beta_1 x\\}\\). Thus, when we log-transform the response variable, the interpretation of the intercept and slope are in terms of the effect on the median of \\(y\\).\nIntercept: The intercept is expected median of \\(y\\) when the predictor variable equals 0. Therefore, when \\(x=0\\),\n\\[\n\\begin{aligned}\n&\\log(y) = \\beta_0 + \\beta_1 \\times 0 = \\beta_0 \\\\[10pt]\n\\Rightarrow &y = \\exp\\{\\beta_0\\}\n\\end{aligned}\n\\]\nInterpretation: When \\(x=0\\), the median of \\(y\\) is expected to be \\(\\exp\\{\\beta_0\\}\\).\nSlope: The slope is the expected change in the median of \\(y\\) when \\(x\\) increases by 1 unit. The change in the median of \\(y\\) is\n\\[\n\\exp\\{[\\beta_0 + \\beta_1 (x+1)] - [\\beta_0 + \\beta_1 x]\\} = \\frac{\\exp\\{\\beta_0 + \\beta_1 (x+1)\\}}{\\exp\\{\\beta_0 + \\beta_1 x\\}} = \\frac{\\exp\\{\\beta_0\\}\\exp\\{\\beta_1 x\\}\\exp\\{\\beta_1\\}}{\\exp\\{\\beta_0\\}\\exp\\{\\beta_1 x\\}} = \\exp\\{\\beta_1\\}\n\\]\nThus, the median of \\(y\\) for \\(x+1\\) is \\(\\exp\\{\\beta_1\\}\\) times the median of \\(y\\) for \\(x\\).\nInterpretation: When \\(x\\) increases by one unit, the median of \\(y\\) is expected to multiply by a factor of \\(\\exp\\{\\beta_1\\}\\)."
  },
  {
    "objectID": "supplemental/log-transformations.html#log-transformation-on-the-predictor-variable",
    "href": "supplemental/log-transformations.html#log-transformation-on-the-predictor-variable",
    "title": "Log Transformations in Linear Regression",
    "section": "Log-transformation on the predictor variable",
    "text": "Log-transformation on the predictor variable\nSuppose we fit a linear regression model with \\(\\log(x)\\), the log-transformed \\(x\\), as the predictor variable. Under this model, we assume a linear relationship exists between \\(\\log(x)\\) and \\(y\\), such that \\(y \\sim N(\\beta_0 + \\beta_1 \\log(x), \\sigma^2)\\) for some \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\). In other words, we can model the relationship between \\(\\log(x)\\) and \\(y\\) using the model in #eq-log-x.\n\\[\ny = \\beta_0 + \\beta_1 \\log(x)\n\\tag{4}\\]\nIntercept: The intercept is the mean of \\(y\\) when \\(\\log(x) = 0\\), i.e. \\(x = 1\\).\nInterpretation: When \\(x = 1\\) \\((\\log(x) = 0)\\), the mean of \\(y\\) is expected to be \\(\\beta_0\\).\nSlope: The slope is interpreted in terms of the change in the mean of \\(y\\) when \\(x\\) is multiplied by a factor of \\(C\\), since \\(\\log(Cx) = \\log(x) + \\log(C)\\). Thus, when \\(x\\) is multiplied by a factor of \\(C\\), the change in the mean of \\(y\\) is\n\\[\n\\begin{aligned}\n[\\beta_0 + \\beta_1 \\log(Cx)] - [\\beta_0 + \\beta_1 \\log(x)] &= \\beta_1 [\\log(Cx) - \\log(x)] \\\\[10pt]\n& = \\beta_1[\\log(C) + \\log(x) - \\log(x)] \\\\[10pt]\n& = \\beta_1 \\log(C)\n\\end{aligned}\n\\]\nThus the mean of \\(y\\) changes by \\(\\beta_1 \\log(C)\\) units.\nInterpretation: When \\(x\\) is multiplied by a factor of \\(C\\), the mean of \\(y\\) is expected to change by \\(\\beta_1 \\log(C)\\) units. For example, if \\(x\\) is doubled, then the mean of \\(y\\) is expected to change by \\(\\beta_1 \\log(2)\\) units."
  },
  {
    "objectID": "supplemental/log-transformations.html#log-transformation-on-the-the-response-and-predictor-variable",
    "href": "supplemental/log-transformations.html#log-transformation-on-the-the-response-and-predictor-variable",
    "title": "Log Transformations in Linear Regression",
    "section": "Log-transformation on the the response and predictor variable",
    "text": "Log-transformation on the the response and predictor variable\nSuppose we fit a linear regression model with \\(\\log(x)\\), the log-transformed \\(x\\), as the predictor variable and \\(\\log(y)\\), the log-transformed \\(y\\), as the response variable. Under this model, we assume a linear relationship exists between \\(\\log(x)\\) and \\(\\log(y)\\), such that \\(\\log(y) \\sim N(\\beta_0 + \\beta_1 \\log(x), \\sigma^2)\\) for some \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\). In other words, we can model the relationship between \\(\\log(x)\\) and \\(\\log(y)\\) using the model in Equation 5.\n\\[\n\\log(y) = \\beta_0 + \\beta_1 \\log(x)\n\\tag{5}\\]\nBecause the response variable is log-transformed, the interpretations on the original scale will be in terms of the median of \\(y\\) (see the section on the log-transformed response variable for more detail).\nIntercept: The intercept is the mean of \\(y\\) when \\(\\log(x) = 0\\), i.e. \\(x = 1\\). Therefore, when \\(\\log(x) = 0\\),\n\\[\n\\begin{aligned}\n&\\log(y) = \\beta_0 + \\beta_1 \\times 0 = \\beta_0 \\\\[10pt]\n\\Rightarrow &y = \\exp\\{\\beta_0\\}\n\\end{aligned}\n\\]\nInterpretation: When \\(x = 1\\) \\((\\log(x) = 0)\\), the median of \\(y\\) is expected to be \\(\\exp\\{\\beta_0\\}\\).\nSlope: The slope is interpreted in terms of the change in the median \\(y\\) when \\(x\\) is multiplied by a factor of \\(C\\), since \\(\\log(Cx) = \\log(x) + \\log(C)\\). Thus, when \\(x\\) is multiplied by a factor of \\(C\\), the change in the median of \\(y\\) is\n\\[\n\\begin{aligned}\n\\exp\\{[\\beta_0 + \\beta_1 \\log(Cx)] - [\\beta_0 + \\beta_1 \\log(x)]\\} &=\n\\exp\\{\\beta_1 [\\log(Cx) - \\log(x)]\\} \\\\[10pt]\n& = \\exp\\{\\beta_1[\\log(C) + \\log(x) - \\log(x)]\\} \\\\[10pt]\n& = \\exp\\{\\beta_1 \\log(C)\\} = C^{\\beta_1}\n\\end{aligned}\n\\]\nThus, the median of \\(y\\) for \\(Cx\\) is \\(C^{\\beta_1}\\) times the median of \\(y\\) for \\(x\\).\nInterpretation: When \\(x\\) is multiplied by a factor of \\(C\\), the median of \\(y\\) is expected to multiple by a factor of \\(C^{\\beta_1}\\). For example, if \\(x\\) is doubled, then the median of \\(y\\) is expected to multiply by \\(2^{\\beta_1}\\)."
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html",
    "href": "supplemental/model-diagnostics-matrix.html",
    "title": "Model Diagnostics",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr. Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document discusses some of the mathematical details of the model diagnostics - leverage, standardized residuals, and Cook’s distance. We assume the reader knowledge of the matrix form for multiple linear regression. Please see Matrix Form of Linear Regression for a review."
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#introduction",
    "href": "supplemental/model-diagnostics-matrix.html#introduction",
    "title": "Model Diagnostics",
    "section": "Introduction",
    "text": "Introduction\nSuppose we have \\(n\\) observations. Let the \\(i^{th}\\) be \\((x_{i1}, \\ldots, x_{ip}, y_i)\\), such that \\(x_{i1}, \\ldots, x_{ip}\\) are the explanatory variables (predictors) and \\(y_i\\) is the response variable. We assume the data can be modeled using the least-squares regression model, such that the mean response for a given combination of explanatory variables follows the form in Equation 1.\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_p x_p\n\\tag{1}\\]\nWe can write the response for the \\(i^{th}\\) observation as shown in Equation 2.\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip} + \\epsilon_i\n\\tag{2}\\]\nsuch that \\(\\epsilon_i\\) is the amount \\(y_i\\) deviates from \\(\\mu\\{y|x_{i1}, \\ldots, x_{ip}\\}\\), the mean response for a given combination of explanatory variables. We assume each \\(\\epsilon_i \\sim N(0,\\sigma^2)\\), where \\(\\sigma^2\\) is a constant variance for the distribution of the response \\(y\\) for any combination of explanatory variables \\(x_1, \\ldots, x_p\\)."
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#matrix-form-for-the-regression-model",
    "href": "supplemental/model-diagnostics-matrix.html#matrix-form-for-the-regression-model",
    "title": "Model Diagnostics",
    "section": "Matrix Form for the Regression Model",
    "text": "Matrix Form for the Regression Model\nWe can represent the Equation 1 and Equation 2 using matrix notation. Let\n\\[\n\\mathbf{Y} = \\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\y_n\\end{bmatrix}\n\\hspace{15mm}\n\\mathbf{X} = \\begin{bmatrix}x_{11} & x_{12} & \\dots & x_{1p} \\\\\nx_{21} & x_{22} & \\dots & x_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\nx_{n1} & x_{n2} & \\dots & x_{np} \\end{bmatrix}\n\\hspace{15mm}\n\\boldsymbol{\\beta}= \\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{bmatrix}\n\\hspace{15mm}\n\\boldsymbol{\\epsilon}= \\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix}\n\\tag{3}\\]\nThus,\n\\[\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{\\epsilon}\\]\nTherefore the estimated response for a given combination of explanatory variables and the associated residuals can be written as\n\\[\n\\hat{\\mathbf{Y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\hspace{10mm} \\mathbf{e} = \\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\n\\tag{4}\\]"
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#hat-matrix-leverage",
    "href": "supplemental/model-diagnostics-matrix.html#hat-matrix-leverage",
    "title": "Model Diagnostics",
    "section": "Hat Matrix & Leverage",
    "text": "Hat Matrix & Leverage\nRecall from the notes Matrix Form of Linear Regression that \\(\\hat{\\boldsymbol{\\beta}}\\) can be written as the following:\n\\[\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\n\\tag{5}\\]\nCombining Equation 4 and Equation 5, we can write \\(\\hat{\\mathbf{Y}}\\) as the following:\n\\[\n\\begin{aligned}\n\\hat{\\mathbf{Y}} &= \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\[10pt]\n&= \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\\\\n\\end{aligned}\n\\tag{6}\\]\nWe define the hat matrix as an \\(n \\times n\\) matrix of the form \\(\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\). Thus Equation 6 becomes\n\\[\n\\hat{\\mathbf{Y}} = \\mathbf{H}\\mathbf{Y}\n\\tag{7}\\]\nThe diagonal elements of the hat matrix are a measure of how far the predictor variables of each observation are from the means of the predictor variables. For example, \\(h_{ii}\\) is a measure of how far the values of the predictor variables for the \\(i^{th}\\) observation, \\(x_{i1}, x_{i2}, \\ldots, x_{ip}\\), are from the mean values of the predictor variables, \\(\\bar{x}_1, \\bar{x}_2, \\ldots, \\bar{x}_p\\). In the case of simple linear regression, the \\(i^{th}\\) diagonal, \\(h_{ii}\\), can be written as\n\\[\nh_{ii} =  \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_{j=1}^{n}(x_j-\\bar{x})^2}\n\\]\nWe call these diagonal elements, the leverage of each observation.\nThe diagonal elements of the hat matrix have the following properties:\n\n\\(0 \\leq h_ii \\leq 1\\)\n\\(\\sum\\limits_{i=1}^{n} h_{ii} = p+1\\), where \\(p\\) is the number of predictor variables in the model.\nThe mean hat value is \\(\\bar{h} = \\frac{\\sum\\limits_{i=1}^{n} h_{ii}}{n} = \\frac{p+1}{n}\\).\n\nUsing these properties, we consider a point to have high leverage if it has a leverage value that is more than 2 times the average. In other words, observations with leverage greater than \\(\\frac{2(p+1)}{n}\\) are considered to be high leverage points, i.e. outliers in the predictor variables. We are interested in flagging high leverage points, because they may have an influence on the regression coefficients.\nWhen there are high leverage points in the data, the regression line will tend towards those points; therefore, one property of high leverage points is that they tend to have small residuals. We will show this by rewriting the residuals from Equation 4 using Equation 7.\n\\[\n\\begin{aligned}\n\\mathbf{e} &= \\mathbf{Y} - \\hat{\\mathbf{Y}} \\\\[10pt]\n& = \\mathbf{Y} - \\mathbf{H}\\mathbf{Y} \\\\[10pt]\n&= (1-\\mathbf{H})\\mathbf{Y}\n\\end{aligned}\n\\tag{8}\\]\nNote that the identity matrix and hat matrix are idempotent, i.e. \\(\\mathbf{I}\\mathbf{I} = \\mathbf{I}\\), \\(\\mathbf{H}\\mathbf{H} = \\mathbf{H}\\). Thus, \\((\\mathbf{I} - \\mathbf{H})\\) is also idempotent. These matrices are also symmetric. Using these properties and Equation 8, we have that the variance-covariance matrix of the residuals \\(\\boldsymbol{e}\\), is\n\\[\n\\begin{aligned}\nVar(\\mathbf{e}) &= \\mathbf{e}\\mathbf{e}^T \\\\[10pt]\n&=  (1-\\mathbf{H})Var(\\mathbf{Y})^T(1-\\mathbf{H})^T \\\\[10pt]\n&= (1-\\mathbf{H})\\hat{\\sigma}^2(1-\\mathbf{H})^T  \\\\[10pt]\n&= \\hat{\\sigma}^2(1-\\mathbf{H})(1-\\mathbf{H})  \\\\[10pt]\n&= \\hat{\\sigma}^2(1-\\mathbf{H})\n\\end{aligned}\n\\tag{9}\\]\nwhere \\(\\hat{\\sigma}^2 = \\frac{\\sum_{i=1}^{n}e_i^2}{n-p-1}\\) is the estimated regression variance. Thus, the variance of the \\(i^{th}\\) residual is \\(Var(e_i) = \\hat{\\sigma}^2(1-h_{ii})\\). Therefore, the higher the leverage, the smaller the variance of the residual. Because the expected value of the residuals is 0, we conclude that points with high leverage tend to have smaller residuals than points with lower leverage."
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#standardized-residuals",
    "href": "supplemental/model-diagnostics-matrix.html#standardized-residuals",
    "title": "Model Diagnostics",
    "section": "Standardized Residuals",
    "text": "Standardized Residuals\nIn general, we standardize a value by shifting by the expected value and rescaling by the standard deviation (or standard error). Thus, the \\(i^{th}\\) standardized residual takes the form\n\\[\nstd.res_i = \\frac{e_i - E(e_i)}{SE(e_i)}\n\\]\nThe expected value of the residuals is 0, i.e. \\(E(e_i) = 0\\). From Equation 9), the standard error of the residual is \\(SE(e_i) = \\hat{\\sigma}\\sqrt{1-h_{ii}}\\). Therefore,\n\\[\nstd.res_i = \\frac{e_i}{\\hat{\\sigma}\\sqrt{1-h_{ii}}}\n\\tag{10}\\]"
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#cooks-distance",
    "href": "supplemental/model-diagnostics-matrix.html#cooks-distance",
    "title": "Model Diagnostics",
    "section": "Cook’s Distance",
    "text": "Cook’s Distance\nCook’s distance is a measure of how much each observation influences the model coefficients, and thus the predicted values. The Cook’s distance for the \\(i^{th}\\) observation can be written as\n\\[\nD_i = \\frac{(\\hat{\\mathbf{Y}} -\\hat{\\mathbf{Y}}_{(i)})^T(\\hat{\\mathbf{Y}} -\\hat{\\mathbf{Y}}_{(i)})}{(p+1)\\hat{\\sigma}}\n\\tag{11}\\]\nwhere \\(\\hat{\\mathbf{Y}}_{(i)}\\) is the vector of predicted values from the model fitted when the \\(i^{th}\\) observation is deleted. Cook’s Distance can be calculated without deleting observations one at a time, since Equation 12 below is mathematically equivalent to Equation 11.\n\\[\nD_i = \\frac{1}{p+1}std.res_i^2\\Bigg[\\frac{h_{ii}}{(1-h_{ii})}\\Bigg] = \\frac{e_i^2}{(p+1)\\hat{\\sigma}^2(1-h_{ii})}\\Bigg[\\frac{h_{ii}}{(1-h_{ii})}\\Bigg]\n\\tag{12}\\]"
  },
  {
    "objectID": "supplemental/model-selection-criteria.html",
    "href": "supplemental/model-selection-criteria.html",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr. Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document discusses some of the mathematical details of Akaike’s Information Criterion (AIC) and Schwarz’s Bayesian Information Criterion (BIC). We assume the reader knowledge of the matrix form for multiple linear regression.Please see Matrix Notation for Multiple Linear Regression for a review."
  },
  {
    "objectID": "supplemental/model-selection-criteria.html#maximum-likelihood-estimation-of-boldsymbolbeta-and-sigma",
    "href": "supplemental/model-selection-criteria.html#maximum-likelihood-estimation-of-boldsymbolbeta-and-sigma",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "Maximum Likelihood Estimation of \\(\\boldsymbol{\\beta}\\) and \\(\\sigma\\)",
    "text": "Maximum Likelihood Estimation of \\(\\boldsymbol{\\beta}\\) and \\(\\sigma\\)\nTo understand the formulas for AIC and BIC, we will first briefly explain the likelihood function and maximum likelihood estimates for regression.\nLet \\(\\mathbf{Y}\\) be \\(n \\times 1\\) matrix of responses, \\(\\mathbf{X}\\), the \\(n \\times (p+1)\\) matrix of predictors, and \\(\\boldsymbol{\\beta}\\), \\((p+1) \\times 1\\) matrix of coefficients. If the multiple linear regression model is correct then,\n\\[\n\\mathbf{Y} \\sim N(\\mathbf{X}\\boldsymbol{\\beta}, \\sigma^2)\n\\tag{1}\\]\nWhen we do linear regression, our goal is to estimate the unknown parameters \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) from Equation 1. In Matrix Notation for Multiple Linear Regression, we showed a way to estimate these parameters using matrix alegbra. Another approach for estimating \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) is using maximum likelihood estimation.\nA likelihood function is used to summarise the evidence from the data in support of each possible value of a model parameter. Using Equation 1, we will write the likelihood function for linear regression as\n\\[\nL(\\mathbf{X}, \\mathbf{Y}|\\boldsymbol{\\beta}, \\sigma^2) = \\prod\\limits_{i=1}^n (2\\pi \\sigma^2)^{-\\frac{1}{2}} \\exp\\bigg\\{-\\frac{1}{2\\sigma^2}\\sum\\limits_{i=1}^n(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta})^T(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta})\\bigg\\}\n\\tag{2}\\]\nwhere \\(Y_i\\) is the \\(i^{th}\\) response and \\(\\mathbf{X}_i\\) is the vector of predictors for the \\(i^{th}\\) observation. One approach estimating \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) is to find the values of those parameters that maximize the likelihood in Equation 2, i.e. maximum likelhood estimation. To make the calculations more manageable, instead of maximizing the likelihood function, we will instead maximize its logarithm, i.e. the log-likelihood function. The values of the parameters that maximize the log-likelihood function are those that maximize the likelihood function. The log-likelihood function we will maximize is\n\\[\n\\begin{aligned}\n\\log L(\\mathbf{X}, \\mathbf{Y}|\\boldsymbol{\\beta}, \\sigma^2) &= \\sum\\limits_{i=1}^n -\\frac{1}{2}\\log(2\\pi\\sigma^2) -\\frac{1}{2\\sigma^2}\\sum\\limits_{i=1}^n(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta})^T(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta}) \\\\\n&= -\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})^T(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})\\\\\n\\end{aligned}\n\\tag{3}\\]\n\nThe maximum likelihood estimate of \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) are \\[\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} \\hspace{10mm} \\hat{\\sigma}^2 = \\frac{1}{n}(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})^T(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta}) = \\frac{1}{n}RSS\n\\tag{4}\\]\nwhere \\(RSS\\) is the residual sum of squares. Note that the maximum likelihood estimate is not exactly equal to the estimate of \\(\\sigma^2\\) we typically use \\(\\frac{RSS}{n-p-1}\\). This is because the maximum likelihood estimate of \\(\\sigma^2\\) in Equation 4 is a biased estimator of \\(\\sigma^2\\). When \\(n\\) is much larger than the number of predictors \\(p\\), then the differences in these two estimates are trivial."
  },
  {
    "objectID": "supplemental/model-selection-criteria.html#aic",
    "href": "supplemental/model-selection-criteria.html#aic",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "AIC",
    "text": "AIC\nAkaike’s Information Criterion (AIC) is\n\\[\nAIC = -2 \\log L + 2(p+1)\n\\tag{5}\\]\nwhere \\(\\log L\\) is the log-likelihood. This is the general form of AIC that can be applied to a variety of models, but for now, let’s focus on AIC for mutliple linear regression.\n\\[\n\\begin{aligned}\nAIC &= -2 \\log L + 2(p+1) \\\\\n&= -2\\bigg[-\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})^T(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})\\bigg] + 2(p+1) \\\\\n&= n\\log\\big(2\\pi\\frac{RSS}{n}\\big) + \\frac{1}{RSS/n}RSS \\\\\n&= n\\log(2\\pi) + n\\log(RSS) - n\\log(n) + 2(p+1)\n\\end{aligned}\n\\tag{6}\\]"
  },
  {
    "objectID": "supplemental/model-selection-criteria.html#bic",
    "href": "supplemental/model-selection-criteria.html#bic",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "BIC",
    "text": "BIC\n[To be added.]"
  },
  {
    "objectID": "supplemental/mlr-matrix.html",
    "href": "supplemental/mlr-matrix.html",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr. Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document provides the details for the matrix notation for multiple linear regression. We assume the reader has familiarity with some linear algebra. Please see Chapter 1 of An Introduction to Statistical Learning for a brief review of linear algebra."
  },
  {
    "objectID": "supplemental/mlr-matrix.html#introduction",
    "href": "supplemental/mlr-matrix.html#introduction",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "Introduction",
    "text": "Introduction\nSuppose we have \\(n\\) observations. Let the \\(i^{th}\\) be \\((x_{i1}, \\ldots, x_{ip}, y_i)\\), such that \\(x_{i1}, \\ldots, x_{ip}\\) are the explanatory variables (predictors) and \\(y_i\\) is the response variable. We assume the data can be modeled using the least-squares regression model, such that the mean response for a given combination of explanatory variables follows the form in Equation 1.\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_p x_p\n\\tag{1}\\]\nWe can write the response for the \\(i^{th}\\) observation as shown in Equation 2\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip} + \\epsilon_i\n\\tag{2}\\]\nsuch that \\(\\epsilon_i\\) is the amount \\(y_i\\) deviates from \\(\\mu\\{y|x_{i1}, \\ldots, x_{ip}\\}\\), the mean response for a given combination of explanatory variables. We assume each \\(\\epsilon_i \\sim N(0,\\sigma^2)\\), where \\(\\sigma^2\\) is a constant variance for the distribution of the response \\(y\\) for any combination of explanatory variables \\(x_1, \\ldots, x_p\\)."
  },
  {
    "objectID": "supplemental/mlr-matrix.html#matrix-representation-for-the-regression-model",
    "href": "supplemental/mlr-matrix.html#matrix-representation-for-the-regression-model",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "Matrix Representation for the Regression Model",
    "text": "Matrix Representation for the Regression Model\nWe can represent the Equation 1 and Equation 2 using matrix notation. Let\n\\[\n\\mathbf{Y} = \\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\y_n\\end{bmatrix}\n\\hspace{15mm}\n\\mathbf{X} = \\begin{bmatrix}x_{11} & x_{12} & \\dots & x_{1p} \\\\\nx_{21} & x_{22} & \\dots & x_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\nx_{n1} & x_{n2} & \\dots & x_{np} \\end{bmatrix}\n\\hspace{15mm}\n\\boldsymbol{\\beta}= \\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{bmatrix}\n\\hspace{15mm}\n\\boldsymbol{\\epsilon}= \\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix}\n\\tag{3}\\]\nThus,\n\\[\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{\\epsilon}\\]\nTherefore the estimated response for a given combination of explanatory variables and the associated residuals can be written as\n\\[\n\\hat{\\mathbf{Y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\hspace{10mm} \\mathbf{e} = \\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\n\\tag{4}\\]"
  },
  {
    "objectID": "supplemental/mlr-matrix.html#estimating-the-coefficients",
    "href": "supplemental/mlr-matrix.html#estimating-the-coefficients",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "Estimating the Coefficients",
    "text": "Estimating the Coefficients\nThe least-squares model is the one that minimizes the sum of the squared residuals. Therefore, we want to find the coefficients, \\(\\hat{\\boldsymbol{\\beta}}\\) that minimizes\n\\[\n\\sum\\limits_{i=1}^{n} e_{i}^2 = \\mathbf{e}^T\\mathbf{e} = (\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})^T(\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})\n\\tag{5}\\]\nwhere \\(\\mathbf{e}^T\\), the transpose of the matrix \\(\\mathbf{e}\\).\n\\[\n(\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})^T(\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{Y}^T\\mathbf{Y} -\n\\mathbf{Y}^T \\mathbf{X}\\hat{\\boldsymbol{\\beta}} - (\\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{Y} +\n\\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{X}\n\\hat{\\boldsymbol{\\beta}})\n\\tag{6}\\]\nNote that \\((\\mathbf{Y^T}\\mathbf{X}\\hat{\\boldsymbol{\\beta}})^T = \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{Y}\\). Since these are both constants (i.e. \\(1\\times 1\\) vectors), \\(\\mathbf{Y^T}\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{Y}\\). Thus, Equation 7 becomes\n\\[\n\\mathbf{Y}^T\\mathbf{Y} - 2 \\mathbf{X}^T\\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{Y} + \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{X}\n\\hat{\\boldsymbol{\\beta}}\n\\tag{7}\\]\nSince we want to find the \\(\\hat{\\boldsymbol{\\beta}}\\) that minimizes Equation 5, will find the value of \\(\\hat{\\boldsymbol{\\beta}}\\) such that the derivative with respect to \\(\\hat{\\boldsymbol{\\beta}}\\) is equal to 0.\n\\[\n\\begin{aligned}\n\\frac{\\partial \\mathbf{e}^T\\mathbf{e}}{\\partial \\hat{\\boldsymbol{\\beta}}} & = \\frac{\\partial}{\\partial \\hat{\\boldsymbol{\\beta}}}(\\mathbf{Y}^T\\mathbf{Y} - 2 \\mathbf{X}^T\\hat{\\boldsymbol{\\beta}}{}^T\\mathbf{Y} + \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}}) = 0 \\\\\n&\\Rightarrow - 2 \\mathbf{X}^T\\mathbf{Y} + 2 \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = 0 \\\\\n& \\Rightarrow 2 \\mathbf{X}^T\\mathbf{Y} = 2 \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\\n& \\Rightarrow \\mathbf{X}^T\\mathbf{Y} = \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\\n& \\Rightarrow (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\\n& \\Rightarrow (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} = \\mathbf{I}\\hat{\\boldsymbol{\\beta}}\n\\end{aligned}\n\\tag{8}\\]\nThus, the estimate of the model coefficients is \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\)."
  },
  {
    "objectID": "supplemental/mlr-matrix.html#variance-covariance-matrix-of-the-coefficients",
    "href": "supplemental/mlr-matrix.html#variance-covariance-matrix-of-the-coefficients",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "Variance-covariance matrix of the coefficients",
    "text": "Variance-covariance matrix of the coefficients\nWe will use two properties to derive the form of the variance-covariance matrix of the coefficients:\n\n\\(E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] = \\sigma^2I\\)\n\\(\\hat{\\boldsymbol{\\beta}} = \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\epsilon\\)\n\nFirst, we will show that \\(E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] = \\sigma^2I\\)\n\\[\n\\begin{aligned}\nE[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] &= E \\begin{bmatrix}\\epsilon_1  & \\epsilon_2 & \\dots & \\epsilon_n \\end{bmatrix}\\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix}  \\\\\n& = E \\begin{bmatrix} \\epsilon_1^2  & \\epsilon_1 \\epsilon_2 & \\dots & \\epsilon_1 \\epsilon_n \\\\\n\\epsilon_2 \\epsilon_1 & \\epsilon_2^2 & \\dots & \\epsilon_2 \\epsilon_n \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\epsilon_n \\epsilon_1 & \\epsilon_n \\epsilon_2 & \\dots & \\epsilon_n^2\n\\end{bmatrix} \\\\\n& = \\begin{bmatrix} E[\\epsilon_1^2]  & E[\\epsilon_1 \\epsilon_2] & \\dots & E[\\epsilon_1 \\epsilon_n] \\\\\nE[\\epsilon_2 \\epsilon_1] & E[\\epsilon_2^2] & \\dots & E[\\epsilon_2 \\epsilon_n] \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nE[\\epsilon_n \\epsilon_1] & E[\\epsilon_n \\epsilon_2] & \\dots & E[\\epsilon_n^2]\n\\end{bmatrix}\n\\end{aligned}\n\\tag{9}\\]\nRecall, the regression assumption that the errors \\(\\epsilon_i's\\) are Normally distributed with mean 0 and variance \\(\\sigma^2\\). Thus, \\(E(\\epsilon_i^2) = Var(\\epsilon_i) = \\sigma^2\\) for all \\(i\\). Additionally, recall the regression assumption that the errors are uncorrelated, i.e. \\(E(\\epsilon_i \\epsilon_j) = Cov(\\epsilon_i, \\epsilon_j) = 0\\) for all \\(i,j\\). Using these assumptions, we can write Equation 9 as\n\\[\nE[\\mathbf{\\epsilon}\\mathbf{\\epsilon}^T]  = \\begin{bmatrix} \\sigma^2  & 0 & \\dots & 0 \\\\\n0 & \\sigma^2  & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\dots & \\sigma^2\n\\end{bmatrix} = \\sigma^2 \\mathbf{I}\n\\tag{10}\\]\nwhere \\(\\mathbf{I}\\) is the \\(n \\times n\\) identity matrix.\nNext, we show that \\(\\hat{\\boldsymbol{\\beta}} = \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\epsilon\\).\nRecall that the \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\) and \\(\\mathbf{Y} = \\mathbf{X}\\mathbf{\\beta} + \\mathbf{\\epsilon}\\). Then,\n\\[\n\\begin{aligned}\n\\hat{\\boldsymbol{\\beta}} &= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} \\\\\n&= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T(\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}) \\\\\n&= \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T \\mathbf{\\epsilon} \\\\\n\\end{aligned}\n\\tag{11}\\]\nUsing these two properties, we derive the form of the variance-covariance matrix for the coefficients. Note that the covariance matrix is \\(E[(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})^T]\\)\n\\[\n\\begin{aligned}\nE[(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})^T] &= E[(\\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon} - \\boldsymbol{\\beta})(\\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon} - \\boldsymbol{\\beta})^T]\\\\\n& = E[(\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}] \\\\\n& = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T]\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n& = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T (\\sigma^2\\mathbf{I})\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n&= \\sigma^2\\mathbf{I}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n& = \\sigma^2\\mathbf{I}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n&  = \\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1} \\\\\n\\end{aligned}\n\\tag{12}\\]"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus BIOS 6312",
    "section": "",
    "text": "Learn how to use modern regression methods to answer scientific questions\nBecome familiar with statistical concepts including exploratory data analysis, estimation, testing in linear, logistic, and survival models\nUnderstand how the development of statistical methodology is motivated by biological and medical problems\nDevelop data analytic skills including familiarity with several statistical programs\nDevelop writing skills needed to communicate the results of a data analysis\nIntroduce reproducible research approaches using R, Rstudio, and Quarto"
  },
  {
    "objectID": "syllabus.html#course-goals",
    "href": "syllabus.html#course-goals",
    "title": "Syllabus BIOS 6312",
    "section": "",
    "text": "Learn how to use modern regression methods to answer scientific questions\nBecome familiar with statistical concepts including exploratory data analysis, estimation, testing in linear, logistic, and survival models\nUnderstand how the development of statistical methodology is motivated by biological and medical problems\nDevelop data analytic skills including familiarity with several statistical programs\nDevelop writing skills needed to communicate the results of a data analysis\nIntroduce reproducible research approaches using R, Rstudio, and Quarto"
  },
  {
    "objectID": "syllabus.html#topics-of-discussion",
    "href": "syllabus.html#topics-of-discussion",
    "title": "Syllabus BIOS 6312",
    "section": "Topics of Discussion",
    "text": "Topics of Discussion\n\nBayesian and Frequentist approaches to fitting regression models\nLinear regression\nLogistic regression\nPoisson regression\nSurvival models, primarily Cox Regression\nMultinomial and ordinal logistic regression\nMultivariable Regression\nMatrix algebra and important results of random vectors\nPrecision, effect modification, and confounding\nSpecification issues in regression models\nModel selection\nCase Studies\nUnderstanding model assumptions and the impact of assumptions on interpretation\nModel Checking: diagnostics, transformations, influential observations, lack-of-fit test"
  },
  {
    "objectID": "syllabus.html#course-description",
    "href": "syllabus.html#course-description",
    "title": "Syllabus BIOS 6312",
    "section": "Course Description",
    "text": "Course Description\n\nBIOS 6312\nThis is the second in a two-course series designed for students who seek to develop skills in modern biostatistical reasoning and data analysis. Students learn modern regression analysis and modeling building techniques from an applied perspective. Theoretical principles will be demonstrated with real-world examples from biomedical studies. This course requires substantial statistical computing in software packages and focuses on R; familiarity with R or proficiency in another high-level statistical program (e.g. Stata) is required. The course covers regression modeling for continuous outcomes, including simple linear regression, multiple linear regression, and analysis of variance with one-way, two-way, three-way, and analysis of covariance models. This is a brief introduction to models for binary outcomes (logistic models), ordinal outcomes (proportional odds models), count outcomes (Poisson/negative binomial models), and time to event outcomes (Kaplan-Meier curves, Cox proportional hazard modeling). Incorporated into the presentation of these models are subtopic topics such as regression diagnostics, nonparametric regression, splines, data reduction techniques, model validation, parametric bootstrapping, and a brief introduction to methods for handling missing data. Students are required to take 6312L concurrently. Prerequisite: Biostatistics 6311 or equivalent; familiarity with R or Stata software packages. SPRING.\n\n\nBIOS 6312L\nThis is a discussion section/lab for Modern Regression Analysis. Students will review relevant theory and work on applications as a group. Computing solutions and extensions will be emphasized. Students are required to take 6312 concurrently."
  },
  {
    "objectID": "syllabus.html#course-materials",
    "href": "syllabus.html#course-materials",
    "title": "Syllabus BIOS 6312",
    "section": "Course Materials",
    "text": "Course Materials\n\nCourse notes\n\nCourse notes will be the primary source\nAvailable on web page\nDaily class schedule will indicate notes being covered\nNotes will be updated throughout semester\n\n\n\nTextbooks\n\nThere are no required textbooks for this course\nThe following are provided as references that are at an appropriate level for this course\n\nRegression Methods in Biostatistics. Vittinghoff, Glidden, Shiboski, and McCulloch\nApplied Liner Regression. Weisberg.\nBayesian and Frequentist Regression Methods. Wakefield"
  },
  {
    "objectID": "syllabus.html#grading-and-evaluation",
    "href": "syllabus.html#grading-and-evaluation",
    "title": "Syllabus BIOS 6312",
    "section": "Grading and Evaluation",
    "text": "Grading and Evaluation\n\nEvaluation components and grade percentages\n\nMidterm (25%)\nTake Home Exam (25%)\nFinal Exam (25%)\nHomework (25%)\nClass participation\nThis is a 4-credit course. Your lab and lecture grades will be the same\n\n\n\nHomework\n\nUp to 1 per week (probably 6 or 7 total)\nWill focus on real data analysis and interpretation with some mathematical derivations of important quantities\nQuestions will focus on specific analyses, with questions stated in as scientific terms as possible\nWork handed in should address the scientific questions\n\nFormat Table and Figures\n\nKeys will be provided shortly after the homework is turned in\n\nNo late homework accepted after the key is posted\n\nAnswers in keys may go beyond what is expected of your homework and present concepts in more detail. You are responsible for any material in the keys for exams.\nYou may discuss the homework with others in the class, but the work you turn in should be your own\nUse Brightspace to turn in homeworks and receive feedback and grade\n\n\n\nIn Class Exams\n\nMidterm and Final in class\n\nFocus on understanding concepts, not memorizing formulas\nI will provide an example midterm and final\nFor midterm, you will be allowed 1 page of your own notes\nFor final, you will be allowed 2 pages of your own notes\n\nAll output will be provide for you to interpret\n\n\n\nTake Home Exam\n\nWill be given approximately mid point between Midterm and Final\nDemonstrate ability to obtain results through software and interpret findings\nOne day to complete and turn in\n\nLikely will be a Monday with no lab scheduled for that day\n\nSimilar to Homework, but work must be your own"
  },
  {
    "objectID": "syllabus.html#expectations-and-policies",
    "href": "syllabus.html#expectations-and-policies",
    "title": "Syllabus BIOS 6312",
    "section": "Expectations and Policies",
    "text": "Expectations and Policies\n\nExpecations you can have of me\n\nYou should expect me to provide feedback on homeworks and exams in a timely fashion\nYou should expect me to be responsive to your questions and concerns. If you have emailed me and not received a response with 24 hours, please feel free to email again. It is best to use my VUMC rather than Vanderbilt email address.\n\nAttendance. The course is offered in-person. If you expect to be absent, please let me know and make plans to catch up. Class will start on time.\nCollaboration\n\nDiscussing course content is highly encouraged\nCollaborating on homeworks is highly encouraged, but you need to turn in your own assignment written in your own words\nExams (take home and in class) are individual effort\n\nAcademic honesty\n\nStudents are expected to follow the Vanderbilt Honor Code\n“Vanderbilt University students pursue all academic endeavors with integrity. They conduct themselves honorably, professionally and respectfully in all realms of their studies in order to promote and secure an atmosphere of dignity and trust. The keystone of our honor system is self-regulation, which requires cooperation and support from each member of the University community.”\n\nUse of generative AI algorithms such as ChatGPT\n\nYou are free to use generative AI in your work, however you must\n\nCite any text that AI generated (even if you edited it) with a bibliographic entry indicating the name and version of the AI model that you used, the date and time it was used, and the exact query or prompt to generate the results\nCite as above any code that was generated for you. I recommend that you do not use it to blindly write code for you. Doing so will probably be more work than simply writing the code yourself. You must verify any code that is generated for you is accurate and answers the question in the assignment instructions.\n\nI recommend using Vanderbilt or VUMC resources\n\nhttps://www.vanderbilt.edu/generative-ai/\nhttps://www.vumc.org/dbmi/GenerativeAI\n\n\nLate work\n\nWhile I expect that work will be turned in on time, things can happen to interrupt your schedule\nMy goal is to provide sufficient time for completing all assignments\nIf you anticipate a problem with a due date, it is best to let me know sooner (e.g. when a homework is assigned) rather than later\nLate homeworks will be accepted on a case by case basis. No late homeworks will be accepted after the key is provided.\n\nVoicing concerns and evaluations\n\nPlease feel free to bring up any concerns you have about the course material, how it is being presented, or how you are being evaluated at any time during the semester. I want you to know that your voice will be heard.\nPlease complete the end of course evaluations. They are a valuable resource for me and help to guide changes from year to year. I read all comments and will take them seriously. Comments about what worked well as well as constructive criticism are appreciated"
  },
  {
    "objectID": "syllabus.html#accommodations",
    "href": "syllabus.html#accommodations",
    "title": "Syllabus BIOS 6312",
    "section": "Accommodations",
    "text": "Accommodations\n\nI encourage students who encounter accessibility challenges to communicate with me regardless of whether they are registered with Equal Opportunity and Access\nPlease communicate with me at your earliest convenience so we can discuss specific actions to address your needs. I will make every effort to accommodate reasonable requests.\nIf you have established accommodations with Equal Opportunity and Access, I will receive an email notifying me of the request.\nIf you need to contact Equal Opportunity and Access to establish service, the address is https://www.vanderbilt.edu/eeo/disability_services/contact_us.php"
  },
  {
    "objectID": "computing.html",
    "href": "computing.html",
    "title": "Computing",
    "section": "",
    "text": "Course notes and primary examples are based on R. I used the following version of R to build the course notes\n\nR.version$version.string\n\n[1] \"R version 4.3.2 (2023-10-31)\"\n\n\nIt is best to be using the latest version of R."
  },
  {
    "objectID": "computing.html#r",
    "href": "computing.html#r",
    "title": "Computing",
    "section": "",
    "text": "Course notes and primary examples are based on R. I used the following version of R to build the course notes\n\nR.version$version.string\n\n[1] \"R version 4.3.2 (2023-10-31)\"\n\n\nIt is best to be using the latest version of R."
  },
  {
    "objectID": "computing.html#rstudio",
    "href": "computing.html#rstudio",
    "title": "Computing",
    "section": "Rstudio",
    "text": "Rstudio\nCurrent versions of RStudio support editing and preview of Quarto documents.\nTo efficiently use Quarto within RStudio it is strongly recommended that you use the latest release of RStudio. You need to be using RStudio (v2023.06) or later.\nYou can download RStudio from https://posit.co/download/rstudio-desktop/."
  },
  {
    "objectID": "computing.html#quarto",
    "href": "computing.html#quarto",
    "title": "Computing",
    "section": "Quarto",
    "text": "Quarto\nQuarto is a multi-language, next generation version of R Markdown from RStudio, with many new features and capabilities. Like R Markdown, Quarto uses Knitr to execute R code, and is therefore able to render most existing Rmd files without modification.\nTo use Quarto with R, you should install the rmarkdown R package:\ninstall.packages(\"rmarkdown\")\nInstallation of the rmarkdown package will also install the knitr package so you will have everything required to render documents containing R code."
  },
  {
    "objectID": "computing.html#r-packages",
    "href": "computing.html#r-packages",
    "title": "Computing",
    "section": "R packages",
    "text": "R packages\nThere are many ways to achieve the same results in R. To create the notes, I utilize base R and a variety of different packages that provide one way of obtaining the results. The packages needed used to create the notes are listed at the beggining of each set of notes.\nIf you do not have the packages I used installed, they will need to be installed first to recreate the analyses in the notes. The packages I use and demonstrate may not be unique. Other packages may do something similar and allow you to arrive at the same final answer.\n\nInstalling and loading R packages\nThe simplest way to install an R packages is to use the install.package function. For example,\ninstall.packages(\"rms\")\nAfter installation, you can load the package\nlibrary(\"rms\")\nSome packages install smoothly, others can generate errors if necessary dependencies are not installed. I will add known issues and solutions to this document as needed throughout the course."
  },
  {
    "objectID": "computing.html#stata",
    "href": "computing.html#stata",
    "title": "Computing",
    "section": "Stata",
    "text": "Stata\nStata also should be able to conduct all of these analyses necessary for this course. The main advantage to Stata is that its syntax is relatively straightforward. It also has a menu system that can be used to create code and make the analysis reproducible through .do files.\nUCLA Statistical Methods and Data Analytics maintain a good website for those interested in learning more about the capabilities of Stata\nThere is a Statamarkdown package under development that allows for Stata code to be used in Quarto. I do not have wide experience with this package, but more information is available at https://github.com/hemken/Statamarkdown. It is of no use if you do not have Stata installed."
  },
  {
    "objectID": "labs/lab2-linearassumptions.html",
    "href": "labs/lab2-linearassumptions.html",
    "title": "Lab 2: Assumption in Linear Regression",
    "section": "",
    "text": "Original simulation (Patrick Ward)\n\nSimulations in R Part 5: Homoskedasticity Assumption in Regression\nConsult parts 1 to 4 if you want more background information simulations and resampling in R\n\nModifications (Chris Slaughter)\n\nAllow for different sample sizes\nAdd robust standard error estimates for comparison\n\nUse the rms packages and ols function to fit linear models\n\nConsider different error distributions"
  },
  {
    "objectID": "labs/lab2-linearassumptions.html#introduction",
    "href": "labs/lab2-linearassumptions.html#introduction",
    "title": "Lab 2: Assumption in Linear Regression",
    "section": "",
    "text": "Original simulation (Patrick Ward)\n\nSimulations in R Part 5: Homoskedasticity Assumption in Regression\nConsult parts 1 to 4 if you want more background information simulations and resampling in R\n\nModifications (Chris Slaughter)\n\nAllow for different sample sizes\nAdd robust standard error estimates for comparison\n\nUse the rms packages and ols function to fit linear models\n\nConsider different error distributions"
  },
  {
    "objectID": "labs/lab2-linearassumptions.html#regression-assumptions",
    "href": "labs/lab2-linearassumptions.html#regression-assumptions",
    "title": "Lab 2: Assumption in Linear Regression",
    "section": "Regression Assumptions",
    "text": "Regression Assumptions\nThere are a number of assumptions that underpin linear regression models. Simulation can be a useful way of exploring these assumptions and understanding how violating these assumptions can lead to bias, large variance in the regression coefficients, and/or poor predictions.\nSome typical assumptions include:\n\nHomoskedasticity\nMulticollinearity of independent variables\nMeasurement Error\nSerial correlation\n\nToday, we will explore the assumption of homoskedasticity and the distribution of the error terms for varying sample sizes."
  },
  {
    "objectID": "labs/lab2-linearassumptions.html#coverage-probability",
    "href": "labs/lab2-linearassumptions.html#coverage-probability",
    "title": "Lab 2: Assumption in Linear Regression",
    "section": "Coverage Probability",
    "text": "Coverage Probability\nWe will primarily be evaluating the coverage probability. The coverage probability is the probability that confidence interval will contain the true values of interest. Coverage probability is a common frequentist statistic that is used to describe the behavior of a model.\nWe can estimate the the coverage probability through statistical simulation. If we simulate many datasets and calculate a 95% confidence interval based on each dataset, we can count the number of times that the confidence interval contains the true value. If a 95% CI contains the true value 95% of the time, the coverage probability is correct."
  },
  {
    "objectID": "labs/lab2-linearassumptions.html#lab-instructions",
    "href": "labs/lab2-linearassumptions.html#lab-instructions",
    "title": "Lab 2: Assumption in Linear Regression",
    "section": "Lab Instructions",
    "text": "Lab Instructions\n\nConsider the following simulation as currently written. Run the simulation an obtain the results for baseline simulation and the homoskedastiicity simulation.\nModify the simulation code to include robust standard error estimates in addition to classical standard error estimates. Then, compare the coverage probabilities of the intercept and slope when using robust standard errors to classical standard errors.\nCreate a new section to allow us to modify the models further. Consider\n\nDifferent sample sizes, particularly sample sizes below 500. How does sample size impact the coverage probabilities when using the robust and classical standard error estimates?\nDifferent distributions for the error variance rather than Normal. Such as,\n\nt-distribution with 1 d.f. (Cauchy; very heavy tailed)\nt-distribution with 7 d.f. (heavy tailed, less so)\nexponential (1) - 1 (skewed right, subtract 1 so has mean 0)\nOther distributions of your choice\n\nI will collect results from the class and collect in a table so we can see that patterns, if any, that appear"
  },
  {
    "objectID": "labs/lab2-linearassumptions.html#creating-the-baseline-simulation",
    "href": "labs/lab2-linearassumptions.html#creating-the-baseline-simulation",
    "title": "Lab 2: Assumption in Linear Regression",
    "section": "Creating the baseline simulation",
    "text": "Creating the baseline simulation\nBefore exploring how violations of the homoskedasticity assumption influence a regression model, we need a baseline model to compare it against. So, we will begin by simulating a simple linear regression with 1 predictor. Our model will look like this:\n\\(y = 2 + 5*x + e\\)\nWhere e will be random error from a normal distribution with a mean of 0 and standard deviation of 1.\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n\n\n✔ broom        1.0.5     ✔ recipes      1.0.9\n✔ dials        1.2.0     ✔ rsample      1.2.0\n✔ dplyr        1.1.2     ✔ tibble       3.2.1\n✔ ggplot2      3.4.4     ✔ tidyr        1.3.0\n✔ infer        1.0.5     ✔ tune         1.1.2\n✔ modeldata    1.2.0     ✔ workflows    1.1.3\n✔ parsnip      1.1.1     ✔ workflowsets 1.0.1\n✔ purrr        1.0.2     ✔ yardstick    1.2.0\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Search for functions across packages at https://www.tidymodels.org/find/\n\nlibrary(patchwork)\nlibrary(rms)\n\nLoading required package: Hmisc\n\n\nLoading required package: lattice\n\n\nLoading required package: survival\n\n\nLoading required package: Formula\n\n\n\nAttaching package: 'Hmisc'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    knitrSet\n\n\nThe following object is masked from 'package:parsnip':\n\n    translate\n\n\nThe following objects are masked from 'package:dplyr':\n\n    src, summarize\n\n\nThe following objects are masked from 'package:base':\n\n    format.pval, units\n\n\nLoading required package: SparseM\n\n\n\nAttaching package: 'SparseM'\n\n\nThe following object is masked from 'package:base':\n\n    backsolve\n\n## set seed for reproducibility\nset.seed(58968)\n\n## create a data frame to store intercept values, slope values, their standard errors, and the model residual standard error, for each simulation\nsim_params &lt;- data.frame(intercept = NA,\n                      slope = NA,\n                      intercept_se = NA,\n                      slope_se = NA,\n                      model_rse = NA)\n\n# true intercept value\nintercept &lt;- 2\n\n# true slope value\nslope &lt;- 5\n\n## Number of indepdendent observations\nn &lt;- 500\n\n## Number of simulation replications to run\nreps &lt;- 5000\n\n# random draw from a uniform distribution to simulate the predictor variable\nX &lt;- runif(n = n, min = -1, max = 1)\n\n## loop for regression model\nfor(i in 1:reps){\n  \n  # create dependent variable, Y\n  Y &lt;- intercept + slope*X + rnorm(n = n, mean = 0, sd = 1)\n  \n  # build model\n  model &lt;- ols(Y ~ X, x=TRUE)\n  \n  # # store predictions\n  fitted_vals &lt;- model$fitted.values\n\n  # # store residuals\n  # output_df[i, 2] &lt;- model$residuals\n  \n  # variance-covariance matrix for the model\n  vcv &lt;- vcov(model)\n  \n  # estimates for the intercept\n  sim_params[i, 1] &lt;- model$coef[1]\n  \n  # estimates for the slope\n  sim_params[i, 2] &lt;- model$coef[2]\n  \n  # SE for the intercept\n  sim_params[i, 3] &lt;- sqrt(diag(vcv)[1])\n  \n  # SE for the slope\n  sim_params[i, 4] &lt;- sqrt(diag(vcv)[2])\n  \n  # model RSE\n  sim_params[i, 5] &lt;- model$stats[\"Sigma\"]\n  \n}\n\nhead(sim_params)\n\n  intercept    slope intercept_se   slope_se model_rse\n1  1.966323 4.977753   0.04495439 0.07883016 1.0036923\n2  1.947826 4.966831   0.04460191 0.07821206 0.9958226\n3  2.111264 4.900025   0.04508832 0.07906500 1.0066825\n4  2.029259 4.970287   0.04317960 0.07571795 0.9640667\n5  1.976044 5.064834   0.04446964 0.07798012 0.9928694\n6  1.977989 5.071687   0.04461868 0.07824146 0.9961969\n\n\nNow we summarize the data to see if we have values close to the specified model parameters\n\n# Calculate mean for each column of sim_params\napply(sim_params, 2, mean)\n\n   intercept        slope intercept_se     slope_se    model_rse \n  1.99925865   4.99970448   0.04479625   0.07855285   1.00016164 \n\n\nThe final model of the ‘r reps’ iterations is also stored from our for loop and we can look directly at it and create plots of the model fit.\n\nd&lt;- datadist(X)\noptions(datadist=\"d\")\n# model summary\nmodel\n\nLinear Regression Model\n \n ols(formula = Y ~ X, x = TRUE)\n \n                  Model Likelihood    Discrimination    \n                        Ratio Test           Indexes    \n Obs     500    LR chi2    1121.81    R2       0.894    \n sigma0.9745    d.f.             1    R2 adj   0.894    \n d.f.    498    Pr(&gt; chi2)  0.0000    g        3.264    \n \n Residuals\n \n      Min       1Q   Median       3Q      Max \n -2.42004 -0.63347 -0.02173  0.71448  2.72872 \n \n \n           Coef   S.E.   t     Pr(&gt;|t|)\n Intercept 2.0326 0.0436 46.57 &lt;0.0001 \n X         4.9585 0.0765 64.78 &lt;0.0001 \n \n\n# model fit plots\nplot(model, which=1)\n\n\n\n\nWe can also create a function that lets us evaluate how often the 95% confidence interval of our simulated beta coefficients cover the true beta coefficients that we specified for the simulation. From there, we can get a coverage probability and a 95% probability coverage interval\n\n### Create a coverage probability function\ncoverage_interval95 &lt;- function(beta_coef, se_beta_coef, true_beta_val, model_df){\n  \n  level95 &lt;- 1 - (1 - 0.95) / 2\n  \n  # lower 95\n  lower95 &lt;- beta_coef - qt(level95, df = model_df)*se_beta_coef\n  \n  # upper 95\n  upper95 &lt;- beta_coef + qt(level95, df = model_df)*se_beta_coef\n  \n  # what rate did we cover the true value (hits and misses)\n  hits &lt;- ifelse(true_beta_val &gt;= lower95 & true_beta_val &lt;= upper95, 1, 0)\n  prob_cover &lt;- mean(hits)\n  \n  # create the probability coverage intervals\n  low_coverage_interval &lt;- prob_cover - 1.96 * sqrt((prob_cover * (1 - prob_cover)) / length(beta_coef))\n  \n  upper_coverage_interval &lt;- prob_cover + 1.96 * sqrt((prob_cover * (1 - prob_cover)) / length(beta_coef))\n  \n  # results in a list\n  return(list('Probability of Covering the True Value' = prob_cover,\n              '95% Probability Coverage Intervals' = c(low_coverage_interval, upper_coverage_interval)))\n  \n}\n\nLet’s apply it to the intercept.\n\ncoverage_interval95(beta_coef = sim_params$intercept,\n                    se_beta_coef = sim_params$intercept_se,\n                    true_beta = intercept,\n                    model_df = model$df.residual)\n\n$`Probability of Covering the True Value`\n[1] 0.948\n\n$`95% Probability Coverage Intervals`\n[1] 0.9418457 0.9541543\n\n\nNow apply it to the slope\n\ncoverage_interval95(beta_coef = sim_params$slope,\n                    se_beta_coef = sim_params$slope_se,\n                    true_beta = slope,\n                    model_df = model$df.residual)\n\n$`Probability of Covering the True Value`\n[1] 0.9538\n\n$`95% Probability Coverage Intervals`\n[1] 0.9479814 0.9596186\n\n\nIn both cases we are covering the true betas around 95% of the time, with relatively small intervals."
  },
  {
    "objectID": "labs/lab2-linearassumptions.html#homoskedasticity",
    "href": "labs/lab2-linearassumptions.html#homoskedasticity",
    "title": "Lab 2: Assumption in Linear Regression",
    "section": "Homoskedasticity",
    "text": "Homoskedasticity\nLinear models make an assumption that the variance of the residuals remain constant across the predicted values (homoskedastic). We can see what this looks like by plotting the fitted values relative to the residuals, which was the first plot in the model check plots we created for the last simulation above. We can see that the residuals exhibit relatively the same amount of variance across the fitted values.\n\nplot(model, which = 1)\n\n\n\n\nLet’s simulate a model with heteroskedastic residuals and see what it looks like. We will keep the same intercept and slope parameters as above. The only thing will we do is add an exponential parameter to the error term of the model to create a heteroskedastic outcome in the residuals.\n\n## parameter for heteroskedasticity \nheteroskedasticity_param &lt;- 2\n\n## set seed for reproducibility\nset.seed(22)\n\n## data frame for results\nheteroskedastic_sim_params &lt;- data.frame(intercept = NA,\n                      slope = NA,\n                      intercept_se = NA,\n                      slope_se = NA,\n                      model_rse = NA)\n\n## for loop\nfor(i in 1:reps){\n  \n  # the error variance of Y is a function of X plus some random noise\n  Y &lt;- intercept + slope*X + rnorm(n = n, mean = 0, sd = exp(X*heteroskedasticity_param))\n  \n  # model\n  heteroskedastic_model &lt;- ols(Y ~ X, x=TRUE)\n  \n  \n  # variance-covariance matrix\n  vcv &lt;- vcov(heteroskedastic_model)\n  \n  # estimates for the intercept\n  heteroskedastic_sim_params[i, 1] &lt;- heteroskedastic_model$coef[1]\n  \n  # estimates for the slope\n  heteroskedastic_sim_params[i, 2] &lt;- heteroskedastic_model$coef[2]\n  \n  # SE for the intercept\n  heteroskedastic_sim_params[i, 3] &lt;- sqrt(diag(vcv)[1])\n  \n  # SE for the slope\n  heteroskedastic_sim_params[i, 4] &lt;- sqrt(diag(vcv)[2])\n  \n  # model RSE\n  heteroskedastic_sim_params[i, 5] &lt;- heteroskedastic_model$stats[\"Sigma\"]\n  \n}\n\nhead(heteroskedastic_sim_params)\n\n  intercept    slope intercept_se  slope_se model_rse\n1  1.884165 4.743169    0.0989576 0.1735279  2.209417\n2  2.096654 5.246191    0.1156089 0.2027270  2.581189\n3  1.950191 4.884566    0.1109784 0.1946072  2.477805\n4  1.996244 5.062478    0.1135296 0.1990809  2.534765\n5  2.048404 5.156628    0.1130880 0.1983065  2.524906\n6  2.043749 4.992856    0.1283863 0.2251330  2.866469\n\nplot(X, Y, pch = 19)\n\n\n\n\nThe relationship between X and Y certainly looks weird given how it starts very tightly on the left side and then fans out on the right side.\nLet’s take the average across all simulations for each coefficient and their corresponding standard errors.\n\napply(heteroskedastic_sim_params, 2, mean)\n\n   intercept        slope intercept_se     slope_se    model_rse \n   2.0006111    5.0038511    0.1119348    0.1962842    2.4991571 \n\n\nThe coefficients of 2.0 for the intercept and 5 for the slope are exactly what we set them as for the simulation. However, notice how much larger the standard errors are for the intercept and slope compared to the original model above. Additionally, notice that the model residual standard error has increased substantially compared to the previous model.\nLet’s get the last model again and check out the fitted vs residual plot.\n\n# fitted vs residuals\nplot(heteroskedastic_model, which = 1)\n\n\n\n\nThat looks like a large amount of heteroskedasticity as the residual variance is no longer homogenous across the range of fitted values. Notice the large fanning out towards the right side of the plot. As the predictions get larger so two does the variability in residuals, which we noticed when we plotted Y and X above.\nWhat we’ve learned is that the estimate of intercept and slope is unbiased for both the heteroskedastic and homoskedastic models, as they both are centered on the parameters that we specified for the simulation (intercept = 2, slope = 5). However, the heteroskedastic model creates greater variance in our coefficients. We can visualize how much uncertainty there is under the heteroskedastic model relative to the homoskedastic model by visualizing the density of the coefficient estimates from our two model simulations.\n\nplt_intercept &lt;- sim_params %&gt;%\n  mutate(model = 'homoskedastic model') %&gt;%\n  bind_rows(\n    heteroskedastic_sim_params %&gt;%\n      mutate(model = 'heteroskedastic model')\n  ) %&gt;%\n  ggplot(aes(x = intercept, fill = model)) +\n  geom_density(alpha = 0.6) +\n  theme_classic() +\n  theme(legend.position = \"top\")\n\nplt_slope &lt;- sim_params %&gt;%\n  mutate(model = 'homoskedastic model') %&gt;%\n  bind_rows(\n    heteroskedastic_sim_params %&gt;%\n      mutate(model = 'heteroskedastic model')\n  ) %&gt;%\n  ggplot(aes(x = slope, fill = model)) +\n  geom_density(alpha = 0.6) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\nplt_intercept | plt_slope\n\n\n\n\nFinally, let’s see how often the 95% coverage interval is covering the true intercept and slope in the heteroskedastic model.\n\ncoverage_interval95(beta_coef = heteroskedastic_sim_params$intercept,\n                    se_beta_coef = heteroskedastic_sim_params$intercept_se,\n                    true_beta = intercept,\n                    model_df = model$df.residual)\n\n$`Probability of Covering the True Value`\n[1] 0.9344\n\n$`95% Probability Coverage Intervals`\n[1] 0.9275374 0.9412626\n\ncoverage_interval95(beta_coef = heteroskedastic_sim_params$slope,\n                    se_beta_coef = heteroskedastic_sim_params$slope_se,\n                    true_beta = slope,\n                    model_df = model$df.residual)\n\n$`Probability of Covering the True Value`\n[1] 0.829\n\n$`95% Probability Coverage Intervals`\n[1] 0.8185637 0.8394363\n\n\nNotice that we are no longer covering the true model values at the 95% level."
  },
  {
    "objectID": "labs/quarto-intro.html",
    "href": "labs/quarto-intro.html",
    "title": "Quarto Introduction",
    "section": "",
    "text": "Open the quarto introduction file in your Rstudio and render to create an html output file\nLearn some basic markdown syntax to include text and run code\nAlter the YAML to include your name and output to a different file format (e.g. docx)"
  },
  {
    "objectID": "labs/quarto-intro.html#learning-objectives",
    "href": "labs/quarto-intro.html#learning-objectives",
    "title": "Quarto Introduction",
    "section": "",
    "text": "Open the quarto introduction file in your Rstudio and render to create an html output file\nLearn some basic markdown syntax to include text and run code\nAlter the YAML to include your name and output to a different file format (e.g. docx)"
  },
  {
    "objectID": "labs/quarto-intro.html#r-packages-used",
    "href": "labs/quarto-intro.html#r-packages-used",
    "title": "Quarto Introduction",
    "section": "R packages used",
    "text": "R packages used\nFor this introduction, I used three R packages. If you have never used these before, they will need to be installed once, e.g.\ninstall.packages(\"ggplot2\")\ninstall.packages(\"rms\")\ninstall.packages(\"Hmisc\")\nIf they have already been installed, you can load these packages.\n\nlibrary(ggplot2)\nlibrary(rms)\n\nLoading required package: Hmisc\n\n\nLoading required package: lattice\n\n\nLoading required package: survival\n\n\nLoading required package: Formula\n\n\n\nAttaching package: 'Hmisc'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    knitrSet\n\n\nThe following objects are masked from 'package:base':\n\n    format.pval, units\n\n\nLoading required package: SparseM\n\n\n\nAttaching package: 'SparseM'\n\n\nThe following object is masked from 'package:base':\n\n    backsolve\n\nlibrary(Hmisc)"
  },
  {
    "objectID": "labs/quarto-intro.html#quarto",
    "href": "labs/quarto-intro.html#quarto",
    "title": "Quarto Introduction",
    "section": "Quarto",
    "text": "Quarto\nQuarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org.\nQuarto includes many more features than I will cover in this course. It is not a learning objective of this course that you learn all of the features included in Quarto. Instead, I want to cover a few of the most useful things and provide a starting point for those interested in learning more."
  },
  {
    "objectID": "labs/quarto-intro.html#running-code",
    "href": "labs/quarto-intro.html#running-code",
    "title": "Quarto Introduction",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed).\n\nPlots\nYou can include plots\n\nggplot(cars, aes(x=speed, y=dist)) + \n  geom_smooth() + \n  xlab(\"Speed (mph)\") + \n  ylab(\"Stopping Distance (feet)\")\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nDate recorded in the 1920s\n\n\n\nSpeed and stopping distance of cars\n\n\n\nTables\nAnd tables that are simple text\n\n# Create some data\nset.seed(12345) # Fpr reproducibility\nexampledata &lt;- data.frame(age=rnorm(500,50,5),\n                          sbp=rnorm(500,120,12),\n                          trt=factor(sample(c(\"Drug\",\"Placebo\"), 500, replace=TRUE))\n)\nf &lt;- summaryM(age + sbp ~ trt, data=exampledata, test=TRUE)\nprint(f, digits=2)\n\n\n\nDescriptive Statistics  (N=500)\n\n+---+------------------+------------------+------------------------+\n|   |Drug              |Placebo           |  Test                  |\n|   |(N=260)           |(N=240)           |Statistic               |\n+---+------------------+------------------+------------------------+\n|age|        47/50/53  |        47/51/54  | F=0.5 d.f.=1,498 P=0.48|\n+---+------------------+------------------+------------------------+\n|sbp|       113/120/129|       112/120/128|F=0.02 d.f.=1,498 P=0.89|\n+---+------------------+------------------+------------------------+\n\n\nTable can also be formatted to improve appearance. Here is an html version of the same table.\n\nhtml(f, digits=3)\n\n\n\n\n\n\n\n\n\n\nDescriptive Statistics (N=500).\n\n\n\nDrug\nN=260\nPlacebo\nN=240\nTest Statistic\n\n\n\n\n\nage\n47.4 50.3 53.2\n47.0 50.7 54.0\nF1 498=0.5, P=0.479\n\n\nsbp\n113 120 129\n112 120 128\nF1 498=0.02, P=0.89\n\n\n\na b c represent the lower quartile a, the median b, and the upper quartile c for continuous variables.\nTest used: Wilcoxon test ."
  },
  {
    "objectID": "labs/quarto-intro.html#quarto-yaml",
    "href": "labs/quarto-intro.html#quarto-yaml",
    "title": "Quarto Introduction",
    "section": "Quarto YAML",
    "text": "Quarto YAML\nThe beginning of this document of the document contains metadata that controls the document appearance, output format, and many other options. The current lines were automatically generated when I create a new document (File &gt; New File &gt; Quarto Document…)\nFor longer documents, it can be useful to add a table of contents. A simple toc can added with the line.\ntoc: true\nAlso, the default output type is an html document. This is a good choice, but if you prefer other options, you can alter the yaml to produce Word files, PDFs, or many other options. Tab-completion is supported to see the various options.\nIn order to create PDFs you will need to install a recent distribution of TeX. We recommend the use of TinyTeX (which is based on TexLive), which you can install with the following command:\nquarto install tinytex\nMore details on authoring Quarto documents in Rstudio are available elsewhere."
  },
  {
    "objectID": "labs/quarto-intro.html#thing-to-try",
    "href": "labs/quarto-intro.html#thing-to-try",
    "title": "Quarto Introduction",
    "section": "Thing to try",
    "text": "Thing to try\n\nAlter the YAML to include your name\nAlter the YAML to output to a different file format, e.g. docx\nAdd some text, code, or graphics and render the document"
  },
  {
    "objectID": "notes/Lec02.review.html",
    "href": "notes/Lec02.review.html",
    "title": "Review of Key Concepts",
    "section": "",
    "text": "Code\nlibrary(Exact)"
  },
  {
    "objectID": "notes/Lec02.review.html#samples-from-populations",
    "href": "notes/Lec02.review.html#samples-from-populations",
    "title": "Review of Key Concepts",
    "section": "1 Samples from populations",
    "text": "1 Samples from populations\n\nScientific hypotheses concern a population\n\nDo teenagers learn math more quickly using the Singapore Method?\nWill people with heart disease live longer if they are prescribed medication X?\n\nTeenagers and people with heart disease, are the populations of interest. However, it is rare that we can ever study the entire population\nThe purpose of inferential statistics is to make valid inference about a populations based on a sample from that population\n\nWe commonly make inference about population parameters such as the mean, median, probability, odds, etc."
  },
  {
    "objectID": "notes/Lec02.review.html#inference-and-variability",
    "href": "notes/Lec02.review.html#inference-and-variability",
    "title": "Review of Key Concepts",
    "section": "2 Inference and variability",
    "text": "2 Inference and variability\n\nWith biological questions, there is inevitably variation in the response across repetitions of the experiment\n\nExposure to a carcinogen increases your risk of cancer, but does not guarantee a cancer will develop\nThus, biological questions must be phrased in a probabilistic (not deterministic) language\nDeterministic: Does medication decrease blood pressure?\nProbabilistic: Does medication tend to decrease blood pressure?\n\nThe wording “tends to” is intentionally vague. There are many possible definitions\n\nA lower average value (arithmetic mean)\nA lower geometric mean (arithmetic mean on log scale)\nA lower median\nMedian(Trt) - Median(Ctrl) \\(&lt;\\) 0.0\nMedian(Trt - Ctrl) \\(&lt;\\) 0.0\nA lower proportion exceeding some threshold\nA lower odds of exceeding some threshold\nPr(Trt \\(&gt;\\) Control) \\(&lt;\\) 0.5\nAnd many others...\n\nDefining “tends to” is primarily dictated by scientific considerations\n\nYou, not the data, get to choose which summary measure you care about\nWhich measure is most important to advance science?\n\nIn this course, we will describe the most common models for modeling means (linear regression), odds (logistic regression), hazards (Cox proportional hazards regression), and rates (Poisson or negative binomial regression). Other modeling choices could be used to answer different scientific questions."
  },
  {
    "objectID": "notes/Lec02.review.html#scientific-versus-statistical-questions",
    "href": "notes/Lec02.review.html#scientific-versus-statistical-questions",
    "title": "Review of Key Concepts",
    "section": "3 Scientific versus statistical questions",
    "text": "3 Scientific versus statistical questions\n\nTo formally answer scientific questions, they must be refined into statistical questions\nScientific question: Does aspirin prevent heart attacks?\n\nAn important question, but can’t be addressed by statistics\n\nCause and effect dependent on study design\n\n\nRefinement 1: Do people who take aspirin not have heart attacks?\n\nThis is problematic because it is deterministic. Some people will have heart attacks even if they take aspirin because there is variability across subjects.\n\nRefinement 2: Do people who take aspirin tend to have fewer heart attacks?\n\nThis refinement acknowledges variability in response across subjects, but lacks a control group. We would need to know how many heart attacks they would have otherwise\n\nFinal refinement: Is the incidence of heart attacks less in people who take aspirin than those who do not?\nBasic science: Is the incidence less by any amount?\nClinical science: Is the incidence less by a clinically relevant amount?\n\nNote that we are addressing statistical association, not causation"
  },
  {
    "objectID": "notes/Lec02.review.html#associations-between-variables",
    "href": "notes/Lec02.review.html#associations-between-variables",
    "title": "Review of Key Concepts",
    "section": "4 Associations between variables",
    "text": "4 Associations between variables\n\nAn association exists between two variables if their probability distributions are not independent\nFor random variables \\(X\\) and \\(Y\\) with joint probability density function (pdf) \\(f(x,y)\\), marginal pdfs \\(f_X(x)\\) and \\(f_Y(y)\\), and conditional pdfs \\(f(x | y)\\) and \\(f(y | x)\\)\n\n\\[f(x,y) = f_X(x)*f_Y(y)\\] \\[f(x | y) = f_X(x)\\] \\[f(y | x) = f_Y(y)\\]\n\nIndependence means that there is no way that information about one variable could ever give any information at all about the probability that the other variable might take on a particular value\nAssociation means that the distribution of one variable differs in some way (e.g. mean, median, variance, probability of being greater than 10) across at least two groups differing in their values of the other variable\nCan we ever establish independence between two variables?\n\nYes, but it is very difficult\nTwo variables can be associated in many different ways. It is hard to examine every characteristic of a distribution across groups\n\nConversely, we can show associations as soon as we establish some information that one variable provides about the other\n\nNegative studies (e.g. studies with p \\(&gt;\\) 0.05 or CI that contains the null value) are sometimes misinterpreted as being evidence of no association\n\nHowever, “Absence of evidence is not the same as evidence of absence”\n\nTo make negative studies meaningful, we must...\n\nSpecify the type of association that we are looking for (e.g. mean, median)\nQuantify the amount of uncertainty that might differ across groups\n\nIf the uncertainty is small enough, we can conclude there is no evidence for a meaningful difference\n\n\n\n\n\n4.1 Example: Inference about an association between exposure (E) and disease (D)\n\n5 Exposed (E+) and 5 Unexposed (E-) subjects were followed for one year, and the number of subjects with and without disease are summarized in the following table\n\n\n\nCode\ntab1 &lt;- matrix(c(3,0,2,5),nrow=2, dimnames=list(c(\"E+\",\"E-\"),c(\"D+\",\"D-\")))\ntab1\n\n\n   D+ D-\nE+  3  2\nE-  0  5\n\n\n\nThe number of E+ and E- were fixed by design (5 each). The random variables are the number of subjects (out of 5) who are D+.\nThe scientific question of interest there an association between exposure and increased (or decreased) risk of disease?\nOur analysis plan should take into account the fact that we have a small sample size. For illustration, I will consider both a Bayesian and frequentist approach so the methods can be compared.\n\n\n4.1.1 Frequentist analysis\n\nResults from the unconditional exact test are provided below. The binomial model assumes that the row or columns margins are fixed, which corresponds to the study design. 1\n\n1 A multinomial model would assume the total sample size is fixed, which is a common design but not appropriate here. Fisher’s exact test assumes that both the row and columns margins are fixed, which is rarely found in practice.\n\nCode\n# For comparing proportions\nexact.test(tab1, model=\"binomial\", conf.int = TRUE, to.plot=FALSE)\n\n\n\n    Z-pooled Exact Test\n\ndata:  3 out of 5 vs. 0 out of 5\ntest statistic = 2.0702, first sample size = 5, second sample size = 5,\np-value = 0.06185\nalternative hypothesis: true difference in proportion is not equal to 0\n95 percent confidence interval:\n -4.551214e-05  9.235547e-01\nsample estimates:\ndifference in proportion \n                     0.6 \n\n\nCode\n# For confidnece intervals in each group\nbinom.test(x=3,n=5)\n\n\n\n    Exact binomial test\n\ndata:  3 and 5\nnumber of successes = 3, number of trials = 5, p-value = 1\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.1466328 0.9472550\nsample estimates:\nprobability of success \n                   0.6 \n\n\nCode\nbinom.test(x=0,n=5)\n\n\n\n    Exact binomial test\n\ndata:  0 and 5\nnumber of successes = 0, number of trials = 5, p-value = 0.0625\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.0000000 0.5218238\nsample estimates:\nprobability of success \n                     0 \n\n\n\nHow would you summarize the results of the study examining the association between E and D? Critique the following\n\nAnswer 1: Since the p-value is greater than 0.05, we conclude that there is no association between exposure E and disease D.\nAnswer 2: Since the p-value is greater than 0.05, we lack evidence to conclude that there is an association between exposure E and disease D.\nAnswer 3: We observed incidence rates of 60% in the exposed (95% CI: [15%, 95%]) and 0% in the unexposed (95% CI: [0%, 52%]). The precision of the study was not adequate to demonstrate that such a large difference in incidence rates would be unlikely in the absence of a true association. We estimated a 60% difference in the probability of disease between exposure group (95% CI: [0% to 92%].\n\n\n\n\n4.1.2 Bayesian approach to 2x2 table analysis\n\nA Bayesian analysis requires specifying appropriate prior distributions for our parameter. Let \\(\\theta_1 = \\textrm{Pr(D+|E+)}\\), \\(\\theta_2 = \\textrm{Pr(D+|E-)}\\), and \\(\\theta = \\theta_1 - \\theta_2\\).\nAssume these prior distribution\n\n\\[\\pi(\\theta_1) \\sim Beta(a_1,b_1)\\] \\[\\pi(\\theta_2) \\sim Beta(a_2,b_2)\\]\n\nWe also have to specify the likelihood the number of subjects (out of 5) who are diseased in the Exposed group and the number of subjects who are diseased in the Unexposed group. Note that this is the same binomial likelihood that is assumed in the frequentist approach above.\n\n\\[X_1 \\sim Bin(5, \\theta_1)\\] \\[X_2 \\sim Bin(5, \\theta_2)\\]\n\nUsing Bayes Rule, the posterior distribution is proportional to the likelihood multiplied by the prior distribution\n\n\\[\n\\begin{split}\np(\\theta_1 | X_1) &\\propto p(X_1|\\theta_1) \\times \\pi(\\theta_1) \\\\\n&\\vdots& \\\\\np(\\theta_1 | X_1) &\\sim Beta(X_1+a_1, (5-X_1) + b_1)\n\\end{split}\n\\]\nand\n\\[\n\\begin{split}\np(\\theta_2 | X_2) & \\propto p(X_2|\\theta_2) \\times \\pi(\\theta_2) \\\\\n& \\vdots \\\\\np(\\theta_2 | X_2) & \\sim Beta(X_2+a_2, (5-X_2) + b_2)\n\\end{split}\n\\]\n\nFor the prior distributions, we choose \\(a_1 = a_2 = 0.5\\) and \\(b_1 = b_2 = 2\\). That is, the prior parameters are the same for \\(\\theta_1\\) and \\(\\theta_2\\) with prior mean 0.20.2\nThe prior and posterior distributions for \\(\\theta_1\\) and \\(\\theta_2\\) are summarized below\n\nThe distribution for the Exposed is shifted to the right (relative to the prior) because 3/5 subjects were D+\nThe distribution for the Unexposed is shifted to the left (relative to the prior) because 0/5 subjects were D+\n\n\n2 Mean of Beta(a,b) is a/(a+b)\n\nCode\n# Prior parameters\na &lt;- .5\nb &lt;- 2\n\na/(a+b) # Prior mean\n\n\n[1] 0.2\n\n\nCode\nplot(function(x) dbeta(x,a,b), xlab=\"Theta_1 or Theta_2\", ylab=\"Density\")\nplot(function(x) dbeta(x,a+3,b+2), add=TRUE, col='red')\nplot(function(x) dbeta(x,a+0,b+5), add=TRUE, col='blue')\nlegend(\"topright\",inset=.05,c(\"Prior distribution\",\"Posterior for theta_1 (Exposed)\",\"Posterior for theta_2 (Unexposed)\"), col=c(\"Black\",\"Red\",\"Blue\"), lty=1)\n\n\n\n\n\n\nOur scientific question of interest was to compare the disease rates in the exposed versus the unexposed. This was parameterized as a risk difference, \\(\\theta = \\theta_1 - \\theta_2\\). One could derive the distribution for the difference in two beta random variables for the analytical solution. Instead, I will estimate an approximate numerical solution and summarize the posterior distribution of theta using the 2.5th, 50th, 97.5th quantiles and the posterior mean.\n\n\n\nCode\nset.seed(1234)\nbeta.diff.posterior &lt;- rbeta(10000,a+3,b+2) - rbeta(10000,a+0,b+5)\nbeta.diff.prior &lt;- rbeta(10000,a,b) - rbeta(10000,a,b)\nquantile(beta.diff.posterior, c(.025, .5, .975))\n\n\n      2.5%        50%      97.5% \n0.02510584 0.40590062 0.76104128 \n\n\nCode\nmean(beta.diff.posterior)\n\n\n[1] 0.4015628\n\n\nCode\nplot(density(beta.diff.prior), main=\"\", xlab=\"Theta = Theta_1 - Theta_2\")\nlines(density(beta.diff.posterior), col=\"Red\")\nlegend(\"topleft\",inset=0.05, c(\"Prior\",\"Posterior\"), col=c(\"Black\",\"Red\"), lty=1)\n\n\n\n\n\n\nBayesian Interpretation\n\nA priori, we assumed the probability of disease in the exposed and unexposed groups was 20%, as represented by independent Beta(0.5, 2) distributions. We were interested in estimating the difference in the risk of disease, exposed versus unexposed. Given the data, the posterior mean probability of disease was 40% higher in the exposed group with a 95% credible interval from 2.5% to 76% higher.\n\nComparison of Bayesian approach to frequentist approach\n\nHere the sample size is small, so the prior distribution has an impact on the findings. If you believe the prior is appropriate, then this impact is appropriate. If you believe the prior is inappropriate, then the results should be calculated under different prior assumptions. It is good practice to consider a range of reasonable prior assumptions (which I have not done here) and evaluate the impact on the findings.\nThe mean estimate of the difference between groups (40% Bayesian, 60% frequentist) differ because of the Bayesian priors used. This is an example of shrinkage– the Bayesian estimate is a weighted average of the the prior (0%, no effect) and the data (60%) while the frequentist estimate puts all of the weight on the observed data (60%).\nThe Bayesian credible interval is tighter than the Frequentist confidence interval. Again, this is due the prior distribution being included in the Bayesian calculation. If you feel the prior is appropriate, then this narrowing of the credible interval is also appropriate. As sample size increases, the two approaches would become more similar."
  },
  {
    "objectID": "notes/Lec02.review.html#multiple-comparisons",
    "href": "notes/Lec02.review.html#multiple-comparisons",
    "title": "Review of Key Concepts",
    "section": "5 Multiple comparisons",
    "text": "5 Multiple comparisons\n\nWhen you perform many hypothesis tests, your chance of making a type 1 error increases\n\nType 1 error\n\nProbability of rejecting the null hypothesis when the null hypothesis is true\nProbability of declaring a “statistically significant difference” when, in truth, there is no difference\n\n\nConsider the follow hypothetical example\n\nAn investigator decides to examine an association between eating red meat and cancer\nThe investigator collects clinical data on a cohort of individuals who eat red meat and a cohort who does not eat red meat\nIn the analysis, the investigator compares incidence rates between the two groups\nAlso makes comparisons stratified by gender, race, and lifestyle factors\nIn summary, the investigator claims “The research study uncovered a significant association between consuming red meat and the incidence of lung cancer in non-smoking males (p \\(&lt;\\) 0.05)” (No significant associations were found in any other subgroup.)\n\nTwo possible conclusions\n\nConclusion 1: There is an association between consuming red meat and cancer in non-smoking men\nConclusion 2: This finding is a type 1 error\nWhich do you suspect is the truth?\n\nProbability of multiple comparisons\n\nFor a null hypothesis \\(H_0\\) that is true, and a test performed at significance level \\(\\alpha\\)\n\n\n\\[\\textrm{Pr}(\\textrm{reject } H_0 | H_0 \\textrm{ is true}) = \\alpha\\]\n\\[\\textrm{Pr}(\\textrm{do not reject } H_0 | H_0 \\textrm{ is true}) = 1 - \\alpha\\]\nNext suppose that \\(n\\) independent hypothesis tests (\\(H1_0, H2_0, \\ldots, Hn_0\\)) are performed at level \\(\\alpha\\) and all \\(n\\) null hypotheses are true\n\\[\\textrm{Pr}(\\textrm{do not reject } H1_0, H2_0, \\ldots, Hn_0 | \\textrm{ all } Hi_0 \\textrm{ are true}) = (1 - \\alpha)^n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nn\n1\n2\n4\n8\n12\n16\n20\n30\n\n\n\n\n\\((1 - .05)^n\\)\n0.95\n0.90\n0.81\n0.66\n0.54\n0.44\n0.36\n0.22\n\n\n\\((1 - .01)^n\\)\n0.99\n0.98\n0.96\n0.92\n0.89\n0.85\n0.82\n0.74\n\n\n\n\n\nIf 30 independent trials are performed at \\(\\alpha=0.05\\) and all null hypotheses are true, the probability of falsely rejecting at least one null hypothesis is \\(78\\%\\)! We are very likely to make a mistake.\nChoosing a smaller \\(\\alpha = 0.01\\) helps, but the probability of at least one type 1 error is still \\(26\\%\\) for 30 tests"
  },
  {
    "objectID": "notes/Lec02.review.html#regression",
    "href": "notes/Lec02.review.html#regression",
    "title": "Review of Key Concepts",
    "section": "6 Regression",
    "text": "6 Regression\n\nThis semester, I will introduce several regression models\nWhich regression model you choose to use is based on the parameter being compared across groups\n\n\\[\n\\begin{split}\n\\textrm{Means} & \\rightarrow  \\textrm{ Linear regression}\\\\\n\\textrm{Odds} & \\rightarrow  \\textrm{ Logistic regression}\\\\\n\\textrm{Rates} & \\rightarrow  \\textrm{ Poisson regression}\\\\\n\\textrm{Hazards} & \\rightarrow  \\textrm{ Cox (proportional hazards) regression}\\\\\n\\end{split}\n\\]\n\nIn Bios 6311, we discussed how to examine association between an outcome and a predictor of interest (POI)\n\nRegression models generalize two sample tests\n\n\n\\[\n\\begin{split}\n\\textrm{2-sample t-test} & \\rightarrow \\textrm{ Linear regression}\\\\\n\\textrm{Pearson } \\chi^2 \\textrm{ test} & \\rightarrow \\textrm{ Logistic regression}\\\\\n\\textrm{Wilcoxon signed-rank test} & \\rightarrow \\textrm{ Propotional odds regression}\\\\\n\\end{split}\n\\]\n\nWe will learn how to adjust for additional variables using a regression model\n\nConfounders\nEffect modifiers\nPrecision variables\n\nWhen building a multivariable regression model, many possible choices along the way\n\nWhich covariates to include?\nWhat model provides the best fit to the data?\nDoes the model satisfy underlying assumptions?\netc.\n\nTheme of this course\n\nWhen building a multivariable model, data driven decisions lead to multiple comparisons problem.\n\nInstead, I will emphasize\n\nPutting science before statistics. Decide the scientific question first, propose a model that answers that scientific question.\nRegression models that are robust to distributional assumptions (relieving the need for extensive model checking)\nDetailed, pre-specified analysis plans. Lay out the process for conducting your analysis before looking at the data to avoid data-driven decision making."
  },
  {
    "objectID": "notes/Lec04.Logistic.html",
    "href": "notes/Lec04.Logistic.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "Types of variables\n\nBinary data: e.g. sex, death\nNominal (unordered categorical) data: e.g. race, martial status\nOrdinal (ordered categorical data): e.g. cancer stage, asthma severity\nQuantitative data: e.g. age, blood pressure\nRight censored data: e.g. time to death\n\nThe measures used to summarize and compare distributions vary according to the type of variable\n\nMeans: Binary, quantitative\nMedians: Ordered, quantitative, censored\nProportions: Binary, nominal, ordinal\nOdds: Binary, nominal, ordinal\nHazards: Censored\n\nWhich regression model you choose to use is based on the parameter being compared across groups\n\n\n\n\n\nParameter\nApproach\n\n\n\n\nMeans\nLinear regression\n\n\nGeometric means\nLinear regression on log scale\n\n\nOdds\nLogistic regression\n\n\nRates\nPoisson regression\n\n\nHazards\nProportional Hazards (Cox) regression\n\n\n\n\n\nGeneral notation for variables and parameters\n\n\n\n\n\n\\(Y_i\\)\nResponse measured on the \\(i\\)th subject\n\n\n\\(X_i\\)\nValue of the predictor measured on the \\(i\\)th subject\n\n\n\\(\\theta_i\\)\nParameter summarizing distribution of \\(Y_i | X_i\\)\n\n\n\n\n\nThe parameter (\\(\\theta_i\\)) might be the mean, geometric mean, odds, rate, instantaneous risk of an event (hazard), etc.\nIn linear regression on means, \\(\\theta_i = E[Y_i | X_i]\\)\nChoice of correct \\(\\theta_i\\) should be based on scientific understanding of problem\nGeneral notation for simple regression model\n\n\\[g(\\theta_i) = \\beta_0 + \\beta_1 \\times X_i\\]\n\nGeneral notation for regression model with one predictor\n\n\n\n\\(g( )\\)\nLink function used for modeling\n\n\n\\(\\beta_0\\)\nIntercept\n\n\n\\(\\beta_1\\)\nSlope for predictor \\(X\\)\n\n\n\n\n\nThe link function is often either the identity function (for modeling means) or log (for modeling geometric means, odds, hazards)\n\nIdentity function: \\(f(x) = x\\)\n\n\n\n\n\nBorrowing information\n\nUse other groups to make estimates in groups with sparse data\nIntuitively, 67 and 69 year olds would provide some relevant information about 68 year olds\nAssuming a straight line relationship tells us about other, even more distant, individuals\nIf we do not want to assume a straight line, we may only want to borrow information from nearby groups\n\nDefining “Contrasts”\n\nDefine a comparison across groups to use when answering scientific questions\nIf the straight line relationship holds, the slope is the difference in parameter between groups differing by 1 unit in \\(X\\)\nIf a non-linear relationship in parameter, the slope is still the average difference in parameter between groups differing by 1 unit in \\(X\\)\nSlope is a (first order or linear) test for trend in the parameter\nStatistical jargon: “a contrast” across groups\n\nThe major difference between different regression models is the interpretation of the parameters\n\nHow do I want to summarize the outcome?\nMean, geometric mean, odds, hazard\n\nHow do I want to compare groups?\n\nDifference, ratio\n\nAnswering these two simple questions provides a starting road-map as to which regression model to choose\nIssues related to the inclusion of covariates remains the same\n\nAddress the scientific question: Predictor of interest, effect modification\nAddress confounding\nIncrease precision"
  },
  {
    "objectID": "notes/Lec04.Logistic.html#general-regression-setting",
    "href": "notes/Lec04.Logistic.html#general-regression-setting",
    "title": "Logistic Regression",
    "section": "",
    "text": "Types of variables\n\nBinary data: e.g. sex, death\nNominal (unordered categorical) data: e.g. race, martial status\nOrdinal (ordered categorical data): e.g. cancer stage, asthma severity\nQuantitative data: e.g. age, blood pressure\nRight censored data: e.g. time to death\n\nThe measures used to summarize and compare distributions vary according to the type of variable\n\nMeans: Binary, quantitative\nMedians: Ordered, quantitative, censored\nProportions: Binary, nominal, ordinal\nOdds: Binary, nominal, ordinal\nHazards: Censored\n\nWhich regression model you choose to use is based on the parameter being compared across groups\n\n\n\n\n\nParameter\nApproach\n\n\n\n\nMeans\nLinear regression\n\n\nGeometric means\nLinear regression on log scale\n\n\nOdds\nLogistic regression\n\n\nRates\nPoisson regression\n\n\nHazards\nProportional Hazards (Cox) regression\n\n\n\n\n\nGeneral notation for variables and parameters\n\n\n\n\n\n\\(Y_i\\)\nResponse measured on the \\(i\\)th subject\n\n\n\\(X_i\\)\nValue of the predictor measured on the \\(i\\)th subject\n\n\n\\(\\theta_i\\)\nParameter summarizing distribution of \\(Y_i | X_i\\)\n\n\n\n\n\nThe parameter (\\(\\theta_i\\)) might be the mean, geometric mean, odds, rate, instantaneous risk of an event (hazard), etc.\nIn linear regression on means, \\(\\theta_i = E[Y_i | X_i]\\)\nChoice of correct \\(\\theta_i\\) should be based on scientific understanding of problem\nGeneral notation for simple regression model\n\n\\[g(\\theta_i) = \\beta_0 + \\beta_1 \\times X_i\\]\n\nGeneral notation for regression model with one predictor\n\n\n\n\\(g( )\\)\nLink function used for modeling\n\n\n\\(\\beta_0\\)\nIntercept\n\n\n\\(\\beta_1\\)\nSlope for predictor \\(X\\)\n\n\n\n\n\nThe link function is often either the identity function (for modeling means) or log (for modeling geometric means, odds, hazards)\n\nIdentity function: \\(f(x) = x\\)\n\n\n\n\n\nBorrowing information\n\nUse other groups to make estimates in groups with sparse data\nIntuitively, 67 and 69 year olds would provide some relevant information about 68 year olds\nAssuming a straight line relationship tells us about other, even more distant, individuals\nIf we do not want to assume a straight line, we may only want to borrow information from nearby groups\n\nDefining “Contrasts”\n\nDefine a comparison across groups to use when answering scientific questions\nIf the straight line relationship holds, the slope is the difference in parameter between groups differing by 1 unit in \\(X\\)\nIf a non-linear relationship in parameter, the slope is still the average difference in parameter between groups differing by 1 unit in \\(X\\)\nSlope is a (first order or linear) test for trend in the parameter\nStatistical jargon: “a contrast” across groups\n\nThe major difference between different regression models is the interpretation of the parameters\n\nHow do I want to summarize the outcome?\nMean, geometric mean, odds, hazard\n\nHow do I want to compare groups?\n\nDifference, ratio\n\nAnswering these two simple questions provides a starting road-map as to which regression model to choose\nIssues related to the inclusion of covariates remains the same\n\nAddress the scientific question: Predictor of interest, effect modification\nAddress confounding\nIncrease precision"
  },
  {
    "objectID": "notes/Lec04.Logistic.html#simple-logistic-regression",
    "href": "notes/Lec04.Logistic.html#simple-logistic-regression",
    "title": "Logistic Regression",
    "section": "2 Simple Logistic Regression",
    "text": "2 Simple Logistic Regression\n\n2.1 Uses of logistic regression\n\nUse logistic regression when you want to make inference about the odds\n\nAllows for continuous (or multiple) grouping variables\nIs OK with binary grouping variables too\nCompares odds of responses across groups using ratios\n\n“Odds ratio”\n\n\nBinary response variable\nWhen using regression with binary response variables, we typically model the (log) odds using logistic regression\n\nConceptually there should be no problem modeling the proportion (which is the mean of the distribution)\nHowever, there are several technical reasons why we do not use linear regression very often with binary responses\nWhy not use linear regression for binary responses?\n\nMany misconceptions about the advantages and disadvantages of analyzing the odds\n\nReasons I consider valid: Scientific basis\n\nUses of odds ratios in case control studies\nPlausibility of linear trends and no effect modifiers\n\nReasons I consider valid: Statistical basis\n\nThere is a mean variance relationship (if not using robust SE) that can be incorporate in the logistic regression model\n\n\n\n\n\n2.2 Reasons to use logistic regression\n\nFirst (scientific) reason: Case-Control Studies\n\nStudying a rare disease, so we do study in reverse\n\ne.g. find subjects with cancer (and suitable controls) and then ascertain exposure of interest\nEstimate distribution of the “effect” across groups defined by “cause”\nProportion (or odds) of smokers among people with or without lung cancer\n\n\n\nCase-Control or Cohort 2x2 Table. In the Case-Control design, the total number of subjects with Cancer \\((a+c)\\) and without cancer \\((b+d)\\) are fixed by design.\n\n\n\nLung Cancer +\nLung Cancer -\n\n\nSmoker\na\nb\n\n\nNon-Smoker\nc\nd\n\n\n\n\nIn contrast, a cohort study samples by exposure (smoking) and then estimates the distribution of the effect in exposure groups\nIn a case-control study, we cannot estimate prevalence (without knowing selection probabilities)\n\ne.g. if doing a 1:1 case-control study, \\((a+c) = b+d\\) so it would look like \\(50\\%\\) of the subjects have cancer\n\nOdds ratios are estimable in either case-control or cohort sampling scheme\n\nCohort study: Odds of cancer among smoker compared to odds of cancer among nonsmokers\nCase-control study: Odds of smoking among cancer compared to odds of smoking among non-cancer\n\nMathematically, these two odds ratios are the same\n\n\n\nOdds ratios are easy to interpret when investigating rare events\n\nOdds = prob / (1 - prob)\nFor rare events, (1 - prob) is approximately 1\n\nOdds is approximately the probability\nOdds ratios are approximately risk ratios\n\nCase-control studies usually used when events are rare\n\n\nSecond (scientific) reason: Linearity\n\nProportions are bounded by 0 and 1\nIt is thus unlikely that a straight line relationship would exists between a proportion and a predictor\n\nUnless the predictor itself is bounded\nOtherwise, there eventually must be a threshold above which the probability does not increase (or only increases a little)\n\n\n\n\n\nCode\nexpit &lt;- function(x) {exp(x)/(1+exp(x))}\nplot(function(x) expit(x), -4,4, ylab=\"Probabilty\", xlab=\"Predictor\")\n\n\n\n\n\nLogistic function will bound probabilities between 0 and 1\n\n\n\n\n\nThird (scientific) reason: Effect modification\n\nThe restriction on ranges for probabilities makes it likely that effect modification must be present with proportions\nExample: Is the association between 2-year relapse rates and having a positive scan modified by gender?\n\nWomen relapse 40% of the time when the scan is negative, and 95% of the time when the scan is positive (an increase of 55%)\n\nIf men relapse 75% of the time when the scan is negative, then a positive scan can increase the relapse rate to at most 100%, which is only a 25% increase\n\n\n\nProportions\n\n\n\n\n\nWomen\nMen\n\n\nNegative Scan\n40%\n75%\n\n\nPositive Scan\n95%\n(up to 100%)\n\n\n\n\n\n\n\nDifference\n55%\nUp to 25%\n\n\nRatio\n1.64\n\\(\\leq 1.33\\)\n\n\n\nWith the odds, the association can hold without effect modification\n\n\n\nOdds\n\n\n\n\n\nWomen\nMen\n\n\nNegative Scan\n0.67\n3\n\n\nPositive Scan\n19\n(up to \\(\\infty\\))\n\n\n\n\n\n\n\nRatio\n28.5\n\\(&lt; \\infty\\)\n\n\n\n\n\n\nIf the o dds of positive scan in men was 85.5, then the odds ratio would be exactly 28.5 (no effect modification)\nFourth (statistics) reason:\n\nClassical linear regression requires equal variances across each predictor group\nBut, with binary data, the variance within a group depends on the mean\nFor binary \\(Y\\), \\(E(Y) = p\\) and \\(Var(Y) = p(1-p)\\)\nWith robust standard errors, the mean-variance relationship is not a major problem. However, a logistic model that correctly models the mean-variance relationship will be more efficient.\n\n\n\n\n2.3 The simple logistic regression model\n\nModeling the odds of binary response variable \\(Y\\) on predictor \\(X\\)\n\nDistribution: \\(\\textrm{Pr}(Y_i = 1) = p_i\\)\nModel: \\(\\textrm{logit}(p_i) = \\textrm{log}\\left(\\frac{p_i}{1-p_i}\\right) = \\beta_0 + \\beta_1 \\times X_i\\)\nWhen \\(X_i = 0\\): log odds = \\(\\beta_0\\)\nWhen \\(X_i = x\\): log odds = \\(\\beta_0 + \\beta_1 \\times x\\)\nWhen \\(X_i = x+1\\): log odds = \\(\\beta_0 + \\beta_1 \\times x + \\beta_1\\)\n\nTo interpret as odds, exponentiate the regression parameters\n\nDistribution: \\(\\textrm{Pr}(Y_i = 1) = p_i\\)\nModel: \\(\\frac{p_i}{1-p_i} = \\exp(\\beta_0 + \\beta_1 \\times X_i) = e^{\\beta_0} \\times e^{\\beta_1 \\times X_i}\\)\nWhen \\(X_i = 0\\): odds = \\(e^{\\beta_0}\\)\nWhen \\(X_i = x\\): odds = \\(e^{\\beta_0} \\times e^{\\beta_1 \\times x}\\)\nWhen \\(X_i = x+1\\): odds = \\(e^{\\beta_0} \\times e^{\\beta_1 \\times x} \\times e^{\\beta_1}\\)\n\nTo interpret as proportions (remember proportion = odds / (1 + odds))\n\nDistribution: \\(\\textrm{Pr}(Y_i = 1) = p_i\\)\nModel: \\(p_i = \\frac{e^{\\beta_0} e^{\\beta_1 \\times X_i}}{1 + e^{\\beta_0} e^{\\beta_1 \\times X_i}}\\)\nWhen \\(X_i = 0\\): \\(p_i = \\frac{e^{\\beta_0}}{1 + e^{\\beta_0}}\\)\n\nWhen \\(X_i = x\\): \\(p_i = \\frac{e^{\\beta_0} e^{\\beta_1 \\times x}}{1 + e^{\\beta_0} e^{\\beta_1 \\times x}}\\)\nWhen \\(X_i = x+1\\): \\(p_i = \\frac{e^{\\beta_0} e^{\\beta_1 \\times x} e^{\\beta_1}}{1 + e^{\\beta_0} e^{\\beta_1 \\times x}e^{\\beta_1}}\\)\n\nMost common interpretations found by exponentiating the coefficients\n\nOdds when predictor is 0 found by exponentiating the intercept: \\(\\exp(\\beta_0)\\)\nOdds ratio between groups differing in the values of the predictor by 1 unit found by exponentiating the slope: \\(\\exp(\\beta_1)\\)\n\nStata commands\nlogit respvar predvar, [robust]\n\nProvides regression parameter estimates an inference on the log odds scale (both coefficients with CIs, SEs, p-values)\n\nlogistic respvar predvar, [robust]\n\nProvides regression parameter estimates and inference on the odds ratio scale (only slope with CIs, SEs, p-values)\n\nR Commands\n\nWith rms package, lrm(respvar ~ predvar, ...)\nIn general, glm(respvar ~ predvar, family=“binomial”)"
  },
  {
    "objectID": "notes/Lec04.Logistic.html#example-survival-on-the-titanic-and-age",
    "href": "notes/Lec04.Logistic.html#example-survival-on-the-titanic-and-age",
    "title": "Logistic Regression",
    "section": "3 Example: Survival on the Titanic and Age",
    "text": "3 Example: Survival on the Titanic and Age\n\nDataset at https://biostat.app.vumc.org/DataSets\nDescribes the survival status of individual passengers on the Titanic\nData on age available for many, but not all, subjects (data continually being updated)\nResponse variables is Survival\n\nBinary variable: 1=Survived, 0=Died\n\nPredictor variable is Age\n\nContinuous grouping variable\n\nPossibly different age effects by sex (effect modification by sex)\n\n\n3.1 Descriptive Plots\n\n\nCode\n# | fig-cap: Missing data patterns in the Tianic dataset\nlibrary(rms)\ntitanic &lt;- read.csv(file=\"data/titanic3.csv\")\nplot(naclus(titanic)) # study patterns of missing values\n\n\n\n\n\n\n\nCode\n# | fig-cap: Scatterplot of age versus survival in the Titanic data with lowess smooth.  This simple plot is not very useful because survival is either 0 or 1, making it hard to visualize any trends.\nggplot(titanic, aes(x=age, y=survived)) + geom_jitter(width=0, height=.02, alpha=.5) + geom_smooth()\n\n\n\n\n\n\n\nCode\n# | fig-cap: Age versus survival by sex in the Titanic data by age using a super smoother.  The trends are clearer with this smoothing approach.\nwith(titanic, \n     plsmo(age, survived, group=sex, datadensity=T, ylab=\"Survived (1=Yes, 0=No)\", xlab=\"Age (years)\")\n)\n\n\n\n\n\n\n\n\n\nComments on the plots\n\nAge is missing for many subjects, which we will not worry about in the following analysis\nThe simple scatterplot, even with superimposes lowess smooth, is worthless. I have jittered the point and altered their opacity to help visualize overlapping point.\nMore advanced plotting available in R (in this case, the plsmo() function) can help to visualize the data\n\n\n\n\n3.2 Regression Model\n\nRegression model for survival on age (ignoring possible effect modification for now)\nAnswer question by assessing linear trends in log odds of survival by age\nEstimate the best fitting line to log odds of survival within age groups\n\n\\[\\textrm{logodds}(\\textrm{Survival} | \\textrm{Age}) = \\beta_0 + \\beta_1 \\times \\textrm{Age}\\]\n\nAn association will exist if the slope \\(\\beta_1\\) is nonzero\nIn that case, the odds (and probability) of survival will be different across different age groups\n\n\n\nCode\nm.titanic &lt;- glm(survived ~ age, data=titanic, family = \"binomial\")\nsummary(m.titanic)\n\n\n\nCall:\nglm(formula = survived ~ age, family = \"binomial\", data = titanic)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept) -0.136534   0.144715  -0.943   0.3454  \nage         -0.007899   0.004407  -1.792   0.0731 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1414.6  on 1045  degrees of freedom\nResidual deviance: 1411.4  on 1044  degrees of freedom\n  (263 observations deleted due to missingness)\nAIC: 1415.4\n\nNumber of Fisher Scoring iterations: 4\n\n\n\\(\\textrm{logodds}(\\textrm{Survival} | \\textrm{Age}) = -0.1365 - 0.007899 \\times \\textrm{Age}\\)\n\nGeneral interpretation\n\nIntercept is labeled “(Intercept)”\nSlope for age is labeled “age”\n\nInterpretation of intercept\n*Estimated log odds for newborns (age=0) is \\(-0.136534\\)\n\nOdds of survival for newborns is \\(e^{-0.136534} = 0.8724\\)\nProbability of survival\n\nProb = odds / (1 + odds)\n\\(0.8724 / (1 + .8724) = 0.4659\\)\n\n\n\n\n\nCode\npredict(m.titanic, newdata=data.frame(age=0), type='response')\n\n\n        1 \n0.4659194 \n\n\n\nInterpretation of slope\n\nEstimate difference in the log odds of survival for two groups differing by one year in age is \\(-0.0078985\\)\nThis estimate averages over males and females\nOlder groups tend to have lower log odds\nOdds Ratio: \\(e^{-0.0078985} = 0.9921\\)\nFor five year difference in age: \\(e^{-0.0078985 \\times 5} = 0.9612\\)\nIn Stata use “lincom age, or” or “lincom 5*age, or”\n\n\nNote that if the straight line relationship does not hold true, we interpret the slope as an average difference in the log odds of survival per one year difference in age\nThere are several ways to get the odds ratio and confidence interval in R\n\n\nCode\n# The coefficient and confidence interval (on the log-odds scale)\ncoef(m.titanic)[\"age\"]\n\n\n         age \n-0.007898504 \n\n\nCode\nconfint.default(m.titanic, \"age\")\n\n\n          2.5 %       97.5 %\nage -0.01653509 0.0007380868\n\n\nCode\n# Odds ratio for age and confidence interval for age (1 year increase)\nexp(coef(m.titanic)[\"age\"])\n\n\n      age \n0.9921326 \n\n\nCode\nexp(confint.default(m.titanic, \"age\"))\n\n\n        2.5 %   97.5 %\nage 0.9836009 1.000738\n\n\nCode\n# Odds ratio for age and confidence interval for age (5 year increase)\nexp(5*coef(m.titanic)[\"age\"])\n\n\n      age \n0.9612771 \n\n\nCode\nexp(5*confint.default(m.titanic, \"age\"))\n\n\n        2.5 %   97.5 %\nage 0.9206499 1.003697\n\n\n\nUsing finalfit to create a nicer output table of the coefficients and confidence intervals\n\nFor finalfit to use a logistic regression model by default, survived must be defined as a factor variable with two levels\n\n\n\n\nCode\nmykable = function(x){\n  knitr::kable(x, row.names = FALSE, align = c(\"l\", \"l\", \"r\", \"r\", \"r\", \"r\", \"r\", \"r\", \"r\"),\n               booktabs=TRUE)\n}\n\n\nlibrary(finalfit)\nlibrary(dplyr)\nlibrary(rms)\nexplanatory = c(\"age\")\n\ntitanic$survived.factor &lt;- factor(titanic$survived, levels=0:1, labels=c(\"Died\",\"Survived\"))\ndependent = 'survived.factor'\n\nlabel(titanic$age) &lt;- \"Age (years)\"\ntitanic %&gt;% \n  finalfit(dependent, explanatory) %&gt;% mykable()\n\n\n\n\n\n\n\n\n\n\n\n\n\nDependent: survived.factor\n\nDied\nSurvived\nOR (univariable)\nOR (multivariable)\n\n\n\n\nAge (years)\nMean (SD)\n30.5 (13.9)\n28.9 (15.1)\n0.99 (0.98-1.00, p=0.073)\n0.99 (0.98-1.00, p=0.073)\n\n\n\n\n\n\n\n3.3 Comments on Interpretation\n\nThe slope for age is expressed as a difference in group means, not the difference due to aging. We did not do a longitudinal study in which repeated measurements were taken on the same subject.\nIf the group log odds are truly linear, then the slope has an exact interpretation as the change in survival due to a one year change in (any) age\nOtherwise, the slope estimates the first order trend of the sample data and we should not treat the estimates of group odds or probabilities as accurate\nIt is difficult to see in the above example, but the CIs around the odds ratios are not symmetric\n\n(Symmetric) CIs are calculated on the log odds scale, and then transformed to the odds scale by expoenentiating the lower and upper limits of the CI\n\n“From logistic regression analysis, we estimate that for each 5 year difference in age, the odds of survival on the Titanic decreased by 3.9%, though this estimate is not statistically significant (\\(p = 0.07\\)). A 95% CI suggests that this observation is not unusual if a group that is five years older might have an odds of survival that was anywhere between 7.9% lower and 0.4% higher than the younger group.”\nThe confidence interval and statistical test given in the output is called a Wald test. Other tests (Score, Likelihood Ratio) are also possible.\n\nAll tests are asymptotically equivalent\nThe Wald test is easiest to obtain, but generally performs the poorest in small sample sizes\nThe Likelihood Ratio test performs the best in small samples. We will discuss it later, including how to obtain the test using post-estimation commands.\nThe Score test is not bad in small samples, but is often hard to obtain from software. It is exactly equal to the Chi-squared test for binary outcomes and categorical predictors.\n\n\n\n3.3.1 Bayesian Estimates and Interpretation\n\nBayesian approach to the logistic model requires specifying\n\nThe model\ne.g. a model for the log odds of survival that is linear in the parameters with an intercept and slope for age\nPrior distributions on parameters\n\nFor the simple logistic regression model, we have parameters \\(\\beta_0\\), and \\(\\beta_1\\).\nFor now, we will use default prior distributions that are are intended to be weakly informative in that they provide moderate regularization and help stabilize computation. See the STAN documentation for more details\nAppropriate priors can be based on scientific considerations\nSensitivity analyses can evaluate the the robustness of finding to different prior assumptions\n\n\nThe likelihood\n\nFor a binomial GLM the likelihood for one observation \\(y\\) can be written as a conditionally binomial probability mass function\n\n\n\\[\\binom{n}{y} \\pi^{y} (1 - \\pi)^{n - y},\\]\n\n\\(n\\) is the known number of trials, \\(\\pi = g^{-1}(\\eta)\\) is the probability of success and \\(\\eta = \\alpha + \\mathbf{x}^\\top \\boldsymbol{\\beta}\\) is a linear predictor\nFor a sample of size \\(N\\), the likelihood of the entire sample is the product of \\(N\\) individual likelihood contributions.\nBecause \\(\\pi\\) is a probability, for a binomial model the link function \\(g\\) maps between the unit interval (the support of \\(\\pi\\)) and the set of all real numbers \\(\\mathbb{R}\\). When applied to a linear predictor \\(\\eta\\) with values in \\(\\mathbb{R}\\), the inverse link function \\(g^{-1}(\\eta)\\) therefore returns a valid probability between 0 and 1.\nThe two most common link functions used for binomial GLMs are the\n\nlogit and\nprobit\n\nWith the logit (or log-odds) link function \\(g(x) = \\ln{\\left(\\frac{x}{1-x}\\right)}\\), the likelihood for a single observation becomes\n\n\\[\\binom{n}{y}\\left(\\text{logit}^{-1}(\\eta)\\right)^y\n\\left(1 - \\text{logit}^{-1}(\\eta)\\right)^{n-y} =\n\\binom{n}{y} \\left(\\frac{e^{\\eta}}{1 + e^{\\eta}}\\right)^{y}\n\\left(\\frac{1}{1 + e^{\\eta}}\\right)^{n - y}\\]\n\nWith the probit link function \\(g(x) = \\Phi^{-1}(x)\\) yields the likelihood\n\n\\[\\binom{n}{y} \\left(\\Phi(\\eta)\\right)^{y}\n\\left(1 - \\Phi(\\eta)\\right)^{n - y},\\]\nwhere \\(\\Phi\\) is the CDF of the standard normal distribution.\n\nOutput from Bayesian logistic regression using logit link function\n\n\n\nCode\nlibrary(rstanarm)\nlibrary(bayesplot)\n\nfit2 &lt;- stan_glm(survived ~ age,\n                 data=titanic, family=binomial(),\n                 seed=1234,\n                 refresh=0)\nsummary(fit2, digits=4, prob=c(.025, .5, .975))\n\n\n\nModel Info:\n function:     stan_glm\n family:       binomial [logit]\n formula:      survived ~ age\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 1046\n predictors:   2\n\nEstimates:\n              mean    sd      2.5%    50%     97.5%\n(Intercept) -0.1343  0.1459 -0.4135 -0.1336  0.1560\nage         -0.0079  0.0044 -0.0169 -0.0078  0.0004\n\nFit Diagnostics:\n           mean   sd     2.5%   50%    97.5%\nmean_PPD 0.4089 0.0217 0.3652 0.4092 0.4512 \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse   Rhat   n_eff\n(Intercept)   0.0029 0.9994 2533 \nage           0.0001 0.9996 2570 \nmean_PPD      0.0004 1.0004 3073 \nlog-posterior 0.0284 1.0047 1340 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\n\n\nAnd a summary of the prior distributions used\n\n\n\nCode\nprior_summary(fit2, digits = 2)\n\n\nPriors for model 'fit2' \n------\nIntercept (after predictors centered)\n ~ normal(location = 0, scale = 2.5)\n\nCoefficients\n  Specified prior:\n    ~ normal(location = 0, scale = 2.5)\n  Adjusted prior:\n    ~ normal(location = 0, scale = 0.17)\n------\nSee help('prior_summary.stanreg') for more details\n\n\n\nInterpretation\n\nSlope for age is of primary scientific importance\nA priori we assume that no association between age and cholesterol. Specifically, we assumed a Normal prior with location (mean) of 0 and scale (standard devation) of 0.17 for \\(\\beta_1\\).\nConditional on the data, we estimate that for every 1 year increase in age, the log odds of decreases by -0.0079 (95% credible interval -0.0169 to 0.0004).\nTo obtain the posterior odds ratio and 95% credible intervals, some additional commands are needed\n\n\n\n\nCode\n# 1 year change in age\nexp(coef(fit2)[\"age\"]) # Posterior Odds Ratio\n\n\n      age \n0.9922461 \n\n\nCode\nexp(posterior_interval(fit2, prob = 0.95)) # 95% credible interval\n\n\n                 2.5%    97.5%\n(Intercept) 0.6613115 1.168821\nage         0.9832330 1.000353\n\n\nCode\n# 5 year change in age\nexp(5*coef(fit2)[\"age\"]) # Posterior Odds Ratio\n\n\n      age \n0.9618272 \n\n\nCode\nexp(5*posterior_interval(fit2, prob = 0.95))[2,] # 95% credible interval\n\n\n     2.5%     97.5% \n0.9189294 1.0017684"
  },
  {
    "objectID": "notes/Lec04.Logistic.html#inference-with-logistic-regression",
    "href": "notes/Lec04.Logistic.html#inference-with-logistic-regression",
    "title": "Logistic Regression",
    "section": "4 Inference with Logistic Regression",
    "text": "4 Inference with Logistic Regression\n\nThe ideas of Signal and Noise found in simple linear regression do not translate well to logistic regression\nWe do not tend to quantify an error distribution with logistic regression\nValid statistical inference (CIs, p-values) about associations requires three general assumptions\nAssumption 1: Approximately Normal distributions for the parameter estimates\n\nLarge N\nNeed for either robust standard errors or classical logistic regression\nDefinition of large depends on the underlying probabilities (odds)\nRecall the rule of thumb for chi-squared tests based on the expected number of events\n\nAssumption 2: Assumptions about the independence of observations\n\nClassical regression: Independence of all observation\nRobust standard errors: Correlated observations within identified clusters\nAssumption 3: Assumptions about variance of observations within groups\n\nClassical regression: Mean-variance relationship for binary data\n\nClassical logistic regression estimates SE using model based estimates\nHence in order to satisfy this requirement, linearity of log odds across groups must hold\n\nRobust standard errors\n\nAllows unequal variance across groups\nHence, do not need linearity of log odds across groups to hold\nValid statistical inference (CIs, p-values) about odds of response in specific groups requires a further assumption\n\nAssumption 4: Adequacy of the linear model\n\nIf we are trying to borrow information about the log odds from neighboring groups, and we are assuming a straight line relationship, the straight line needs to be true\nNeeded for either classical or robust standard errors\nNote that we can model transformations of the measured predictor if we feel a straight line is not appropriate\nInference about individual observations (prediction intervals, P-values) in specific groups requires no further assumptions because we have binary data\n\nFor binary data, if we know the mean (proportion), we know everything about the distribution including the variance\nThis differs from linear regression where we can have a correct model for the mean, but the assumption about the error distribution (Normality, homoskedasticity) can be incorrect\n\n\n\n\n4.1 Interpreting “Positive” Results\n\nSlope is statistically different from 0 using robust standard errors\nObserved data is atypical of a setting with no linear trend in odds of response across groups\nData suggests evidence of a trend toward larger (or smaller) odds in groups having larger values of the predictor\n(To the extent the data appears linear, estimates of the group odds or probabilities will be reliable)\n\n\n\n4.2 Interpreting “Negative” Results\n\nMany possible reasons why the slope is not statistically different from 0 using robust standard errors\n\nThere may be no association between the response and predictor\nThere may be an association in the parameter considered, but the best fitting line has zero slope\nThere may be a first order trend in the log odds, but we lacked the precision to be confident that it truly exists (a type II error)"
  },
  {
    "objectID": "notes/Lec04.Logistic.html#example-analysis-revisited-effect-modification",
    "href": "notes/Lec04.Logistic.html#example-analysis-revisited-effect-modification",
    "title": "Logistic Regression",
    "section": "5 Example analysis revisited: Effect Modification",
    "text": "5 Example analysis revisited: Effect Modification\n\nRecall in our Titanic example that the effect of age appeared to differ by sex\n\nWe ignored this difference earlier, so our estimated age effect was a (weighted) average of the age effect in males and the age effect in female\nHere is the plot again describing the trends we see in survival by age and sex (using plsmo).\n\n\n\n\nCode\n# | fig-cap: Age versus survival by gender in the Titanic data by age using a super smoother.\nwith(titanic, \n     plsmo(age, survived, group=sex, datadensity=T, ylab=\"Survived (1=Yes, 0=No)\", xlab=\"Age (years)\")\n)\n\n\n\n\n\n\nWe could describe the observed differences in two way, both being correct\nGender modifies the age effect\n\nIn males, the probability of survival worsened with age while in female the probabilty of survival improved with age\nEmphasizes that the female age slope is positive while the male age slope is negative\n\nAge modifies the gender effect\n\nThe survival rates of male and females were more similar at younger ages than older ages\nCould specify the odds ratio of survival comparing females to males at specific ages\n\n\n\n5.1 Stratified analysis by sex\n\nThe log odds of survival in females\n\n\n\nCode\nfit.titanic.female &lt;- glm(survived ~ age, data=titanic, subset=sex==\"female\")\nfit.titanic.female\n\n\n\nCall:  glm(formula = survived ~ age, data = titanic, subset = sex == \n    \"female\")\n\nCoefficients:\n(Intercept)          age  \n   0.637644     0.004006  \n\nDegrees of Freedom: 387 Total (i.e. Null);  386 Residual\n  (78 observations deleted due to missingness)\nNull Deviance:      72.25 \nResidual Deviance: 70.93    AIC: 447.7\n\n\n\nThe log odds of survival in males\n\n\n\nCode\nfit.titanic.male &lt;- glm(survived ~ age, data=titanic, subset=sex==\"male\")\nfit.titanic.male\n\n\n\nCall:  glm(formula = survived ~ age, data = titanic, subset = sex == \n    \"male\")\n\nCoefficients:\n(Intercept)          age  \n   0.316337    -0.003635  \n\nDegrees of Freedom: 657 Total (i.e. Null);  656 Residual\n  (185 observations deleted due to missingness)\nNull Deviance:      107.3 \nResidual Deviance: 105.5    AIC: 669.1\n\n\n\n5.1.1 Odds ratios and confidence intervals for age effect by sex\n\nConsider a 5 year change in age\n\n\n\nCode\n# Females\nexp(5*coef(fit.titanic.female)[\"age\"])\n\n\n     age \n1.020234 \n\n\nCode\nexp(5*confint.default(fit.titanic.female,\"age\"))\n\n\n       2.5 %  97.5 %\nage 1.005398 1.03529\n\n\nCode\n# Males\nexp(5*coef(fit.titanic.male)[\"age\"])\n\n\n      age \n0.9819903 \n\n\nCode\nexp(5*confint.default(fit.titanic.male,\"age\"))\n\n\n        2.5 %    97.5 %\nage 0.9715019 0.9925919\n\n\n\n\n\n5.2 Effect modification using interaction terms\n\nInstead of fitting two seaparte models for male and females, we could estimate all parameters in a single regression model\n\nLet \\(p_i\\) be the probability of survival for passenger \\(i\\) and \\(\\textrm{logit}(p)= \\textrm{log}\\left(\\frac{p}{1-p}\\right)\\)\nLet \\(X_{1i}\\) be the age of subject \\(i\\)\nLet \\(X_{2i}\\) be an indicator variable for female sex. \\(X_{2i}=1\\) if a subject is female and \\(X_{2i} = 0\\) if a subject is male\n\n\n\\[\\textrm{logit}(p | X_{1i},X_{2i}) = \\beta_0 + \\beta_1 * X_{1i} + \\beta_{2i} * X_{2i} + \\beta_3*X_{1i}*X_{2i}\\]\n\nIn males, \\(X_{2i} = 0\\), this model reduces to\n\n\\[\\textrm{logit}(p | X_{1i}, X_{2i}=0) = \\beta_0 + \\beta_1 * X_{1i}\\]\n\nIn females, \\(X_{2i} = 1\\), this model can be expressed as\n\n\\[\\textrm{logit}(p | X_{1i}, X_{2i}=1) = (\\beta_0+\\beta_2) + (\\beta_1+\\beta_3) * X_{1i}\\]\n\n\\(\\hat{\\beta_1}\\) is the estimate age effect in males\n\\(\\hat{\\beta_1} + \\hat{\\beta_3}\\) is the estimated age effect in females\n\\(\\hat{\\beta_3}\\) is the estimated difference between the age effect in male and the age effect in females\n\n\n\nCode\n# female has already been defined in the dataset, but if I wanted to create this variable I could do so\ntitanic$female &lt;- (titanic$sex==\"female\")+0\n\nfit.titanic.interact &lt;- glm(survived ~ age + female + age*female, data=titanic)\nsummary(fit.titanic.interact)\n\n\n\nCall:\nglm(formula = survived ~ age + female + age * female, data = titanic)\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.316337   0.037944   8.337 2.39e-16 ***\nage         -0.003635   0.001124  -3.233  0.00126 ** \nfemale       0.321307   0.059757   5.377 9.35e-08 ***\nage:female   0.007641   0.001823   4.192 3.01e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.1693472)\n\n    Null deviance: 252.69  on 1045  degrees of freedom\nResidual deviance: 176.46  on 1042  degrees of freedom\n  (263 observations deleted due to missingness)\nAIC: 1116.9\n\nNumber of Fisher Scoring iterations: 2\n\n\n\nWe can see that the parameter estimates from the interaction model are the same as the estimates from the two stratified models\n\n\n\nCode\n# Interaction model\ncoef(fit.titanic.interact)\n\n\n (Intercept)          age       female   age:female \n 0.316337448 -0.003634770  0.321306890  0.007641206 \n\n\nCode\n# Model fit just on male subjects\ncoef(fit.titanic.male)\n\n\n(Intercept)         age \n 0.31633745 -0.00363477 \n\n\nCode\n# Model fit just on female subjects\ncoef(fit.titanic.female)\n\n\n(Intercept)         age \n0.637644338 0.004006436 \n\n\nCode\n# Linear combinations from the interaction model give the female intercept and age slope\ncoef(fit.titanic.interact)[1] + coef(fit.titanic.interact)[3]\n\n\n(Intercept) \n  0.6376443 \n\n\nCode\ncoef(fit.titanic.interact)[2] + coef(fit.titanic.interact)[4]\n\n\n        age \n0.004006436"
  },
  {
    "objectID": "notes/Lec06.ConfoundingEffectModificationMediation.html",
    "href": "notes/Lec06.ConfoundingEffectModificationMediation.html",
    "title": "Confouding, Effect Modification, and Mediation",
    "section": "",
    "text": "Most often scientific questions are translated into comparing the distribution of some response variable across groups of interest\nGroups are defined by the predictor of interest (POI)\n\nCategorical predictors of interest: Treatment or control, knockout or\nwild type, ethnic group\nContinuous predictors of interest: Age, BMI, cholesterol, blood pressure\n\nOften we need to consider additional variables other than POI because …\n\nWe want to make comparisons in different strata\n\ne.g if we stratify by gender, we may get different answers to our scientific question in men and women\n\n\nGroups being compared differ in other ways\nConfounding: A variable that is related to both the outcome and predictor of interest\nLess variability in the response if we control for other variables\nPrecision: If we restrict to looking within certain strata, may get smaller $\\sigma^2$\nCovariates other than the Predictor of Interest are included in the model as\\…\n\nEffect modifiers\nConfounders\nPrecision variables\nNot necessarily mutually exclusive"
  },
  {
    "objectID": "notes/Lec06.ConfoundingEffectModificationMediation.html#overview",
    "href": "notes/Lec06.ConfoundingEffectModificationMediation.html#overview",
    "title": "Confouding, Effect Modification, and Mediation",
    "section": "",
    "text": "Most often scientific questions are translated into comparing the distribution of some response variable across groups of interest\nGroups are defined by the predictor of interest (POI)\n\nCategorical predictors of interest: Treatment or control, knockout or\nwild type, ethnic group\nContinuous predictors of interest: Age, BMI, cholesterol, blood pressure\n\nOften we need to consider additional variables other than POI because …\n\nWe want to make comparisons in different strata\n\ne.g if we stratify by gender, we may get different answers to our scientific question in men and women\n\n\nGroups being compared differ in other ways\nConfounding: A variable that is related to both the outcome and predictor of interest\nLess variability in the response if we control for other variables\nPrecision: If we restrict to looking within certain strata, may get smaller $\\sigma^2$\nCovariates other than the Predictor of Interest are included in the model as\\…\n\nEffect modifiers\nConfounders\nPrecision variables\nNot necessarily mutually exclusive"
  },
  {
    "objectID": "notes/Lec06.ConfoundingEffectModificationMediation.html#effect-modification",
    "href": "notes/Lec06.ConfoundingEffectModificationMediation.html#effect-modification",
    "title": "Confouding, Effect Modification, and Mediation",
    "section": "2 Effect Modification",
    "text": "2 Effect Modification\n\nThe association between the Response and the Predictor of Interest differs in strata defined by the effect modifier\nStatistical term: “Interaction” between the effect modifier and the POI\n\n\n2.1 Effect modification depends on the measure of effect that you choose\n\nChoice of summary measure: mean, median, geometric mean, odds, hazard\nChoice of comparisons across groups: differences, ratios\n\n\n\n2.2 Examples of Effect Modification\n\n2.2.1 Example 1: Is serum LDL by gender modified by smoking?\n\n\n\n\nMean\nMean\nMedian\nMedian\n\n\n\nWomen\nMen\nWomen\nMen\n\n\nNo Smoke\n120\n122\n120\n115\n\n\nSmoke\n133\n122\n133\n124\n\n\nDiff\n-13\n0\n-13\n-9\n\n\nRatio\n0.90\n1\n0.90\n0.93\n\n\n\n\nEffect modification for mean, not really for median\n\nHolds for both difference or ratio\n\n\n\n\n2.2.2 Example 2: Creatinine by stroke (modified by gender?)\n\n\n\n\nMean\nMean\nMedian\nMedian\n\n\n\nWomen\nMen\nWomen\nMen\n\n\nNo Stroke\n0.72\n1.08\n0.7\n1.1\n\n\nStroke\n1.01\n1.51\n1.0\n1.5\n\n\nDiff\n-0.29\n-0.43\n-0.3\n-0.4\n\n\nRatio\n0.71\n0.72\n0.70\n0.73\n\n\n\n\nEffect modification for difference, not really for ratio\n\nTrue for Mean or median\n\n\n\n\n2.2.3 Example 3: Stroke by smoking (modified by gender?)\n\n\n\n\nProportion\nProportion\nOdds\nOdds\n\n\n\nWomen\nMen\nWomen\nMen\n\n\nNo Smoke\n0.10\n0.16\n0.03\n0.19\n\n\nSmoke\n0.16\n0.26\n0.19\n0.35\n\n\nDiff\n-0.06\n-0.10\n-0.16\n-0.16\n\n\nRatio\n0.62\n0.62\n0.16\n0.54\n\n\n\n\nProportion: No effect modification for ratio, small amount for difference\nOdds: No effect modification for difference, yes for ratio\n\n\n\n2.2.4 Example 4: Stroke by smoking (modified by CVD?)\n\n\n\n\nProportion\nProportion\nOdds\nOdds\n\n\n\nNone\nCVD\nNone\nCVD\n\n\nNo Smoke\n0.02\n0.33\n0.02\n0.50\n\n\nSmoke\n0.04\n0.50\n0.04\n1.00\n\n\nDiff\n-0.02\n-0.17\n-0.02\n-0.50\n\n\nRatio\n0.50\n0.67\n0.50\n0.50\n\n\n\n\nEffect Modficiation?\n\nProportion: Yes for ratio, yes for difference\nOdds: Yes for difference, no for ratio\n\n\n\n\n2.2.5 Example 5: CHD by current smoking (modified by gender?)\n\n\n\n\nProportion\nProportion\nOdds\nOdds\n\n\n\nWomen\nMen\nWomen\nMen\n\n\nNo Smoke\n0.18\n0.26\n0.22\n0.36\n\n\nSmoke\n0.05\n0.24\n0.05\n0.32\n\n\nDiff\n0.13\n0.02\n0.17\n0.03\n\n\nRatio\n3.60\n1.08\n4.17\n1.11\n\n\n\n\nEffect Modfication?\n\nProportion: Yes for ratio, yes for difference\nOdds: Yes for difference, yes for ratio\n\n\n\n\n2.2.6 Example 6: CHD by ever smoke (modified by gender?)\n\n\n\n\nProportion\nProportion\nOdds\nOdds\n\n\n\nWomen\nMen\nWomen\nMen\n\n\nNever Smoke\n0.16\n0.25\n0.19\n0.33\n\n\nEver Smoke\n0.16\n0.26\n0.19\n0.35\n\n\nDiff\n0.00\n-0.01\n0.00\n-0.02\n\n\nRatio\n1.00\n0.96\n1.00\n0.95\n\n\n\n\nEffect Modfication?\n\nProportion: No for ratio, no for difference\nOdds: No for difference, no for ratio\n\n\n\n\n2.2.7 Summary comments on examples\n\nIf there is an effect, will see effect modification on at least one of the difference and ratio scale\nIf there is no effect (example 6), will see no effect modification on both difference and ratio scale\nIn real world, will usually see effect modification on both scales. The real question is the effect modification scientifically meaningful.\n\nIf we find there is important effect modification, science will go forward estimating effects separately\nModels with interaction terms are useful for testing if effect modification is present (statistically)\n\n\n\n\nAside: Be careful when comparing two ratios\n\nHow close are two ratios?\n\n0.20 and 0.25 VERSUS 5.0 and 4.0?\n0.10 and 0.15 VERSUS 10.0 and 6.7?\n\nCompare the ratio of ratios, not the difference\nWe might consider ratios to be more different when both ratios are \\(&gt;1\\) than when both are \\(&lt;1\\). But, that would be wrong.\n\n\n\n\n\n2.3 Analysis of Effect Modification\n\nWhen the scientific question involves effect modification\n\nConduct analysis within each stratum separately\nIf we want to estimate the degree of effect modification or test its existence, use a regression model including\n\nPredictor of interest (main effect)\nEffect modifying variable (main effect)\nA covariate modeling the interaction (usually a product)\n\n\n2.3.1 Impact of ignoring effect modification\n\nBy design or mistake, we sometimes do not model effect modification\nMight perform\n\nUnadjusted analysis: POI only\nAdjusted analysis: POI and third variable, but no interaction term\n\nIf effect modification exists, an unadjusted analysis will give different results according to the association between the POI and effect modifier in the sample\n\nIf the POI and the effect modifier are not associated\n\nUnadjusted analysis tends toward an (approximate) weighted average of the stratum specific effects\n\nWith means, exactly a weighted average\nWith odds and hazards, an approximate weighted average (because they are non-linear functions of the mean)\n\n\nIf the POI and the effect modifier are associated in the sample\n\nThe “average” effect is confounded and thus unreliable (variables can be both effect modifiers and confounders)\n\nIf effect modification exists, an analysis adjusting only for the third variable (but no interaction) will tend toward a weighted average of the stratum specific effects\n\nHence, an association in one stratum and not the other will make an adjusted analysis look like an association (provide the sample size is large enough)"
  },
  {
    "objectID": "notes/Lec06.ConfoundingEffectModificationMediation.html#confounding",
    "href": "notes/Lec06.ConfoundingEffectModificationMediation.html#confounding",
    "title": "Confouding, Effect Modification, and Mediation",
    "section": "3 Confounding",
    "text": "3 Confounding\n\n3.1 Simpson’s Paradox\n\nConfounding has its roots in Simpson’s Paradox\nGiven binary variables \\(Y\\) (response), \\(X\\) (POI), and \\(Z\\) (strata) it is possible to have …\n\n\\[\\textrm{Pr}(Y=1 | X=1, Z=1) &gt; \\textrm{Pr}(Y=1 | X=0, Z=1)\\] \\[\\textrm{Pr}(Y=1 | X=1, Z=0) &gt; \\textrm{Pr}(Y=1 | X=0, Z=0)\\]\n… but to also have …\n\\[\\textrm{Pr}(Y=1 | X=1) &lt; \\textrm{Pr}(Y=1 | X=0)\\]\n\n3.1.1 Example: Probability of death (Y) at two hospitals (X) stratified by poor patient condition (Z)\n\nQuestion: Which hospital do you want to be treated at?\n\nConsider the results overall (averaging over Z) and conditional on Z below\n\n\n\n\n\nOverall\nDied\nSurvived\nDeath Rate\n\n\n\n\nHospital A\n16\n784\n2.0%\n\n\nHospital B\n63\n2037\n3.0%\n\n\n\n\n\n\nGood Condition\nDied\nSurvived\nDeath Rate\n\n\n\n\nHospital A\n8\n592\n1.3%\n\n\nHospital B\n6\n594\n1.0%\n\n\n\n\n\n\nPoor Condition\nDied\nSurvived\nDeath Rate\n\n\n\n\nHospital A\n8\n192\n4.0%\n\n\nHospital B\n57\n1443\n3.8%\n\n\n\n\nIgnoring condition, Hospital B has a higher death rather. However, within both poor and good condition, Hospital B has a lower death rate.\n\nPoor condition is a confounder. Hospital B has more subjects with poor condition and subjects with poor condition have a higher death rate.\n\n\n\n\n\n3.2 Definition of Confounding\n\nThe association between a predictor of interest and the response is confounded by a third variable if\n\nThe third variable is associated with the predictor of interest in the sample, AND\nThe third variable is associated with the response\n\nCausally (in truth)\nIn groups that are homogeneous with respect to the predictor of interest\nNot in the causal pathway of interest\n\n\nWe must consider our belief about the causal relationships among the measured variables\n\nThere is no statistical test for causality\nInference about causation comes only from the study design\nBUT, consideration of the causal relationships helps us to decide which statistical questions to answer\n\nClassic confounder\n\nA clear case of confounding occurs when some third variable is a “cause” of both the POI and response\n\n\nWe generally adjust for such a confounder\n\n3.2.1 Directed Acyclyic Graph\n\n\n\n\nflowchart LR\n  X[Predictor\\nof interest] -- Causal? --&gt; Y[Outcome]\n  X[Predictor\\nof interest] &lt;-- Association --&gt; Z[Confounder]\n  Z[Confounder] -- Causal --&gt; Y[Outcome]\n\n\n\n\n\n\nExample: Ice cream (POI), murder rate (outcome), and temperature (confounder) in New York City during the summer\n\n\n\n\n\nflowchart LR\n  X[Ice Cream] -- Causal? --&gt; Y[Murder Rate]\n  X[Ice Cream] &lt;-- Association --&gt; Z[Air temperature]\n  Z[Air temperature] -- Causal --&gt; Y[Murder Rate]\n\n\n\n\n\n\n\n3.2.2 Causal pathways\n\nA variable in the causal pathway of interest\n\nNot a confounder, so we would not adjust for such a variable\nIf we did adjust, we would lose ability to detect associations between the POI and the outcome\nExample: Second hand smoke (POI), stunted growth (confounder), FEV1 (outcome)\n\nScientific question is about the impact of smoking on lung function\n\nStunted growth addresses lung anatomy, not lung function, which we don’t care about it\n\n\n\n\n\n\n\nflowchart LR\n  X[Second hand smoke] --&gt; Y[FEV1]\n  X[Second hand smoke] --&gt; Z[Stuntend growth]\n  Z[Stunted growth] --&gt; Y[FEV1]\n\n\n\n\n\n\n\nA variable in the causal pathway not of interest\n\nHowever, we want to adjust for a variable in a causal pathway that is not of interest\n\nExample: Work stress causing ulcers by hormonal effects versus alcoholism\nDirected Acyclyic Graph\n\nWe can adjust for alcoholism to estimate the path through horomonal effects\nAlternatively, we can adjust for hormonal effects to estimate the effect through alcoholism\n\n\n\n\n\n\n\nflowchart LR\n  X[Work stress] --&gt; W[Hormonal Effects]\n  X[Work Stress] --&gt; Z[Alcholism]\n  W[Hormonal Effects] --&gt; Y[Ulcers]\n  Z[Alcholism] --&gt; Y[Ulcers]\n\n\n\n\n\n\n\nSurrogate for response\n\nAdjustment for a surrogate is a bad idea\nAs the name implies, surrogates are a substitute for the response variable\nDirected Acyclyic Graph where forced vital capacity (FVC) is a surrogate for forced exp\n\n\n\n\n\nflowchart LR\n  X[Second hand smoke] --&gt; Z[FVC]\n  Z[FVC] --&gt; Y[FEV1]\n\n\n\n\n\n\n\nMany other (complicated) patterns possible\n\nGreenland, Pearl, and Robins. Causal Diagrams for Epidemiologic Research. Epidemiology. (1999) http://www.jstor.org/stable/3702180\nhttps://www.dagitty.net/\n\n\n\n\n\n3.3 Diagnosing Confounding\n\nConfounding typically produces a difference between unadjusted and adjusted analyses\nThis symptom is not proof of confounding\nSuch a difference can occur when there is no confounding\nSymptom is more indicative of confounding when modeling means (linear regression) than when modeling odds (logistic regression) or hazards (Cox, proportional hazards regression)\nEstimates of association from unadjusted analysis are markedly different from estimates of association from adjusted analysis\nAssociation within each stratum is similar to each other, but different from the association in the combined data\nIn linear regression, differences between adjusted and unadjusted analyses are diagnostic of confounding\nPrecision variables tend to change standard errors, but not slope estimates\nEffect modification would show differences between adjusted analysis and unadjusted analysis, but would also show different associations in the strata\nMore difficult to diagnosis confounding with non-linear functions of the mean\nCommon non-linear functions: Odds (odds ratios), hazards (hazard ratios)\nMay show the symptoms of confounding when confounding is not present\nAdjusting for precision variables can appear to be confounding\nIn logistic and PH regression, difference between adjusted and unadjusted analyses are more difficult to judge\nComparison in more homogeneous groups (i.e. after adjustment for a precision variable) will drive slope estimates away from the null\nExample: Suppose you have a sample where 50% of the subjects die\nWhat is the variability?\nWe can reduce this variability by changing \\(p\\), the probability of death\nEstimate \\(p\\) in different stratum. One stratum may have a higher \\(p\\), another a lower \\(p\\).\nBy making the estimate more precise, we have also impacted the mean"
  },
  {
    "objectID": "notes/Lec06.ConfoundingEffectModificationMediation.html#precision-variables",
    "href": "notes/Lec06.ConfoundingEffectModificationMediation.html#precision-variables",
    "title": "Confouding, Effect Modification, and Mediation",
    "section": "4 Precision Variables",
    "text": "4 Precision Variables\n\n4.1 Overview\n\nSometimes the scientific question to be answered is chosen based on which questions can be answered most precisely\nIn general, questions can be answered more precisely when the within group distribution is less variable\nComparing groups that are similar with respect to other important risk factors decreases variability\nThe precision variability is independent of the cause of the response\nIf we adjust for such a variable, we tend to gain precision\nDirected Acyclyic Graph:\n\n\n\n\n\nflowchart LR\n  X[Predictor] --&gt; Y[Outcome]\n  Z[Precision] --&gt; Y[Outcome]\n\n\n\n\n\n\nStandard errors are the key to precision\nGreater precision is achieved with smaller standard errors\nStandard errors are decreased by either increasing \\(V\\) or decreasing $n$\nTypically: \\(se(\\hat{\\theta}) = \\sqrt{\\frac{V}{n}}\\)\nWidth of CI: \\(2 \\times (\\textrm{crit value}) \\times se(\\hat{\\theta})\\)\nTest statistic: \\(Z = \\frac{\\hat{\\theta} - \\theta_0}{se(\\hat{\\theta})}\\)\nOptions for increasing precision\nIncrease sample size\nDecrease $V$\n(Decrease confidence level)\n\n\n\n4.2 Adjusting for Precision Variables\n\n4.2.1 Precision for Difference of Independent Means\nIndependent observations where group 1 has a different mean and variance than group 2\n$ Y_{ij} \\sim (\\mu_j, \\sigma_j^2), j = 1, 2; i = 1, \\ldots, n_j$\n$n = n_1 + n_2$; $r = n_1 / n_2$\n$\\theta = \\mu_1 - \\mu_2$,\n$\\hat{\\theta} = \\overline{Y}_1 - \\overline{Y}_2$\n$V = (r+1)(\\frac{\\sigma_1^2}{r} + \\sigma_2^2)$\n$se(\\hat{\\theta}) = \\sqrt{\\frac{V}{n}} = \\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}$\nMight control for some variable in order to decrease the within group\nvariability\nRestrict population sampled\nStandardize ancillary treatments\nStandardize measurement procedure\n#### Precision for Linear Regression\nIndependent continuous outcome associated with covariate ($X$)\n$\\textrm{ind } Y_i | X_i ~ \\sim(\\beta_0 + \\beta_1 X_i, \\sigma^2_{Y|X}), i = 1, \\ldots, n$\n$\\theta = \\beta_1, \\hat{\\theta} = \\hat{\\beta_1}$ from LS regression\n$V = \\frac{\\sigma^2_{Y|X}}{\\textrm{Var}(X)}$\n$se(\\hat{\\theta}) = \\sqrt{\\frac{\\hat{\\sigma}^2_{Y|X}}{n \\hat{\\textrm{Var}}(X)}}$\nAdjusting for covariates ($W$) decreases the within group standard\ndeviation\n$\\textrm{Var}(Y | X)$ versus $\\textrm{Var}(Y | X, W)$\nIndependent continuous outcome associated with covariate ($X$) and\nprecision variable ($W$)\n$\\textrm{ind } Y_i | X_i, W_i ~ \\sim(\\beta_0 + \\beta_1 X_i + \\beta_2 W_i, \\sigma^2_{Y|X,W}), i = 1, \\ldots, n$\n$\\theta = \\beta_1, \\hat{\\theta} = \\hat{\\beta_1}$ from LS regression\n$V = \\frac{\\sigma^2_{Y|X,W}}{\\textrm{Var}(X)(1-r^2_{X,W})}$\n$se(\\hat{\\theta}) = \\sqrt{\\frac{\\hat{\\sigma}^2_{Y|X}}{n \\hat{\\textrm{Var}}(X)(1-r^2_{X,W})}}$\n$\\sigma^2_{Y|X,W} = \\sigma^2_{Y|X} - \\beta_2^2 \\textrm{Var}(W | X)$\n#### Precision for Difference of Proportions\nWhen analyzing proportions (means), the mean variance relationship is\ncritical\nPrecision is greatest when proportion is close to 0 or 1\nGreater homogeneity of groups makes results more deterministic (this is\nthe goal, at least)\nIndependent binary outcomes\n$\\textrm{ind } Y_{ij} \\sim B(1, p_j), i = 1, \\ldots, n_j; j = 1, 2$\n$n = n_1 + n_2; r = n_1 / n_2$\n$\\theta = p_1 - p_2$,\n$\\hat{\\theta} = \\hat{p}_1 - \\hat{p_2} = \\overline{Y}_1 - \\overline{Y}_2$\n$\\sigma^2_j = p_j(1-p_j)$\n$V = (r+1)(\\frac{\\sigma_1^2}{r} + \\sigma_2^2)$\n$se(\\hat{\\theta}) = \\sqrt{\\frac{V}{n}} = \\sqrt{\\frac{\\sigma^2_1}{n_1} + \\frac{\\sigma^2_2}{n_2}}$\n#### Precision for Odds\nWhen analyzing odds (a nonlinear function of the mean), adjusting for\nprecision variables results in more extreme estimates\n$\\textrm{Odds} = \\frac{p}{1-p}$\nOdds using average of stratum specific $p$ is not the average of stratum\nspecific odds\nExample: Stroke by smoking (in CVD strata)\nNo association between smoking and CVD in the sample: 10% smokers in\neach group\nCVD is not a confounder, but is clearly a precision variable\nNote that the unadjusted odds ratio is attenuated toward the null\ncompared to the adjusted odds ratios\n-------- ------- ------ ------ ------ ------ ------ ------- ------- -------\n$N$ $p$ odds $N$ $p$ odds $N$ $p$ odds\nSmoke 1000 0.04 0.04 100 0.50 1.00 1100 0.082 0.089\nNonsmk 10000 0.02 0.02 1000 0.33 0.50 11000 0.048 0.051\nRatio 2.00 2.00 1.75\n-------- ------- ------ ------ ------ ------ ------ ------- ------- -------\nDiagnosing Confounding\n----------------------\n### Adjustment for Covariates\nWe include predictors in an analysis for a number of reasons. In order\nof importance\\…\n1. Scientific question\n2. Predictor of Interest\n3. Effect Modifiers\n4. Adjust for confounding\n5. Gain precision\nAdjustment for covariates changes the question being answered by the\nstatistical analysis\nAdjustments can be made to isolate associations that are of particular\ninterest\nWhen consulting with a scientist, it is often difficult to decide\nwhether the interest in an additional covariate is due to confounding,\neffect modification, or precision\nThe distinction is important because I tend to treat these variable\ndifferently in the analysis\nOften the scientific question dictates inclusion of particular\npredictors\nPredictor of interest: The scientific parameter of interest can be\nmodeled by multiple predictors (e.g. dummy variables, polynomials,\nsplines)\nEffect Modifiers: The scientific question relates to the detection of\neffect modification\nConfounders: The scientific question may be state in terms of adjusting\nfor known (or suspected) confounders\n### Confounder Detection\nUnanticipated confounding\nSome times we must explore our data to assess whether our results were\nconfounded by some variable\nGoal is to assess the “independent effect” of the predictor of interest\non the outcome\nConfounders\nVariables (causally) predictive of the outcome, but not in the causal\npathway\nBest method: Think about the scientific problem beforehand (perhaps draw\nDAG)\nUsing data, often assessed in the control group\nVariables associated with the predictor of interest in the sample\nNote that statistical significance is not relevant because this tells us\nabout associations in the population\nDetection of confounding ultimately must rely on our best knowledge\nabout the possible scientific mechanisms\nEffect of confounding: A confounder can make the association between the\npredictor of interest and the response variable look\\…\nStronger than the true association\nWeaker than the true association\nThe complete reverse of the true association (“qualitative confounding”)\nGraphical Methods for Visualizing Effect Modification, Confounding, and Precision\n---------------------------------------------------------------------------------\nConduct stratified analysis to distinguish between\nEffect modifiers\nConfounders\nPrecision variables\n### Effect Modifiers\nEstimates of treatment effect differ among strata\nWhen analyzing difference of means of continuous data, stratified smooth\ncurves of the data are non-parallel\nGraphical techniques difficult in other settings\n![image](./effectmod/effmodplot.pdf)\n### Confounders\nEstimates of treatment effect the same across strata, AND\nConfounder is causally associated with the response, AND\nConfounder associated with the POI in the sample\nWhen analyzing difference of means of continuous data\nStratified smooth curve of data are parallel\nDistribution of POI differs across strata\nUnadjusted and adjusted analyses give different estimates\n![image](./effectmod/confoundplot.pdf)\n### Precision Variables\nEstimates of treatment effect the same across strata, AND\nVariable is causally associated with the response, AND\nVariable is not associated with the POI in the sample\nWhen analyzing difference of means of continuous data\nStratified smooth curve of data are parallel\nDistribution of POI same across strata\nUnadjusted and adjusted analyses give similar estimates but with smaller\nstandard errors\n![image](./effectmod/precisionplot.pdf)"
  }
]